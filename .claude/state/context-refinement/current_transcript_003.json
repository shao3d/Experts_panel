[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_rz7chsv2mi8",
        "type": "tool_result",
        "content": "     1→\"\"\"Map service for processing posts in parallel chunks.\"\"\"\n     2→\n     3→import asyncio\n     4→import json\n     5→from typing import List, Dict, Any, Optional, Callable\n     6→from datetime import datetime\n     7→import logging\n     8→from pathlib import Path\n     9→from string import Template\n    10→\n    11→from openai import AsyncOpenAI\n    12→from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n    13→import httpx\n    14→from json_repair import repair_json\n    15→\n    16→from ..models.post import Post\n    17→from .openrouter_adapter import create_openrouter_client, convert_model_name\n    18→from ..utils.language_utils import prepare_prompt_with_language_instruction, prepare_system_message_with_language\n    19→\n    20→logger = logging.getLogger(__name__)\n    21→\n    22→\n    23→class MapService:\n    24→    \"\"\"Service for the Map phase of the Map-Resolve-Reduce pipeline.\n    25→\n    26→    Processes posts in parallel chunks, scoring relevance for each post\n    27→    relative to the user's query using Gemini 2.0 Flash.\n    28→    \"\"\"\n    29→\n    30→    DEFAULT_CHUNK_SIZE = 40\n    31→    DEFAULT_MODEL = \"qwen-2.5-72b\"  # Changed to Qwen for better document ranking ($0.08/$0.33 per 1M)\n    32→\n    33→    def __init__(\n    34→        self,\n    35→        api_key: str,\n    36→        chunk_size: int = DEFAULT_CHUNK_SIZE,\n    37→        model: str = DEFAULT_MODEL,\n    38→        max_parallel: int = None\n    39→    ):\n    40→        \"\"\"Initialize MapService.\n    41→\n    42→        Args:\n    43→            api_key: OpenAI API key\n    44→            chunk_size: Number of posts per chunk (default 30)\n    45→            model: OpenAI model to use (default gpt-4o-mini)\n    46→            max_parallel: Maximum parallel API calls (None = all chunks in parallel)\n    47→        \"\"\"\n    48→        self.client = create_openrouter_client(api_key=api_key)\n    49→        self.chunk_size = chunk_size\n    50→        self.model = convert_model_name(model)\n    51→        self.max_parallel = max_parallel\n    52→        self._prompt_template = self._load_prompt_template()\n    53→\n    54→    def _load_prompt_template(self) -> Template:\n    55→        \"\"\"Load the map phase prompt template.\"\"\"\n    56→        try:\n    57→            # Use relative path from current file location\n    58→            prompt_dir = Path(__file__).parent.parent.parent / \"prompts\"\n    59→            prompt_path = prompt_dir / \"map_prompt.txt\"\n    60→\n    61→            with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n    62→                return Template(f.read())\n    63→        except FileNotFoundError:\n    64→            logger.error(f\"Map prompt template not found at {prompt_path}\")\n    65→            raise\n    66→\n    67→    def _chunk_posts(self, posts: List[Post]) -> List[List[Post]]:\n    68→        \"\"\"Split posts into chunks of specified size.\n    69→\n    70→        Args:\n    71→            posts: List of Post objects to chunk\n    72→\n    73→        Returns:\n    74→            List of post chunks\n    75→        \"\"\"\n    76→        chunks = []\n    77→        for i in range(0, len(posts), self.chunk_size):\n    78→            chunks.append(posts[i:i + self.chunk_size])\n    79→        return chunks\n    80→\n    81→    def _get_russian_month(self, date: datetime) -> str:\n    82→        \"\"\"Get Russian month name from date.\n    83→\n    84→        Args:\n    85→            date: Datetime object\n    86→\n    87→        Returns:\n    88→            Russian month name with year\n    89→        \"\"\"\n    90→        month_names = {\n    91→            1: \"январь\", 2: \"февраль\", 3: \"март\",\n    92→            4: \"апрель\", 5: \"май\", 6: \"июнь\",\n    93→            7: \"июль\", 8: \"август\", 9: \"сентябрь\",\n    94→            10: \"октябрь\", 11: \"ноябрь\", 12: \"декабрь\"\n    95→        }\n    96→        month_name = month_names.get(date.month, \"unknown\")\n    97→        return f\"{month_name} {date.year}\"\n    98→\n    99→    def _sanitize_text(self, text: str) -> str:\n   100→        \"\"\"Remove invalid escape sequences from text.\n   101→\n   102→        Args:\n   103→            text: Text to sanitize\n   104→\n   105→        Returns:\n   106→            Sanitized text safe for JSON\n   107→        \"\"\"\n   108→        import re\n   109→        if not text:\n   110→            return text\n   111→        # Remove invalid escape sequences, keep only valid JSON escapes\n   112→        return re.sub(r'\\\\(?![ntr\"\\\\/])', '', text)\n   113→\n   114→    def _format_posts_for_prompt(self, posts: List[Post]) -> str:\n   115→        \"\"\"Format posts for inclusion in the prompt.\n   116→\n   117→        Args:\n   118→            posts: List of Post objects\n   119→\n   120→        Returns:\n   121→            Formatted string representation of posts\n   122→        \"\"\"\n   123→        formatted_posts = []\n   124→        for post in posts:\n   125→\n   126→            post_data = {\n   127→                \"telegram_message_id\": post.telegram_message_id,\n   128→                \"date\": post.created_at.strftime(\"%Y-%m-%d %H:%M\"),\n   129→                \"month_year\": self._get_russian_month(post.created_at),\n   130→                \"content\": self._sanitize_text(post.message_text) or \"[Media only]\",\n   131→                \"author\": self._sanitize_text(post.author_name) or \"Unknown\"\n   132→            }\n   133→\n   134→\n   135→            formatted_posts.append(post_data)\n   136→\n   137→        return json.dumps(formatted_posts, ensure_ascii=False, indent=2)\n   138→\n   139→    @retry(\n   140→        stop=stop_after_attempt(3),\n   141→        wait=wait_exponential(multiplier=2, min=4, max=60),\n   142→        retry=retry_if_exception_type((httpx.HTTPStatusError, json.JSONDecodeError, ValueError)),\n   143→        reraise=True\n   144→    )\n   145→    async def _process_chunk(\n   146→        self,\n   147→        chunk: List[Post],\n   148→        query: str,\n   149→        chunk_index: int,\n   150→        progress_callback: Optional[Callable] = None\n   151→    ) -> Dict[str, Any]:\n   152→        \"\"\"Process a single chunk of posts.\n   153→\n   154→        Args:\n   155→            chunk: List of posts in this chunk\n   156→            query: User's query\n   157→            chunk_index: Index of this chunk (for progress tracking)\n   158→            progress_callback: Optional callback for progress updates\n   159→\n   160→        Returns:\n   161→            Dictionary with relevant_posts and chunk_summary\n   162→        \"\"\"\n   163→        try:\n   164→            if progress_callback:\n   165→                await progress_callback({\n   166→                    \"phase\": \"map\",\n   167→                    \"status\": \"processing\",\n   168→                    \"message\": f\"Processing chunk {chunk_index + 1}\",\n   169→                    \"chunk_index\": chunk_index,\n   170→                    \"total_chunks\": None  # Will be set by parent\n   171→                })\n   172→\n   173→\n   174→            # Format the prompt\n   175→            posts_formatted = self._format_posts_for_prompt(chunk)\n   176→\n   177→            # Create base prompt\n   178→            base_prompt = self._prompt_template.substitute(\n   179→                query=query,\n   180→                posts=posts_formatted\n   181→            )\n   182→\n   183→            # Apply language instruction based on query language\n   184→            prompt = prepare_prompt_with_language_instruction(base_prompt, query)\n   185→\n   186→\n   187→\n   188→            # Prepare enhanced system message with language instruction\n   189→            enhanced_system = prepare_system_message_with_language(\n   190→                \"You are a helpful assistant that analyzes Telegram posts.\",\n   191→                query\n   192→            )\n   193→\n   194→            # Call OpenAI API - use simple JSON mode for Qwen compatibility\n   195→            response = await self.client.chat.completions.create(\n   196→                model=self.model,\n   197→                messages=[\n   198→                    {\"role\": \"system\", \"content\": enhanced_system},\n   199→                    {\"role\": \"user\", \"content\": prompt}\n   200→                ],\n   201→                temperature=0.3,\n   202→                response_format={\"type\": \"json_object\"}  # Simple JSON mode for Qwen\n   203→            )\n   204→\n   205→            # Parse response (strict mode guarantees valid JSON)\n   206→            raw_content = response.choices[0].message.content\n   207→\n   208→            # Validate non-empty response\n   209→            if not raw_content or not raw_content.strip():\n   210→                logger.error(f\"Empty response from API for chunk {chunk_index}\")\n   211→                raise ValueError(f\"Empty API response for chunk {chunk_index}\")\n   212→\n   213→            result = json.loads(raw_content)\n   214→\n   215→\n   216→            # Add chunk metadata\n   217→            result[\"chunk_index\"] = chunk_index\n   218→            result[\"posts_processed\"] = len(chunk)\n   219→\n   220→            if progress_callback:\n   221→                await progress_callback({\n   222→                    \"phase\": \"map\",\n   223→                    \"status\": \"progress\",\n   224→                    \"message\": f\"Chunk {chunk_index + 1} completed\",\n   225→                    \"chunk_index\": chunk_index,\n   226→                    \"relevant_found\": len(result.get(\"relevant_posts\", []))\n   227→                })\n   228→\n   229→            return result\n   230→\n   231→        except json.JSONDecodeError as e:\n   232→            logger.error(f\"JSON decode error in chunk {chunk_index}: {str(e)}\")\n   233→            # Log response preview for debugging (capture raw_content from outer scope)\n   234→            try:\n   235→                response_preview = raw_content[:500] if 'raw_content' in locals() else \"N/A\"\n   236→                logger.error(f\"Response preview (first 500 chars): {response_preview}\")\n   237→            except Exception:\n   238→                pass\n   239→            if progress_callback:\n   240→                await progress_callback({\n   241→                    \"phase\": \"map\",\n   242→                    \"chunk\": chunk_index,\n   243→                    \"status\": \"error\",\n   244→                    \"message\": f\"JSON decode error in chunk {chunk_index + 1}\"\n   245→                })\n   246→            raise\n   247→        except Exception as e:\n   248→            logger.error(f\"Error processing chunk {chunk_index}: {str(e)}\")\n   249→            if progress_callback:\n   250→                await progress_callback({\n   251→                    \"phase\": \"map\",\n   252→                    \"chunk\": chunk_index,\n   253→                    \"status\": \"error\",\n   254→                    \"message\": f\"Error in chunk {chunk_index + 1}: {str(e)}\"\n   255→                })\n   256→            raise\n   257→\n   258→    async def process(\n   259→        self,\n   260→        posts: List[Post],\n   261→        query: str,\n   262→        expert_id: str = \"unknown\",\n   263→        progress_callback: Optional[Callable] = None\n   264→    ) -> Dict[str, Any]:\n   265→        \"\"\"Process all posts in parallel chunks.\n   266→\n   267→        Args:\n   268→            posts: List of all Post objects to process\n   269→            query: User's query\n   270→            expert_id: Expert identifier for logging\n   271→            progress_callback: Optional callback for progress updates\n   272→\n   273→        Returns:\n   274→            Dictionary with aggregated results from all chunks\n   275→        \"\"\"\n   276→        import time\n   277→        phase_start_time = time.time()\n   278→\n   279→        if not posts:\n   280→            return {\n   281→                \"relevant_posts\": [],\n   282→                \"total_processed\": 0,\n   283→                \"chunks_processed\": 0,\n   284→                \"summary\": \"No posts to process\"\n   285→            }\n   286→\n   287→        # Split into chunks\n   288→        chunks = self._chunk_posts(posts)\n   289→        total_chunks = len(chunks)\n   290→\n   291→        logger.info(f\"[{expert_id}] Processing {len(posts)} posts in {total_chunks} chunks of {self.chunk_size}\")\n   292→\n   293→        if progress_callback:\n   294→            await progress_callback({\n   295→                \"phase\": \"map\",\n   296→                \"status\": \"starting\",\n   297→                \"message\": f\"Starting Map phase: {len(posts)} posts in {total_chunks} chunks\",\n   298→                \"total_chunks\": total_chunks,\n   299→                \"chunk_index\": 0\n   300→            })\n   301→\n   302→        # Process chunks in parallel with dynamic concurrency limit\n   303→        # If max_parallel is None, use the number of chunks (all in parallel)\n   304→        parallel_limit = self.max_parallel if self.max_parallel is not None else total_chunks\n   305→        semaphore = asyncio.Semaphore(parallel_limit)\n   306→\n   307→        async def process_with_semaphore(chunk, index):\n   308→            async with semaphore:\n   309→                return await self._process_chunk(chunk, query, index, progress_callback)\n   310→\n   311→        # Create tasks for all chunks\n   312→        tasks = [\n   313→            process_with_semaphore(chunk, i)\n   314→            for i, chunk in enumerate(chunks)\n   315→        ]\n   316→\n   317→        # Wait for all chunks to complete\n   318→        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)\n   319→\n   320→        # Aggregate results and identify failed chunks for retry\n   321→        all_relevant_posts = []\n   322→        chunks_with_errors = []\n   323→        failed_chunks = []\n   324→\n   325→        for i, result in enumerate(chunk_results):\n   326→            if isinstance(result, Exception):\n   327→                logger.error(f\"Chunk {i} failed: {result}\")\n   328→                chunks_with_errors.append(i)\n   329→                # Store failed chunk info for retry\n   330→                failed_chunks.append((chunks[i], i))\n   331→            else:\n   332→                all_relevant_posts.extend(result.get(\"relevant_posts\", []))\n   333→\n   334→        # Global retry mechanism for failed chunks\n   335→        retry_attempts = 0\n   336→        max_global_retries = 1\n   337→\n   338→        while failed_chunks and retry_attempts < max_global_retries:\n   339→            retry_attempts += 1\n   340→            logger.info(f\"[{expert_id}] Global retry attempt {retry_attempts} for {len(failed_chunks)} failed chunks\")\n   341→\n   342→            if progress_callback:\n   343→                await progress_callback({\n   344→                    \"phase\": \"map\",\n   345→                    \"status\": \"retrying\",\n   346→                    \"message\": f\"Global retry {retry_attempts} for {len(failed_chunks)} failed chunks\"\n   347→                })\n   348→\n   349→            # Retry failed chunks\n   350→            retry_tasks = [\n   351→                self._process_chunk(chunk, query, index, progress_callback)\n   352→                for chunk, index in failed_chunks\n   353→            ]\n   354→\n   355→            retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)\n   356→\n   357→            # Process retry results\n   358→            new_failed_chunks = []\n   359→\n   360→            for (chunk, original_index), result in zip(failed_chunks, retry_results):\n   361→                if isinstance(result, Exception):\n   362→                    logger.error(f\"[{expert_id}] Chunk {original_index} failed on retry {retry_attempts}: {result}\")\n   363→                    new_failed_chunks.append((chunk, original_index))\n   364→                else:\n   365→                    logger.info(f\"[{expert_id}] Chunk {original_index} recovered on retry {retry_attempts}\")\n   366→                    all_relevant_posts.extend(result.get(\"relevant_posts\", []))\n   367→                    # Remove from error list since it recovered\n   368→                    chunks_with_errors.remove(original_index)\n   369→\n   370→            failed_chunks = new_failed_chunks\n   371→\n   372→        if failed_chunks:\n   373→            logger.warning(f\"[{expert_id}] {len(failed_chunks)} chunks still failed after {max_global_retries} global retries\")\n   374→            for chunk, index in failed_chunks:\n   375→                logger.error(f\"[{expert_id}] Chunk {index} permanently failed after all retries\")\n   376→\n   377→        # Sort by relevance (HIGH > MEDIUM > LOW)\n   378→        relevance_order = {\"HIGH\": 0, \"MEDIUM\": 1, \"LOW\": 2}\n   379→        all_relevant_posts.sort(\n   380→            key=lambda x: (relevance_order.get(x.get(\"relevance\", \"LOW\"), 3), x.get(\"telegram_message_id\", 0))\n   381→        )\n   382→\n   383→        # Create final summary\n   384→        high_count = sum(1 for p in all_relevant_posts if p.get(\"relevance\") == \"HIGH\")\n   385→        medium_count = sum(1 for p in all_relevant_posts if p.get(\"relevance\") == \"MEDIUM\")\n   386→        low_count = sum(1 for p in all_relevant_posts if p.get(\"relevance\") == \"LOW\")\n   387→\n   388→        summary = (\n   389→            f\"Processed {len(posts)} posts in {total_chunks} chunks. \"\n   390→            f\"Found {len(all_relevant_posts)} relevant posts: \"\n   391→            f\"{high_count} HIGH, {medium_count} MEDIUM, {low_count} LOW relevance.\"\n   392→        )\n   393→\n   394→        if chunks_with_errors:\n   395→            summary += f\" {len(chunks_with_errors)} chunks had errors.\"\n   396→\n   397→        # Log phase completion with timing\n   398→        duration_ms = int((time.time() - phase_start_time) * 1000)\n   399→        logger.info(\n   400→            f\"[{expert_id}] Map Phase END: {duration_ms}ms, \"\n   401→            f\"found {len(all_relevant_posts)} posts \"\n   402→            f\"({high_count} HIGH, {medium_count} MEDIUM, {low_count} LOW)\"\n   403→        )\n   404→\n   405→        if progress_callback:\n   406→            await progress_callback({\n   407→                \"phase\": \"map\",\n   408→                \"status\": \"completed\",\n   409→                \"message\": \"Map phase completed\",\n   410→                \"summary\": summary\n   411→            })\n   412→\n   413→        return {\n   414→            \"relevant_posts\": all_relevant_posts,\n   415→            \"total_processed\": len(posts),\n   416→            \"chunks_processed\": total_chunks - len(chunks_with_errors),\n   417→            \"chunks_with_errors\": chunks_with_errors,\n   418→            \"summary\": summary,\n   419→            \"statistics\": {\n   420→                \"high_relevance\": high_count,\n   421→                \"medium_relevance\": medium_count,\n   422→                \"low_relevance\": low_count\n   423→            }\n   424→        }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_bq0nuq114in",
        "type": "tool_result",
        "content": "     1→\"\"\"Reduce service for synthesizing comprehensive answers from posts.\"\"\"\n     2→\n     3→import json\n     4→import logging\n     5→from typing import List, Dict, Any, Optional, Callable\n     6→from datetime import datetime\n     7→from pathlib import Path\n     8→from string import Template\n     9→\n    10→from openai import AsyncOpenAI\n    11→from tenacity import retry, stop_after_attempt, wait_exponential\n    12→\n    13→from .openrouter_adapter import create_openrouter_client, convert_model_name\n    14→from ..models.base import SessionLocal\n    15→from ..utils.language_utils import prepare_prompt_with_language_instruction, prepare_system_message_with_language\n    16→\n    17→logger = logging.getLogger(__name__)\n    18→\n    19→\n    20→class ReduceService:\n    21→    \"\"\"Service for the Reduce phase of the Map-Resolve-Reduce pipeline.\n    22→\n    23→    Synthesizes a comprehensive answer from all gathered posts,\n    24→    using Gemini 2.0 Flash to create a coherent response with source attribution.\n    25→    \"\"\"\n    26→\n    27→    DEFAULT_MODEL = \"gemini-2.0-flash\"  # Upgraded from gpt-4o-mini for better synthesis (maps to gemini-2.5-flash)\n    28→    MAX_CONTEXT_POSTS = 50  # Limit for token management\n    29→\n    30→    def __init__(\n    31→        self,\n    32→        api_key: str,\n    33→        model: str = DEFAULT_MODEL,\n    34→        use_personal_style: bool = True\n    35→    ):\n    36→        \"\"\"Initialize ReduceService.\n    37→\n    38→        Args:\n    39→            api_key: OpenAI API key\n    40→            model: OpenAI model to use (default gpt-4o-mini)\n    41→            use_personal_style: Use Refat's personal writing style (default True)\n    42→        \"\"\"\n    43→        self.client = create_openrouter_client(api_key=api_key)\n    44→        self.model = convert_model_name(model)\n    45→        self.use_personal_style = use_personal_style\n    46→        self._prompt_template = self._load_prompt_template()\n    47→\n    48→    def _load_prompt_template(self) -> Template:\n    49→        \"\"\"Load the reduce phase prompt template.\"\"\"\n    50→        try:\n    51→            # Use relative path from current file location\n    52→            prompt_dir = Path(__file__).parent.parent.parent / \"prompts\"\n    53→            # Choose prompt based on style preference\n    54→            prompt_filename = \"reduce_prompt_personal.txt\" if self.use_personal_style else \"reduce_prompt.txt\"\n    55→            prompt_path = prompt_dir / prompt_filename\n    56→\n    57→            with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n    58→                return Template(f.read())\n    59→        except FileNotFoundError:\n    60→            logger.error(f\"Reduce prompt template not found at {prompt_path}\")\n    61→            raise\n    62→\n    63→\n    64→    def _format_posts_for_synthesis(\n    65→        self,\n    66→        enriched_posts: List[Dict[str, Any]]\n    67→    ) -> str:\n    68→        \"\"\"Format enriched posts for the synthesis prompt.\n    69→\n    70→        Args:\n    71→            enriched_posts: List of enriched post data\n    72→\n    73→        Returns:\n    74→            Formatted JSON string of posts\n    75→        \"\"\"\n    76→        # Sort by relevance FIRST (before limiting)\n    77→        relevance_order = {\"HIGH\": 0, \"MEDIUM\": 1, \"LOW\": 2, \"CONTEXT\": 3}\n    78→        sorted_posts = sorted(\n    79→            enriched_posts,\n    80→            key=lambda x: relevance_order.get(x.get(\"relevance\", \"MEDIUM\"), 3)\n    81→        )\n    82→\n    83→        # Then limit to MAX_CONTEXT_POSTS (50)\n    84→        posts_to_include = sorted_posts[:self.MAX_CONTEXT_POSTS]\n    85→\n    86→        if len(enriched_posts) > self.MAX_CONTEXT_POSTS:\n    87→            logger.warning(\n    88→                f\"Limiting context to {self.MAX_CONTEXT_POSTS} posts \"\n    89→                f\"(had {len(enriched_posts)}). Kept best by relevance.\"\n    90→            )\n    91→\n    92→        # Format posts for prompt\n    93→        formatted_posts = []\n    94→        for post in posts_to_include:\n    95→            formatted_post = {\n    96→                \"telegram_message_id\": post[\"telegram_message_id\"],\n    97→                \"date\": post.get(\"created_at\", \"Unknown\"),\n    98→                \"content\": post.get(\"content\") or \"[Media only]\",\n    99→                \"author\": post.get(\"author\") or \"Unknown\",\n   100→                \"relevance\": post.get(\"relevance\", \"MEDIUM\"),\n   101→                \"is_original\": post.get(\"is_original\", True)\n   102→            }\n   103→            formatted_posts.append(formatted_post)\n   104→\n   105→        return json.dumps(formatted_posts, ensure_ascii=False, indent=2)\n   106→\n   107→    @retry(\n   108→        stop=stop_after_attempt(3),\n   109→        wait=wait_exponential(multiplier=1, min=4, max=10),\n   110→        reraise=True\n   111→    )\n   112→    async def _synthesize_answer(\n   113→        self,\n   114→        query: str,\n   115→        posts_json: str,\n   116→        expert_id: str = \"unknown\",\n   117→        progress_callback: Optional[Callable] = None\n   118→    ) -> Dict[str, Any]:\n   119→        \"\"\"Call GPT-4o-mini to synthesize the final answer.\n   120→\n   121→        Args:\n   122→            query: User's query\n   123→            posts_json: JSON string of formatted posts\n   124→            progress_callback: Optional callback for progress updates\n   125→\n   126→        Returns:\n   127→            Synthesized answer response\n   128→        \"\"\"\n   129→        if progress_callback:\n   130→            await progress_callback({\n   131→                \"phase\": \"reduce\",\n   132→                \"status\": \"synthesizing\",\n   133→                \"message\": \"Synthesizing comprehensive answer\",\n   134→                \"processed\": 0,\n   135→                \"total\": 1\n   136→            })\n   137→\n   138→        # Create base prompt\n   139→        base_prompt = self._prompt_template.substitute(\n   140→            query=query,\n   141→            posts=posts_json\n   142→        )\n   143→\n   144→        # Apply language instruction based on query language\n   145→        prompt = prepare_prompt_with_language_instruction(base_prompt, query)\n   146→\n   147→        # Prepare enhanced system message with language instruction\n   148→        enhanced_system = prepare_system_message_with_language(\n   149→            \"You are a helpful assistant that synthesizes information from Telegram posts.\",\n   150→            query\n   151→        )\n   152→\n   153→        # Call OpenAI API\n   154→        response = await self.client.chat.completions.create(\n   155→            model=self.model,\n   156→            messages=[\n   157→                {\"role\": \"system\", \"content\": enhanced_system},\n   158→                {\"role\": \"user\", \"content\": prompt}\n   159→            ],\n   160→            temperature=0.4,  # Balance between accuracy and natural language\n   161→            response_format={\"type\": \"json_object\"}\n   162→        )\n   163→\n   164→        # Parse response with robust error handling\n   165→        raw_content = response.choices[0].message.content\n   166→        try:\n   167→            result = json.loads(raw_content)\n   168→        except json.JSONDecodeError as e:\n   169→            logger.error(f\"JSON parsing failed: {e}\")\n   170→            logger.debug(f\"Raw content (first 500 chars): {raw_content[:500]}\")\n   171→\n   172→            # Try to fix common escape issues\n   173→            try:\n   174→                # Fix invalid escape sequences by escaping backslashes\n   175→                fixed_content = raw_content.replace('\\\\', '\\\\\\\\')\n   176→                # But preserve valid JSON escapes\n   177→                fixed_content = (fixed_content\n   178→                    .replace('\\\\\\\\n', '\\\\n')\n   179→                    .replace('\\\\\\\\t', '\\\\t')\n   180→                    .replace('\\\\\\\\r', '\\\\r')\n   181→                    .replace('\\\\\\\\\"', '\\\\\"')\n   182→                    .replace('\\\\\\\\/', '\\\\/')\n   183→                )\n   184→                result = json.loads(fixed_content)\n   185→                logger.info(\"Successfully parsed JSON after fixing escape sequences\")\n   186→            except json.JSONDecodeError as e2:\n   187→                logger.error(f\"JSON parsing failed even after fixes: {e2}\")\n   188→                logger.error(f\"Full raw content: {raw_content}\")\n   189→                raise ValueError(f\"Failed to parse GPT response as JSON: {e}\") from e\n   190→\n   191→        # ALWAYS sanitize answer string to prevent frontend JSON parsing errors\n   192→        # This is critical: even if JSON parsing succeeded, the answer string may\n   193→        # contain invalid escape sequences that will break frontend JSON.parse()\n   194→        if \"answer\" in result and isinstance(result[\"answer\"], str):\n   195→            import re\n   196→            answer = result[\"answer\"]\n   197→            # Remove invalid escape sequences by cleaning backslashes\n   198→            # Keep only valid JSON escapes: \\n \\t \\r \\\" \\/ \\\\\n   199→            answer = re.sub(r'\\\\(?![ntr\"\\\\/])', '', answer)\n   200→            result[\"answer\"] = answer\n   201→            logger.info(\"Sanitized answer string for safe JSON transmission\")\n   202→\n   203→        # Track token usage\n   204→        if response.usage:\n   205→            result[\"token_usage\"] = {\n   206→                \"prompt_tokens\": response.usage.prompt_tokens,\n   207→                \"completion_tokens\": response.usage.completion_tokens,\n   208→                \"total_tokens\": response.usage.total_tokens\n   209→            }\n   210→\n   211→        if progress_callback:\n   212→            await progress_callback({\n   213→                \"phase\": \"reduce\",\n   214→                \"status\": \"synthesized\",\n   215→                \"message\": \"Answer synthesis completed\",\n   216→                \"processed\": 1,\n   217→                \"total\": 1\n   218→            })\n   219→\n   220→        # Validate that all referenced posts are in main_sources (important for Gemini)\n   221→        import re\n   222→        mentioned_ids = re.findall(r'\\[post:(\\d+)\\]', result.get(\"answer\", \"\"))\n   223→        main_sources = result.get(\"main_sources\", [])\n   224→\n   225→        # Check for any missing IDs\n   226→        missing_ids = []\n   227→        for post_id_str in mentioned_ids:\n   228→            post_id = int(post_id_str)\n   229→            if post_id not in main_sources:\n   230→                missing_ids.append(post_id)\n   231→                logger.warning(f\"Post {post_id} mentioned in answer but not in main_sources!\")\n   232→\n   233→        # Add missing IDs to main_sources to maintain consistency\n   234→        if missing_ids:\n   235→            logger.info(f\"Adding {len(missing_ids)} missing IDs to main_sources: {missing_ids}\")\n   236→            result[\"main_sources\"] = sorted(list(set(main_sources + missing_ids)))\n   237→\n   238→        # Add fact validation\n   239→        try:\n   240→            from .fact_validator import FactValidator\n   241→\n   242→            with SessionLocal() as db:\n   243→                validator = FactValidator(db)\n   244→                validation_report = validator.generate_validation_report(\n   245→                    answer=result.get(\"answer\", \"\"),\n   246→                    post_ids=result.get(\"main_sources\", [])\n   247→                )\n   248→                result[\"validation\"] = validation_report\n   249→                result[\"accuracy_score\"] = validation_report.get(\"accuracy_score\", 1.0)\n   250→\n   251→                # Log validation results\n   252→                logger.info(f\"[{expert_id}] Fact validation completed - Accuracy: {result['accuracy_score']:.2%}\")\n   253→                if result[\"accuracy_score\"] < 1.0:\n   254→                    logger.warning(f\"[{expert_id}] Some referenced posts may not exist or have incorrect dates\")\n   255→        except Exception as e:\n   256→            logger.error(f\"Fact validation failed: {e}\")\n   257→            # Don't fail the request, just log the error\n   258→            result[\"validation\"] = {\"error\": str(e)}\n   259→\n   260→        return result\n   261→\n   262→    async def process(\n   263→        self,\n   264→        enriched_posts: List[Dict[str, Any]],\n   265→        query: str,\n   266→        expert_id: str = \"unknown\",\n   267→        progress_callback: Optional[Callable] = None\n   268→    ) -> Dict[str, Any]:\n   269→        \"\"\"Process and synthesize a comprehensive answer.\n   270→\n   271→        Args:\n   272→            enriched_posts: List of enriched post data from Resolve phase\n   273→            query: User's query\n   274→            expert_id: Expert identifier for logging\n   275→            progress_callback: Optional callback for progress updates\n   276→\n   277→        Returns:\n   278→            Dictionary with final answer and metadata\n   279→        \"\"\"\n   280→        import time\n   281→        phase_start_time = time.time()\n   282→\n   283→        if not enriched_posts:\n   284→            return {\n   285→                \"answer\": \"К сожалению, не найдено релевантных постов для ответа на ваш вопрос.\",\n   286→                \"main_sources\": [],\n   287→                \"confidence\": \"LOW\",\n   288→                \"has_expert_comments\": False,\n   289→                \"language\": \"ru\",\n   290→                \"posts_analyzed\": 0,\n   291→                \"summary\": \"Нет постов для анализа\"\n   292→            }\n   293→\n   294→        logger.info(f\"[{expert_id}] Synthesizing answer from {len(enriched_posts)} posts\")\n   295→\n   296→        if progress_callback:\n   297→            await progress_callback({\n   298→                \"phase\": \"reduce\",\n   299→                \"status\": \"starting\",\n   300→                \"message\": f\"Starting Reduce phase with {len(enriched_posts)} posts\",\n   301→                \"processed\": 0,\n   302→                \"total\": 1\n   303→            })\n   304→\n   305→        # Format posts for synthesis\n   306→        posts_json = self._format_posts_for_synthesis(enriched_posts)\n   307→\n   308→        # Synthesize the answer (no comments needed)\n   309→        synthesis = await self._synthesize_answer(\n   310→            query,\n   311→            posts_json,\n   312→            expert_id,\n   313→            progress_callback\n   314→        )\n   315→\n   316→        # Extract main sources from synthesis or derive from high-relevance posts\n   317→        main_sources = synthesis.get(\"main_sources\", [])\n   318→        if not main_sources:\n   319→            # Fallback to high-relevance posts\n   320→            main_sources = [\n   321→                p[\"telegram_message_id\"]\n   322→                for p in enriched_posts\n   323→                if p.get(\"relevance\") == \"HIGH\"\n   324→            ][:5]  # Top 5\n   325→\n   326→        # Count posts by relevance\n   327→        relevance_counts = {\n   328→            \"HIGH\": sum(1 for p in enriched_posts if p.get(\"relevance\") == \"HIGH\"),\n   329→            \"MEDIUM\": sum(1 for p in enriched_posts if p.get(\"relevance\") == \"MEDIUM\"),\n   330→            \"LOW\": sum(1 for p in enriched_posts if p.get(\"relevance\") == \"LOW\"),\n   331→            \"CONTEXT\": sum(1 for p in enriched_posts if p.get(\"relevance\") == \"CONTEXT\")\n   332→        }\n   333→\n   334→        summary = (\n   335→            f\"Synthesized answer from {len(enriched_posts)} posts. \"\n   336→            f\"Distribution: {relevance_counts['HIGH']} HIGH, \"\n   337→            f\"{relevance_counts['MEDIUM']} MEDIUM, \"\n   338→            f\"{relevance_counts['LOW']} LOW, \"\n   339→            f\"{relevance_counts['CONTEXT']} CONTEXT.\"\n   340→        )\n   341→\n   342→        # Log phase completion with timing\n   343→        duration_ms = int((time.time() - phase_start_time) * 1000)\n   344→        logger.info(\n   345→            f\"[{expert_id}] Reduce Phase END: {duration_ms}ms, \"\n   346→            f\"synthesized answer from {len(enriched_posts)} posts, \"\n   347→            f\"confidence={synthesis.get('confidence', 'MEDIUM')}\"\n   348→        )\n   349→\n   350→        if progress_callback:\n   351→            await progress_callback({\n   352→                \"phase\": \"reduce\",\n   353→                \"status\": \"completed\",\n   354→                \"message\": \"Reduce phase completed\",\n   355→                \"summary\": summary,\n   356→                \"processed\": 1,\n   357→                \"total\": 1\n   358→            })\n   359→\n   360→        return {\n   361→            \"answer\": synthesis.get(\"answer\", \"\"),\n   362→            \"main_sources\": main_sources,\n   363→            \"confidence\": synthesis.get(\"confidence\", \"MEDIUM\"),\n   364→            \"has_expert_comments\": synthesis.get(\"has_expert_comments\", False),\n   365→            \"language\": synthesis.get(\"language\", \"ru\"),\n   366→            \"posts_analyzed\": len(enriched_posts),\n   367→            \"expert_comments_included\": 0,\n   368→            \"relevance_distribution\": relevance_counts,\n   369→            \"token_usage\": synthesis.get(\"token_usage\"),\n   370→            \"summary\": summary\n   371→        }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_cnpqhrr1u1j",
        "type": "tool_result",
        "content": "     1→\"\"\"OpenRouter adapter for OpenAI-compatible API.\"\"\"\n     2→\n     3→import os\n     4→from openai import AsyncOpenAI\n     5→\n     6→\n     7→def create_openrouter_client(api_key: str = None) -> AsyncOpenAI:\n     8→    \"\"\"Create OpenAI client configured for OpenRouter.\n     9→\n    10→    Args:\n    11→        api_key: OpenRouter API key. If not provided, uses OPENAI_API_KEY env var.\n    12→\n    13→    Returns:\n    14→        AsyncOpenAI client configured for OpenRouter\n    15→    \"\"\"\n    16→    if not api_key:\n    17→        api_key = os.getenv(\"OPENAI_API_KEY\")\n    18→\n    19→    return AsyncOpenAI(\n    20→        api_key=api_key,\n    21→        base_url=\"https://openrouter.ai/api/v1\"\n    22→    )\n    23→\n    24→\n    25→def convert_model_name(openai_model: str) -> str:\n    26→    \"\"\"Convert OpenAI model name to OpenRouter format.\n    27→\n    28→    Args:\n    29→        openai_model: OpenAI model name (e.g., \"gpt-4o-mini\")\n    30→\n    31→    Returns:\n    32→        OpenRouter model name (e.g., \"openai/gpt-4o-mini\")\n    33→    \"\"\"\n    34→    # Map of common models including Gemini\n    35→    model_map = {\n    36→        # OpenAI models\n    37→        \"gpt-4o-mini\": \"openai/gpt-4o-mini\",\n    38→        \"gpt-4o\": \"openai/gpt-4o\",\n    39→        \"gpt-4-turbo\": \"openai/gpt-4-turbo\",\n    40→        \"gpt-3.5-turbo\": \"openai/gpt-3.5-turbo\",\n    41→\n    42→        # Google Gemini models\n    43→        \"gemini-2.0-flash\": \"google/gemini-2.0-flash-001\",  # Stable paid version ($0.10/$0.40 per 1M)\n    44→        \"gemini-2.0-flash-free\": \"google/gemini-2.0-flash-exp:free\",  # Free experimental version\n    45→        \"gemini-2.5-flash\": \"google/gemini-2.5-flash\",\n    46→        \"gemini-1.5-flash\": \"google/gemini-1.5-flash\",\n    47→\n    48→        # Alibaba Qwen models (October 2025)\n    49→        \"qwen-2.5-72b\": \"qwen/qwen-2.5-72b-instruct\",  # $0.08/$0.33 per 1M - great for document ranking\n    50→        \"qwen-2.5-32b\": \"qwen/qwen-2.5-32b-instruct\",  # Smaller, faster variant\n    51→        \"qwen-2.5-coder-32b\": \"qwen/qwen-2.5-coder-32b-instruct\"  # Code-specific\n    52→    }\n    53→\n    54→    # Return mapped model or add openai/ prefix if not found\n    55→    if openai_model in model_map:\n    56→        return model_map[openai_model]\n    57→    # If model already has a provider prefix (google/, anthropic/, etc.), don't modify\n    58→    elif \"/\" in openai_model:\n    59→        return openai_model\n    60→    # Only add openai/ prefix to models without any provider prefix\n    61→    else:\n    62→        return f\"openai/{openai_model}\"\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_tefcp8hrdy",
        "type": "tool_result",
        "content": "     1→\"\"\"Simplified Resolve service - enriches posts by following author's links without GPT evaluation.\"\"\"\n     2→\n     3→import logging\n     4→from typing import List, Dict, Any, Optional, Callable, Set\n     5→from sqlalchemy.orm import Session\n     6→from sqlalchemy import or_\n     7→\n     8→from ..models.post import Post\n     9→from ..models.link import Link, LinkType\n    10→from ..models.base import SessionLocal\n    11→\n    12→logger = logging.getLogger(__name__)\n    13→\n    14→\n    15→class SimpleResolveService:\n    16→    \"\"\"Simplified Resolve service that trusts author's linking decisions.\n    17→\n    18→    Key principle: The author already made the cognitive work of deciding what to link.\n    19→    We simply follow these links to depth 1, no GPT evaluation needed.\n    20→    \"\"\"\n    21→\n    22→    def __init__(self):\n    23→        \"\"\"Initialize SimpleResolveService - no API key needed!\"\"\"\n    24→        pass\n    25→\n    26→    def _fetch_posts_by_telegram_ids(\n    27→        self,\n    28→        db: Session,\n    29→        telegram_message_ids: List[int],\n    30→        expert_id: str\n    31→    ) -> Dict[int, Post]:\n    32→        \"\"\"Fetch posts from database by telegram_message_id.\n    33→\n    34→        Args:\n    35→            db: Database session\n    36→            telegram_message_ids: List of telegram message IDs\n    37→            expert_id: Expert identifier to filter posts\n    38→\n    39→        Returns:\n    40→            Dictionary mapping telegram_message_id to Post object\n    41→        \"\"\"\n    42→        if not telegram_message_ids:\n    43→            return {}\n    44→\n    45→        posts = db.query(Post).filter(\n    46→            Post.telegram_message_id.in_(telegram_message_ids),\n    47→            Post.expert_id == expert_id\n    48→        ).all()\n    49→\n    50→        return {post.telegram_message_id: post for post in posts}\n    51→\n    52→    def _get_linked_posts(\n    53→        self,\n    54→        db: Session,\n    55→        initial_post_ids: Set[int],\n    56→        expert_id: str\n    57→    ) -> Set[int]:\n    58→        \"\"\"Get all posts linked at depth 1 from initial posts.\n    59→\n    60→        Philosophy: Author's links are intentional - if they linked it, it's relevant.\n    61→        98.9% of links are MENTION type (thematic connections).\n    62→\n    63→        Args:\n    64→            db: Database session\n    65→            initial_post_ids: Set of telegram_message_ids to expand from\n    66→            expert_id: Expert identifier to filter posts\n    67→\n    68→        Returns:\n    69→            Set of all linked telegram_message_ids (including initial)\n    70→        \"\"\"\n    71→        if not initial_post_ids:\n    72→            return set()\n    73→\n    74→        # Get Post objects to get their database IDs\n    75→        posts = db.query(Post).filter(\n    76→            Post.telegram_message_id.in_(initial_post_ids),\n    77→            Post.expert_id == expert_id\n    78→        ).all()\n    79→\n    80→        if not posts:\n    81→            logger.warning(f\"No posts found for telegram_ids: {initial_post_ids}\")\n    82→            return initial_post_ids\n    83→\n    84→        post_id_to_telegram = {p.post_id: p.telegram_message_id for p in posts}\n    85→        db_post_ids = list(post_id_to_telegram.keys())\n    86→\n    87→        # Get ALL links where our posts are source or target\n    88→        links = db.query(Link).filter(\n    89→            or_(\n    90→                Link.source_post_id.in_(db_post_ids),\n    91→                Link.target_post_id.in_(db_post_ids)\n    92→            )\n    93→        ).all()\n    94→\n    95→        linked_telegram_ids = set(initial_post_ids)\n    96→\n    97→        # Collect only NEW linked post IDs (not in initial set)\n    98→        linked_db_ids = set()\n    99→        for link in links:\n   100→            # Only add posts that are NOT in our initial set\n   101→            if link.source_post_id not in db_post_ids:\n   102→                linked_db_ids.add(link.source_post_id)\n   103→            if link.target_post_id not in db_post_ids:\n   104→                linked_db_ids.add(link.target_post_id)\n   105→\n   106→        # Convert database IDs back to telegram_message_ids\n   107→        if linked_db_ids:\n   108→            linked_posts = db.query(Post).filter(\n   109→                Post.post_id.in_(linked_db_ids),\n   110→                Post.expert_id == expert_id\n   111→            ).all()\n   112→\n   113→            for post in linked_posts:\n   114→                linked_telegram_ids.add(post.telegram_message_id)\n   115→\n   116→        logger.info(\n   117→            f\"[{expert_id}] Expanded {len(initial_post_ids)} posts to {len(linked_telegram_ids)} \"\n   118→            f\"(+{len(linked_telegram_ids) - len(initial_post_ids)} linked posts)\"\n   119→        )\n   120→\n   121→        return linked_telegram_ids\n   122→\n   123→    async def process(\n   124→        self,\n   125→        relevant_posts: List[Dict[str, Any]],\n   126→        query: str,  # Kept for interface compatibility\n   127→        expert_id: str,\n   128→        progress_callback: Optional[Callable] = None\n   129→    ) -> Dict[str, Any]:\n   130→        \"\"\"Process and enrich posts with linked context at depth 1.\n   131→\n   132→        Args:\n   133→            relevant_posts: List of relevant post data from Map phase\n   134→            query: User's query (not used in simplified version)\n   135→            expert_id: Expert identifier to filter posts\n   136→            progress_callback: Optional callback for progress updates\n   137→\n   138→        Returns:\n   139→            Dictionary with enriched post data\n   140→        \"\"\"\n   141→        import time\n   142→        phase_start_time = time.time()\n   143→\n   144→        if not relevant_posts:\n   145→            return {\n   146→                \"enriched_posts\": [],\n   147→                \"links_followed\": 0,\n   148→                \"additional_posts\": 0,\n   149→                \"summary\": \"No posts to enrich\"\n   150→            }\n   151→\n   152→        # Extract telegram_message_ids from relevant posts\n   153→        relevant_ids = [p[\"telegram_message_id\"] for p in relevant_posts]\n   154→\n   155→        logger.info(f\"[{expert_id}] Resolve Phase START: Enriching {len(relevant_ids)} relevant posts with linked context\")\n   156→\n   157→        if progress_callback:\n   158→            await progress_callback({\n   159→                \"phase\": \"resolve\",\n   160→                \"status\": \"starting\",\n   161→                \"message\": f\"Expanding {len(relevant_ids)} posts with author's links\",\n   162→                \"processed\": 0,\n   163→                \"total\": len(relevant_ids)\n   164→            })\n   165→\n   166→        # Create database session\n   167→        db = SessionLocal()\n   168→        try:\n   169→            # Get all linked posts at depth 1 - NO LIMITS!\n   170→            all_post_ids = self._get_linked_posts(\n   171→                db,\n   172→                set(relevant_ids),\n   173→                expert_id\n   174→            )\n   175→\n   176→            # Fetch all posts (original + linked)\n   177→            posts_map = self._fetch_posts_by_telegram_ids(\n   178→                db, list(all_post_ids), expert_id\n   179→            )\n   180→\n   181→            # Build enriched result\n   182→            enriched_posts = []\n   183→\n   184→            # First add original relevant posts with their scores\n   185→            for post_data in relevant_posts:\n   186→                tid = post_data[\"telegram_message_id\"]\n   187→                if tid in posts_map:\n   188→                    post = posts_map[tid]\n   189→                    enriched_posts.append({\n   190→                        \"telegram_message_id\": tid,\n   191→                        \"relevance\": post_data.get(\"relevance\", \"MEDIUM\"),\n   192→                        \"reason\": post_data.get(\"reason\", \"\"),\n   193→                        \"is_original\": True,\n   194→                        \"content\": post.message_text,\n   195→                        \"author\": post.author_name,\n   196→                        \"created_at\": post.created_at.isoformat() if post.created_at else None\n   197→                    })\n   198→\n   199→            # Then add ALL linked posts\n   200→            linked_count = 0\n   201→            for tid in all_post_ids:\n   202→                if tid not in relevant_ids and tid in posts_map:\n   203→                    post = posts_map[tid]\n   204→                    enriched_posts.append({\n   205→                        \"telegram_message_id\": tid,\n   206→                        \"relevance\": \"CONTEXT\",\n   207→                        \"reason\": \"Author-linked context (depth 1)\",\n   208→                        \"is_original\": False,\n   209→                        \"content\": post.message_text,\n   210→                        \"author\": post.author_name,\n   211→                        \"created_at\": post.created_at.isoformat() if post.created_at else None\n   212→                    })\n   213→                    linked_count += 1\n   214→\n   215→            summary = (\n   216→                f\"Enriched {len(relevant_ids)} posts with {linked_count} author-linked posts. \"\n   217→                f\"Total context: {len(enriched_posts)} posts.\"\n   218→            )\n   219→\n   220→            # Log phase completion with timing\n   221→            duration_ms = int((time.time() - phase_start_time) * 1000)\n   222→            logger.info(\n   223→                f\"[{expert_id}] Resolve Phase END: {duration_ms}ms, \"\n   224→                f\"expanded {len(relevant_ids)} posts to {len(enriched_posts)} posts (+{linked_count} linked)\"\n   225→            )\n   226→\n   227→            if progress_callback:\n   228→                await progress_callback({\n   229→                    \"phase\": \"resolve\",\n   230→                    \"status\": \"completed\",\n   231→                    \"message\": \"Context expansion completed\",\n   232→                    \"summary\": summary,\n   233→                    \"processed\": len(all_post_ids),\n   234→                    \"total\": len(all_post_ids)\n   235→                })\n   236→\n   237→            return {\n   238→                \"enriched_posts\": enriched_posts,\n   239→                \"links_followed\": linked_count,\n   240→                \"additional_posts\": linked_count,\n   241→                \"summary\": summary\n   242→            }\n   243→\n   244→        finally:\n   245→            db.close()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  }
]