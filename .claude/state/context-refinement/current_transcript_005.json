[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_hhtbsaa9rfh",
        "type": "tool_result",
        "content": "     1â†’\"\"\"Database initialization and session management.\"\"\"\n     2â†’\n     3â†’import os\n     4â†’from typing import AsyncGenerator\n     5â†’from contextlib import asynccontextmanager\n     6â†’\n     7â†’from sqlalchemy import Table, Column, Integer, Boolean, Text, TIMESTAMP, ForeignKey, event\n     8â†’from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine\n     9â†’from sqlalchemy.pool import NullPool\n    10â†’\n    11â†’from .base import Base, engine\n    12â†’\n    13â†’\n    14â†’# Define comment_group_drift table (created via SQL migration)\n    15â†’comment_group_drift = Table(\n    16â†’    'comment_group_drift',\n    17â†’    Base.metadata,\n    18â†’    Column('id', Integer, primary_key=True),\n    19â†’    Column('post_id', Integer, ForeignKey('posts.post_id'), nullable=False, unique=True),\n    20â†’    Column('has_drift', Boolean, nullable=False, default=False),\n    21â†’    Column('drift_topics', Text),  # JSON\n    22â†’    Column('analyzed_at', TIMESTAMP, nullable=False),\n    23â†’    Column('analyzed_by', Text, nullable=False),\n    24â†’    Column('expert_id', Text),  # Expert identifier for multi-expert support\n    25â†’    extend_existing=True\n    26â†’)\n    27â†’\n    28â†’\n    29â†’def enable_sqlite_foreign_keys(dbapi_connection, connection_record):\n    30â†’    \"\"\"Enable foreign key support in SQLite.\"\"\"\n    31â†’    cursor = dbapi_connection.cursor()\n    32â†’    cursor.execute(\"PRAGMA foreign_keys=ON\")\n    33â†’    cursor.close()\n    34â†’\n    35â†’\n    36â†’def init_db():\n    37â†’    \"\"\"Initialize database with all tables.\"\"\"\n    38â†’    # Import all models to register them with Base\n    39â†’    from . import Post, Link, Comment\n    40â†’\n    41â†’    # Enable foreign keys for SQLite\n    42â†’    event.listen(engine, \"connect\", enable_sqlite_foreign_keys)\n    43â†’\n    44â†’    # Create all tables\n    45â†’    Base.metadata.create_all(bind=engine)\n    46â†’    print(\"Database initialized successfully!\")\n    47â†’\n    48â†’\n    49â†’def drop_db():\n    50â†’    \"\"\"Drop all tables (use with caution!).\"\"\"\n    51â†’    Base.metadata.drop_all(bind=engine)\n    52â†’    print(\"All tables dropped!\")\n    53â†’\n    54â†’\n    55â†’def reset_db():\n    56â†’    \"\"\"Reset database by dropping and recreating all tables.\"\"\"\n    57â†’    drop_db()\n    58â†’    init_db()\n    59â†’    print(\"Database reset complete!\")\n    60â†’\n    61â†’\n    62â†’# Async support for future use\n    63â†’DATABASE_URL_ASYNC = \"sqlite+aiosqlite:///../data/experts.db\"\n    64â†’\n    65â†’async_engine = create_async_engine(\n    66â†’    DATABASE_URL_ASYNC,\n    67â†’    echo=False,\n    68â†’    poolclass=NullPool,  # Recommended for SQLite\n    69â†’)\n    70â†’\n    71â†’AsyncSessionLocal = async_sessionmaker(\n    72â†’    async_engine,\n    73â†’    class_=AsyncSession,\n    74â†’    expire_on_commit=False\n    75â†’)\n    76â†’\n    77â†’\n    78â†’@asynccontextmanager\n    79â†’async def get_async_session() -> AsyncGenerator[AsyncSession, None]:\n    80â†’    \"\"\"Async context manager for database sessions.\"\"\"\n    81â†’    async with AsyncSessionLocal() as session:\n    82â†’        try:\n    83â†’            yield session\n    84â†’            await session.commit()\n    85â†’        except Exception:\n    86â†’            await session.rollback()\n    87â†’            raise\n    88â†’        finally:\n    89â†’            await session.close()\n    90â†’\n    91â†’\n    92â†’async def init_async_db():\n    93â†’    \"\"\"Initialize database asynchronously.\"\"\"\n    94â†’    async with async_engine.begin() as conn:\n    95â†’        # Enable foreign keys for SQLite\n    96â†’        await conn.execute(\"PRAGMA foreign_keys=ON\")\n    97â†’        # Create all tables\n    98â†’        await conn.run_sync(Base.metadata.create_all)\n    99â†’    print(\"Async database initialized successfully!\")\n   100â†’\n   101â†’\n   102â†’async def drop_async_db():\n   103â†’    \"\"\"Drop all tables asynchronously.\"\"\"\n   104â†’    async with async_engine.begin() as conn:\n   105â†’        await conn.run_sync(Base.metadata.drop_all)\n   106â†’    print(\"All tables dropped (async)!\")\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Now let me read the remaining important files to complete the analysis:\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_nn1usk7doqn",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/data/json_parser.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_hvfs2tmmw6m",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/data/channel_syncer.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_ncwvov3ulm",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/utils/language_utils.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_exqe4fv1dek",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/utils/entities_converter.py"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_nn1usk7doqn",
        "type": "tool_result",
        "content": "     1â†’\"\"\"JSON parser for Telegram channel exports.\"\"\"\n     2â†’\n     3â†’import json\n     4â†’import argparse\n     5â†’import sys\n     6â†’from pathlib import Path\n     7â†’from datetime import datetime\n     8â†’from typing import List, Dict, Any, Optional, Tuple\n     9â†’from collections import defaultdict\n    10â†’\n    11â†’# Add parent directory to path for imports\n    12â†’sys.path.append(str(Path(__file__).parent.parent))\n    13â†’\n    14â†’from models import SessionLocal, Post, Link, LinkType\n    15â†’from models.database import init_db\n    16â†’from utils.entities_converter import entities_to_markdown_from_json\n    17â†’\n    18â†’\n    19â†’class TelegramJsonParser:\n    20â†’    \"\"\"Parser for Telegram JSON export files.\"\"\"\n    21â†’\n    22â†’    def __init__(self, session, expert_id: str):\n    23â†’        \"\"\"Initialize parser with database session.\n    24â†’\n    25â†’        Args:\n    26â†’            session: Database session\n    27â†’            expert_id: Expert identifier (e.g., 'refat', 'ai_architect')\n    28â†’        \"\"\"\n    29â†’        self.session = session\n    30â†’        self.expert_id = expert_id\n    31â†’        self.stats = {\n    32â†’            'posts_imported': 0,\n    33â†’            'links_created': 0,\n    34â†’            'errors': 0,\n    35â†’            'skipped': 0\n    36â†’        }\n    37â†’        # Track posts by telegram_message_id for link resolution\n    38â†’        self.post_id_map = {}\n    39â†’\n    40â†’    def parse_file(self, json_path: str) -> Dict[str, int]:\n    41â†’        \"\"\"Parse Telegram JSON export file and import to database.\"\"\"\n    42â†’        print(f\"Parsing file: {json_path}\")\n    43â†’\n    44â†’        try:\n    45â†’            with open(json_path, 'r', encoding='utf-8') as f:\n    46â†’                data = json.load(f)\n    47â†’        except Exception as e:\n    48â†’            print(f\"Error reading JSON file: {e}\")\n    49â†’            return self.stats\n    50â†’\n    51â†’        # Extract channel/chat info\n    52â†’        channel_info = self._extract_channel_info(data)\n    53â†’\n    54â†’        # Parse messages\n    55â†’        messages = data.get('messages', [])\n    56â†’        total = len(messages)\n    57â†’        print(f\"Found {total} messages to import\")\n    58â†’\n    59â†’        # Process in batches for better performance\n    60â†’        batch_size = 100\n    61â†’        for i in range(0, total, batch_size):\n    62â†’            batch = messages[i:i+batch_size]\n    63â†’            self._process_message_batch(batch, channel_info)\n    64â†’\n    65â†’            # Show progress\n    66â†’            progress = min(i + batch_size, total)\n    67â†’            percent = (progress / total) * 100\n    68â†’            print(f\"Progress: {progress}/{total} ({percent:.1f}%)\")\n    69â†’\n    70â†’        # Commit all changes\n    71â†’        try:\n    72â†’            self.session.commit()\n    73â†’            print(\"âœ… Import completed successfully!\")\n    74â†’        except Exception as e:\n    75â†’            print(f\"Error committing changes: {e}\")\n    76â†’            self.session.rollback()\n    77â†’            self.stats['errors'] += 1\n    78â†’\n    79â†’        # Create links after all posts are imported\n    80â†’        self._create_links(messages)\n    81â†’\n    82â†’        return self.stats\n    83â†’\n    84â†’    def _extract_channel_info(self, data: Dict) -> Dict[str, str]:\n    85â†’        \"\"\"Extract channel/chat information from JSON.\"\"\"\n    86â†’        return {\n    87â†’            'channel_id': str(data.get('id', 'unknown')),\n    88â†’            'channel_name': data.get('name', data.get('title', 'Unknown Channel')),\n    89â†’            'type': data.get('type', 'channel')\n    90â†’        }\n    91â†’\n    92â†’    def _process_message_batch(self, messages: List[Dict], channel_info: Dict):\n    93â†’        \"\"\"Process a batch of messages.\"\"\"\n    94â†’        for msg in messages:\n    95â†’            try:\n    96â†’                post = self._message_to_post(msg, channel_info)\n    97â†’                if post:\n    98â†’                    self.session.add(post)\n    99â†’                    self.session.flush()  # Get the post_id\n   100â†’\n   101â†’                    # Map telegram message ID to our post_id for link resolution\n   102â†’                    if msg.get('id'):\n   103â†’                        self.post_id_map[msg['id']] = post.post_id\n   104â†’\n   105â†’                    self.stats['posts_imported'] += 1\n   106â†’            except Exception as e:\n   107â†’                print(f\"Error processing message {msg.get('id', 'unknown')}: {e}\")\n   108â†’                self.stats['errors'] += 1\n   109â†’\n   110â†’    def _message_to_post(self, msg: Dict, channel_info: Dict) -> Optional[Post]:\n   111â†’        \"\"\"Convert Telegram message to Post model.\"\"\"\n   112â†’        # Skip service messages\n   113â†’        if msg.get('type') != 'message':\n   114â†’            self.stats['skipped'] += 1\n   115â†’            return None\n   116â†’\n   117â†’        # Extract text content\n   118â†’        text = self._extract_text(msg)\n   119â†’        if not text and not msg.get('media_type'):\n   120â†’            self.stats['skipped'] += 1\n   121â†’            return None\n   122â†’\n   123â†’        # Parse timestamps\n   124â†’        created_at = self._parse_timestamp(msg.get('date'))\n   125â†’        edited_at = self._parse_timestamp(msg.get('edited'))\n   126â†’\n   127â†’        # Extract media metadata\n   128â†’        media_metadata = self._extract_media_metadata(msg)\n   129â†’\n   130â†’        # Create Post object\n   131â†’        post = Post(\n   132â†’            channel_id=channel_info['channel_id'],\n   133â†’            channel_name=channel_info['channel_name'],\n   134â†’            telegram_message_id=msg.get('id'),\n   135â†’            expert_id=self.expert_id,\n   136â†’            message_text=text,\n   137â†’            author_name=msg.get('from', msg.get('author', '')),\n   138â†’            author_id=msg.get('from_id', ''),\n   139â†’            created_at=created_at,\n   140â†’            edited_at=edited_at,\n   141â†’            view_count=msg.get('views', 0),\n   142â†’            forward_count=msg.get('forwards', 0),\n   143â†’            media_metadata=media_metadata,\n   144â†’            is_forwarded=1 if msg.get('forwarded_from') else 0,\n   145â†’            forward_from_channel=msg.get('forwarded_from')\n   146â†’        )\n   147â†’\n   148â†’        return post\n   149â†’\n   150â†’    def _extract_text(self, msg: Dict) -> str:\n   151â†’        \"\"\"Extract text content from message and convert entities to markdown.\"\"\"\n   152â†’        text = msg.get('text', '')\n   153â†’\n   154â†’        # Convert entities to markdown using the utility function\n   155â†’        markdown_text = entities_to_markdown_from_json(text)\n   156â†’\n   157â†’        return markdown_text.strip() if markdown_text else ''\n   158â†’\n   159â†’    def _parse_timestamp(self, date_str: Optional[str]) -> Optional[datetime]:\n   160â†’        \"\"\"Parse Telegram timestamp format.\"\"\"\n   161â†’        if not date_str:\n   162â†’            return None\n   163â†’\n   164â†’        try:\n   165â†’            # Telegram uses ISO format: \"2023-01-01T12:00:00\"\n   166â†’            return datetime.fromisoformat(date_str.replace('T', ' '))\n   167â†’        except:\n   168â†’            try:\n   169â†’                # Alternative format\n   170â†’                return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n   171â†’            except:\n   172â†’                return None\n   173â†’\n   174â†’    def _extract_media_metadata(self, msg: Dict) -> Optional[Dict]:\n   175â†’        \"\"\"Extract media metadata from message.\"\"\"\n   176â†’        media_type = msg.get('media_type')\n   177â†’        if not media_type:\n   178â†’            return None\n   179â†’\n   180â†’        metadata = {\n   181â†’            'type': media_type,\n   182â†’            'mime_type': msg.get('mime_type'),\n   183â†’            'file': msg.get('file'),\n   184â†’            'file_name': msg.get('file_name'),\n   185â†’            'duration': msg.get('duration_seconds'),\n   186â†’            'width': msg.get('width'),\n   187â†’            'height': msg.get('height'),\n   188â†’            'thumbnail': msg.get('thumbnail')\n   189â†’        }\n   190â†’\n   191â†’        # Remove None values\n   192â†’        metadata = {k: v for k, v in metadata.items() if v is not None}\n   193â†’\n   194â†’        return metadata if metadata else None\n   195â†’\n   196â†’    def _create_links(self, messages: List[Dict]):\n   197â†’        \"\"\"Create Link records for replies, forwards, and text links.\"\"\"\n   198â†’        print(\"\\nCreating links between posts...\")\n   199â†’        import re\n   200â†’\n   201â†’        for msg in messages:\n   202â†’            # Skip non-messages\n   203â†’            if msg.get('type') != 'message' or msg.get('id') not in self.post_id_map:\n   204â†’                continue\n   205â†’\n   206â†’            source_post_id = self.post_id_map[msg['id']]\n   207â†’\n   208â†’            # Handle replies\n   209â†’            reply_to = msg.get('reply_to_message_id')\n   210â†’            if reply_to and reply_to in self.post_id_map:\n   211â†’                target_post_id = self.post_id_map[reply_to]\n   212â†’                self._create_link(source_post_id, target_post_id, LinkType.REPLY)\n   213â†’\n   214â†’            # Handle text_link elements (links in text to other posts)\n   215â†’            text = msg.get('text')\n   216â†’            if isinstance(text, list):\n   217â†’                for item in text:\n   218â†’                    if isinstance(item, dict) and item.get('type') == 'text_link':\n   219â†’                        href = item.get('href', '')\n   220â†’                        # Extract post ID from t.me links (e.g., https://t.me/channelname/123)\n   221â†’                        match = re.search(r't\\.me/[\\w]+/(\\d+)', href)\n   222â†’                        if match:\n   223â†’                            linked_msg_id = int(match.group(1))\n   224â†’                            if linked_msg_id in self.post_id_map:\n   225â†’                                target_post_id = self.post_id_map[linked_msg_id]\n   226â†’                                self._create_link(source_post_id, target_post_id, LinkType.MENTION)\n   227â†’\n   228â†’            # Handle forwards (if we want to track them as links)\n   229â†’            if msg.get('forwarded_from'):\n   230â†’                # For now, we just store the forward source in the post\n   231â†’                # Could create links if we have the original posts\n   232â†’                pass\n   233â†’\n   234â†’        try:\n   235â†’            self.session.commit()\n   236â†’            print(f\"âœ… Created {self.stats['links_created']} links\")\n   237â†’        except Exception as e:\n   238â†’            print(f\"Error creating links: {e}\")\n   239â†’            self.session.rollback()\n   240â†’\n   241â†’    def _create_link(self, source_id: int, target_id: int, link_type: LinkType):\n   242â†’        \"\"\"Create a link between two posts.\"\"\"\n   243â†’        # Check if link already exists\n   244â†’        existing = self.session.query(Link).filter(\n   245â†’            Link.source_post_id == source_id,\n   246â†’            Link.target_post_id == target_id,\n   247â†’            Link.link_type == link_type.value\n   248â†’        ).first()\n   249â†’\n   250â†’        if existing:\n   251â†’            return  # Skip duplicate\n   252â†’\n   253â†’        try:\n   254â†’            link = Link(\n   255â†’                source_post_id=source_id,\n   256â†’                target_post_id=target_id,\n   257â†’                link_type=link_type.value,\n   258â†’                created_at=datetime.utcnow()\n   259â†’            )\n   260â†’            self.session.add(link)\n   261â†’            self.session.flush()  # Flush to catch constraint errors immediately\n   262â†’            self.stats['links_created'] += 1\n   263â†’        except Exception as e:\n   264â†’            self.session.rollback()\n   265â†’            # Ignore duplicate link errors\n   266â†’            if 'UNIQUE constraint failed' not in str(e):\n   267â†’                print(f\"Error creating link: {e}\")\n   268â†’\n   269â†’\n   270â†’def main():\n   271â†’    \"\"\"CLI interface for JSON parser.\"\"\"\n   272â†’    parser = argparse.ArgumentParser(description='Import Telegram JSON exports to database')\n   273â†’    parser.add_argument('json_file', help='Path to Telegram JSON export file')\n   274â†’    parser.add_argument('--init-db', action='store_true',\n   275â†’                       help='Initialize database before import')\n   276â†’    parser.add_argument('--dry-run', action='store_true',\n   277â†’                       help='Parse without saving to database')\n   278â†’    parser.add_argument('--expert-id', required=True,\n   279â†’                       help='Expert identifier (e.g., refat, ai_architect)')\n   280â†’\n   281â†’    args = parser.parse_args()\n   282â†’\n   283â†’    # Validate file exists\n   284â†’    if not Path(args.json_file).exists():\n   285â†’        print(f\"Error: File not found: {args.json_file}\")\n   286â†’        return 1\n   287â†’\n   288â†’    # Initialize database if requested\n   289â†’    if args.init_db:\n   290â†’        print(\"Initializing database...\")\n   291â†’        init_db()\n   292â†’\n   293â†’    # Create database session\n   294â†’    session = SessionLocal()\n   295â†’\n   296â†’    try:\n   297â†’        # Parse and import\n   298â†’        parser = TelegramJsonParser(session, expert_id=args.expert_id)\n   299â†’        stats = parser.parse_file(args.json_file)\n   300â†’\n   301â†’        # Print statistics\n   302â†’        print(\"\\n\" + \"=\"*50)\n   303â†’        print(\"Import Statistics:\")\n   304â†’        print(f\"  Posts imported: {stats['posts_imported']}\")\n   305â†’        print(f\"  Links created: {stats['links_created']}\")\n   306â†’        print(f\"  Messages skipped: {stats['skipped']}\")\n   307â†’        print(f\"  Errors: {stats['errors']}\")\n   308â†’        print(\"=\"*50)\n   309â†’\n   310â†’    finally:\n   311â†’        session.close()\n   312â†’\n   313â†’    return 0\n   314â†’\n   315â†’\n   316â†’if __name__ == \"__main__\":\n   317â†’    exit(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_hvfs2tmmw6m",
        "type": "tool_result",
        "content": "     1â†’\"\"\"\n     2â†’Telegram Channel Syncer\n     3â†’Incremental sync of Telegram channel posts and comments.\n     4â†’\n     5â†’Usage:\n     6â†’    from src.data.channel_syncer import TelegramChannelSyncer\n     7â†’\n     8â†’    syncer = TelegramChannelSyncer(api_id, api_hash, session_name)\n     9â†’    result = await syncer.sync_channel_incremental(channel_username, dry_run=False)\n    10â†’\"\"\"\n    11â†’\n    12â†’import asyncio\n    13â†’import sys\n    14â†’from pathlib import Path\n    15â†’from datetime import datetime\n    16â†’from typing import List, Dict, Any, Optional\n    17â†’import os\n    18â†’\n    19â†’from telethon import TelegramClient\n    20â†’from telethon.errors import FloodWaitError, ChannelPrivateError\n    21â†’from telethon.tl.types import Message\n    22â†’from sqlalchemy.orm import Session\n    23â†’from sqlalchemy import text\n    24â†’\n    25â†’# Add parent directory to path for imports\n    26â†’sys.path.append(str(Path(__file__).parent.parent))\n    27â†’\n    28â†’from models.base import SessionLocal\n    29â†’from models.post import Post\n    30â†’from models.comment import Comment\n    31â†’from models.link import Link, LinkType\n    32â†’from data.telegram_comments_fetcher import SafeTelegramCommentsFetcher\n    33â†’from utils.entities_converter import entities_to_markdown_from_telethon\n    34â†’\n    35â†’\n    36â†’class TelegramChannelSyncer(SafeTelegramCommentsFetcher):\n    37â†’    \"\"\"\n    38â†’    Extends SafeTelegramCommentsFetcher with incremental sync capabilities.\n    39â†’\n    40â†’    Features:\n    41â†’    - Fetch new posts since last sync (incremental)\n    42â†’    - Update comments for recent posts (depth=N)\n    43â†’    - Track sync state in database\n    44â†’    - Calculate new comment groups needing drift analysis\n    45â†’    \"\"\"\n    46â†’\n    47â†’    RECENT_POSTS_DEPTH = 10  # Check last 10 posts for new comments (~30 days)\n    48â†’\n    49â†’    def __init__(self, api_id: int, api_hash: str, session_name: str = 'telegram_fetcher'):\n    50â†’        \"\"\"Initialize syncer with Telegram credentials.\"\"\"\n    51â†’        super().__init__(api_id, api_hash, session_name)\n    52â†’        self.stats = {\n    53â†’            'new_posts_found': 0,\n    54â†’            'new_posts_saved': 0,\n    55â†’            'posts_updated': 0,\n    56â†’            'new_comments_found': 0,\n    57â†’            'new_comments_saved': 0,\n    58â†’            'errors': 0,\n    59â†’            'duration_seconds': 0\n    60â†’        }\n    61â†’\n    62â†’    async def sync_channel_incremental(\n    63â†’        self,\n    64â†’        channel_username: str,\n    65â†’        expert_id: str,\n    66â†’        dry_run: bool = False\n    67â†’    ) -> Dict[str, Any]:\n    68â†’        \"\"\"\n    69â†’        Fetch new posts and comments since last sync.\n    70â†’\n    71â†’        Args:\n    72â†’            channel_username: Channel to sync (e.g., 'nobilix')\n    73â†’            expert_id: Expert identifier (e.g., 'refat', 'ai_architect')\n    74â†’            dry_run: If True, don't save to database\n    75â†’\n    76â†’        Returns:\n    77â†’            {\n    78â†’                \"success\": bool,\n    79â†’                \"new_posts\": List[int],  # telegram_message_ids\n    80â†’                \"updated_posts\": List[int],\n    81â†’                \"new_comment_groups\": List[int],\n    82â†’                \"stats\": {...},\n    83â†’                \"errors\": List[str]\n    84â†’            }\n    85â†’        \"\"\"\n    86â†’        start_time = datetime.utcnow()\n    87â†’        errors = []\n    88â†’        new_posts = []\n    89â†’        updated_posts = []\n    90â†’\n    91â†’        print(\"=\" * 60)\n    92â†’        print(\"ðŸ”„ Telegram Channel Sync\")\n    93â†’        print(\"=\" * 60)\n    94â†’        print(f\"Mode: {'DRY-RUN' if dry_run else 'LIVE'}\")\n    95â†’        print(f\"Channel: {channel_username}\")\n    96â†’        print()\n    97â†’\n    98â†’        # Connect to Telegram\n    99â†’        print(\"ðŸ“± Connecting to Telegram...\")\n   100â†’        self.client = TelegramClient(self.session_name, self.api_id, self.api_hash)\n   101â†’        await self.client.start()\n   102â†’        print(\"âœ… Connected!\")\n   103â†’\n   104â†’        # Get channel entity\n   105â†’        print(f\"\\nðŸ” Finding channel: {channel_username}\")\n   106â†’        try:\n   107â†’            channel = await self.client.get_entity(channel_username)\n   108â†’            print(f\"âœ… Found: {channel.title}\")\n   109â†’        except Exception as e:\n   110â†’            error_msg = f\"Cannot find channel {channel_username}: {e}\"\n   111â†’            print(f\"âŒ {error_msg}\")\n   112â†’            errors.append(error_msg)\n   113â†’            return self._build_result(False, new_posts, updated_posts, [], errors, start_time)\n   114â†’\n   115â†’        # Open database\n   116â†’        db = SessionLocal()\n   117â†’        try:\n   118â†’            # Get last synced message ID\n   119â†’            last_synced_id = self._get_last_synced_id(db, channel_username)\n   120â†’            print(f\"\\nðŸ“Š Last synced message ID: {last_synced_id or 'None (first sync)'}\")\n   121â†’\n   122â†’            # Fetch new posts\n   123â†’            print(f\"\\nðŸ“¥ Fetching new posts (min_id={last_synced_id or 0})...\")\n   124â†’            new_posts_data = await self._fetch_new_posts(channel, last_synced_id)\n   125â†’            self.stats['new_posts_found'] = len(new_posts_data)\n   126â†’            print(f\"âœ… Found {len(new_posts_data)} new posts\")\n   127â†’\n   128â†’            if new_posts_data:\n   129â†’                # Save new posts to database\n   130â†’                if not dry_run:\n   131â†’                    new_posts = self._save_posts_to_db(db, new_posts_data, channel, expert_id)\n   132â†’                    self.stats['new_posts_saved'] = len(new_posts)\n   133â†’                    db.commit()  # Commit now so new posts are visible to next query\n   134â†’                    print(f\"âœ… Saved {len(new_posts)} new posts\")\n   135â†’                else:\n   136â†’                    new_posts = [p['telegram_message_id'] for p in new_posts_data]\n   137â†’                    print(f\"[DRY-RUN] Would save {len(new_posts)} posts\")\n   138â†’\n   139â†’            # Update recent posts comments\n   140â†’            print(f\"\\nðŸ”„ Updating comments for last {self.RECENT_POSTS_DEPTH} posts...\")\n   141â†’            updated_result = await self.update_recent_posts_comments(\n   142â†’                db=db,\n   143â†’                channel_username=channel_username,\n   144â†’                expert_id=expert_id,\n   145â†’                depth=self.RECENT_POSTS_DEPTH,\n   146â†’                dry_run=dry_run\n   147â†’            )\n   148â†’            updated_posts = updated_result.get('updated_posts', [])\n   149â†’            self.stats['posts_updated'] = len(updated_posts)\n   150â†’            self.stats['new_comments_found'] += updated_result.get('stats', {}).get('comments_found', 0)\n   151â†’            self.stats['new_comments_saved'] += updated_result.get('stats', {}).get('comments_saved', 0)\n   152â†’\n   153â†’            # Update comments for NEW posts (if any)\n   154â†’            if new_posts:\n   155â†’                print(f\"\\nðŸ”„ Updating comments for {len(new_posts)} new posts...\")\n   156â†’                new_posts_result = await self.update_specific_posts_comments(\n   157â†’                    db=db,\n   158â†’                    channel_username=channel_username,\n   159â†’                    expert_id=expert_id,\n   160â†’                    telegram_message_ids=new_posts,\n   161â†’                    dry_run=dry_run\n   162â†’                )\n   163â†’                new_posts_updated = new_posts_result.get('updated_posts', [])\n   164â†’                # Merge updated posts lists\n   165â†’                updated_posts.extend(new_posts_updated)\n   166â†’                updated_posts = list(set(updated_posts))  # Remove duplicates\n   167â†’                self.stats['posts_updated'] = len(updated_posts)\n   168â†’                self.stats['new_comments_found'] += new_posts_result.get('stats', {}).get('comments_found', 0)\n   169â†’                self.stats['new_comments_saved'] += new_posts_result.get('stats', {}).get('comments_saved', 0)\n   170â†’\n   171â†’            # Update sync state\n   172â†’            if not dry_run and new_posts:\n   173â†’                max_message_id = max(new_posts)\n   174â†’                self._update_sync_state(db, channel_username, max_message_id)\n   175â†’                print(f\"âœ… Updated sync state: last_synced_message_id = {max_message_id}\")\n   176â†’\n   177â†’            # Calculate new comment groups needing drift analysis for this expert\n   178â†’            new_comment_groups = self.calculate_new_comment_groups(db, expert_id)\n   179â†’            print(f\"\\nðŸŽ¯ Comment groups needing drift analysis: {len(new_comment_groups)}\")\n   180â†’\n   181â†’            # Commit all changes\n   182â†’            if not dry_run:\n   183â†’                db.commit()\n   184â†’                print(\"âœ… Database committed\")\n   185â†’\n   186â†’            # Calculate duration\n   187â†’            duration = (datetime.utcnow() - start_time).total_seconds()\n   188â†’            self.stats['duration_seconds'] = duration\n   189â†’\n   190â†’            print(f\"\\nâœ… Sync complete! Duration: {duration:.1f}s\")\n   191â†’\n   192â†’            return self._build_result(True, new_posts, updated_posts, new_comment_groups, errors, start_time)\n   193â†’\n   194â†’        except Exception as e:\n   195â†’            db.rollback()\n   196â†’            error_msg = f\"Sync failed: {e}\"\n   197â†’            print(f\"\\nâŒ {error_msg}\")\n   198â†’            errors.append(error_msg)\n   199â†’            self.stats['errors'] += 1\n   200â†’            return self._build_result(False, new_posts, updated_posts, [], errors, start_time)\n   201â†’        finally:\n   202â†’            db.close()\n   203â†’            await self.client.disconnect()\n   204â†’\n   205â†’    async def update_recent_posts_comments(\n   206â†’        self,\n   207â†’        db: Session,\n   208â†’        channel_username: str,\n   209â†’        expert_id: str,\n   210â†’        depth: int = 10,\n   211â†’        dry_run: bool = False\n   212â†’    ) -> Dict[str, Any]:\n   213â†’        \"\"\"\n   214â†’        Refresh comments for last N posts for specific expert.\n   215â†’\n   216â†’        Args:\n   217â†’            db: Database session\n   218â†’            channel_username: Channel to sync\n   219â†’            expert_id: Expert identifier for filtering posts\n   220â†’            depth: How many recent posts to check\n   221â†’            dry_run: If True, don't save to database\n   222â†’\n   223â†’        Returns:\n   224â†’            {\n   225â†’                \"updated_posts\": List[int],  # telegram_message_ids\n   226â†’                \"stats\": {\"comments_found\": int, \"comments_saved\": int}\n   227â†’            }\n   228â†’        \"\"\"\n   229â†’        updated_posts = []\n   230â†’        comments_found = 0\n   231â†’        comments_saved = 0\n   232â†’\n   233â†’        # Get last N posts for this expert only\n   234â†’        recent_posts = db.query(Post.post_id, Post.telegram_message_id, Post.channel_id)\\\n   235â†’            .filter(Post.expert_id == expert_id)\\\n   236â†’            .order_by(Post.telegram_message_id.desc())\\\n   237â†’            .limit(depth)\\\n   238â†’            .all()\n   239â†’\n   240â†’        if not recent_posts:\n   241â†’            return {\"updated_posts\": [], \"stats\": {\"comments_found\": 0, \"comments_saved\": 0}}\n   242â†’\n   243â†’        # Get channel entity\n   244â†’        channel = await self.client.get_entity(channel_username)\n   245â†’\n   246â†’        # Fetch comments for each post\n   247â†’        for post_id, telegram_message_id, channel_id in recent_posts:\n   248â†’            print(f\"  ðŸ“ Post #{telegram_message_id}...\", end=\" \")\n   249â†’\n   250â†’            comments = await self.get_discussion_replies(channel, telegram_message_id)\n   251â†’            comments_found += len(comments)\n   252â†’\n   253â†’            if comments:\n   254â†’                if not dry_run:\n   255â†’                    # Save comments with proper channel_id filtering\n   256â†’                    saved = self.save_comments_to_db(db, comments, channel_id)\n   257â†’                    comments_saved += saved\n   258â†’                    db.commit()\n   259â†’                    if saved == len(comments):\n   260â†’                        print(f\"âœ… {saved} comments\")\n   261â†’                    elif saved > 0:\n   262â†’                        print(f\"âœ… {saved}/{len(comments)} comments (skipped {len(comments) - saved} duplicates)\")\n   263â†’                    else:\n   264â†’                        print(f\"âš ï¸  0/{len(comments)} comments (all duplicates)\")\n   265â†’                else:\n   266â†’                    print(f\"[DRY-RUN] Would save {len(comments)} comments\")\n   267â†’\n   268â†’                updated_posts.append(telegram_message_id)\n   269â†’            else:\n   270â†’                print(\"No comments\")\n   271â†’\n   272â†’            # Rate limiting\n   273â†’            await asyncio.sleep(self.DELAY_BETWEEN_POSTS)\n   274â†’\n   275â†’        return {\n   276â†’            \"updated_posts\": updated_posts,\n   277â†’            \"stats\": {\n   278â†’                \"comments_found\": comments_found,\n   279â†’                \"comments_saved\": comments_saved\n   280â†’            }\n   281â†’        }\n   282â†’\n   283â†’    async def update_specific_posts_comments(\n   284â†’        self,\n   285â†’        db: Session,\n   286â†’        channel_username: str,\n   287â†’        expert_id: str,\n   288â†’        telegram_message_ids: List[int],\n   289â†’        dry_run: bool = False\n   290â†’    ) -> Dict[str, Any]:\n   291â†’        \"\"\"\n   292â†’        Refresh comments for specific posts by telegram_message_ids.\n   293â†’\n   294â†’        Args:\n   295â†’            db: Database session\n   296â†’            channel_username: Channel to sync\n   297â†’            expert_id: Expert identifier for filtering posts\n   298â†’            telegram_message_ids: List of specific telegram_message_ids to check\n   299â†’            dry_run: If True, don't save to database\n   300â†’\n   301â†’        Returns:\n   302â†’            {\n   303â†’                \"updated_posts\": List[int],  # telegram_message_ids\n   304â†’                \"stats\": {\"comments_found\": int, \"comments_saved\": int}\n   305â†’            }\n   306â†’        \"\"\"\n   307â†’        updated_posts = []\n   308â†’        comments_found = 0\n   309â†’        comments_saved = 0\n   310â†’\n   311â†’        if not telegram_message_ids:\n   312â†’            return {\"updated_posts\": [], \"stats\": {\"comments_found\": 0, \"comments_saved\": 0}}\n   313â†’\n   314â†’        # Get posts from database for these telegram_message_ids\n   315â†’        posts_data = db.query(Post.post_id, Post.telegram_message_id, Post.channel_id)\\\n   316â†’            .filter(Post.expert_id == expert_id)\\\n   317â†’            .filter(Post.telegram_message_id.in_(telegram_message_ids))\\\n   318â†’            .all()\n   319â†’\n   320â†’        if not posts_data:\n   321â†’            return {\"updated_posts\": [], \"stats\": {\"comments_found\": 0, \"comments_saved\": 0}}\n   322â†’\n   323â†’        # Get channel entity\n   324â†’        channel = await self.client.get_entity(channel_username)\n   325â†’\n   326â†’        # Create a dict for quick lookup\n   327â†’        posts_dict = {msg_id: (post_id, channel_id) for post_id, msg_id, channel_id in posts_data}\n   328â†’\n   329â†’        # Fetch comments for each specific post\n   330â†’        for telegram_message_id in telegram_message_ids:\n   331â†’            if telegram_message_id not in posts_dict:\n   332â†’                continue  # Skip if not found in DB\n   333â†’\n   334â†’            post_id, channel_id = posts_dict[telegram_message_id]\n   335â†’            print(f\"  ðŸ“ New Post #{telegram_message_id}...\", end=\" \")\n   336â†’\n   337â†’            comments = await self.get_discussion_replies(channel, telegram_message_id)\n   338â†’            comments_found += len(comments)\n   339â†’\n   340â†’            if comments:\n   341â†’                if not dry_run:\n   342â†’                    # Save comments with proper channel_id filtering\n   343â†’                    saved = self.save_comments_to_db(db, comments, channel_id)\n   344â†’                    comments_saved += saved\n   345â†’                    db.commit()\n   346â†’                    if saved == len(comments):\n   347â†’                        print(f\"âœ… {saved} comments\")\n   348â†’                    elif saved > 0:\n   349â†’                        print(f\"âœ… {saved}/{len(comments)} comments (skipped {len(comments) - saved} duplicates)\")\n   350â†’                    else:\n   351â†’                        print(f\"âš ï¸  0/{len(comments)} comments (all duplicates)\")\n   352â†’                else:\n   353â†’                    print(f\"[DRY-RUN] Would save {len(comments)} comments\")\n   354â†’\n   355â†’                updated_posts.append(telegram_message_id)\n   356â†’            else:\n   357â†’                print(\"No comments\")\n   358â†’\n   359â†’            # Rate limiting\n   360â†’            await asyncio.sleep(self.DELAY_BETWEEN_POSTS)\n   361â†’\n   362â†’        return {\n   363â†’            \"updated_posts\": updated_posts,\n   364â†’            \"stats\": {\n   365â†’                \"comments_found\": comments_found,\n   366â†’                \"comments_saved\": comments_saved\n   367â†’            }\n   368â†’        }\n   369â†’\n   370â†’    def calculate_new_comment_groups(self, db: Session, expert_id: str) -> List[int]:\n   371â†’        \"\"\"\n   372â†’        Find posts with new comments that need drift analysis for specific expert.\n   373â†’\n   374â†’        Args:\n   375â†’            db: Database session\n   376â†’            expert_id: Expert identifier to filter posts\n   377â†’\n   378â†’        Returns:\n   379â†’            List of post_ids that need drift analysis (new comments since last sync)\n   380â†’        \"\"\"\n   381â†’        # Find posts with comments but no drift records OR posts that need drift analysis reset\n   382â†’        query = text(\"\"\"\n   383â†’            SELECT DISTINCT c.post_id\n   384â†’            FROM comments c\n   385â†’            JOIN posts p ON c.post_id = p.post_id\n   386â†’            LEFT JOIN comment_group_drift cgd ON c.post_id = cgd.post_id\n   387â†’            WHERE c.telegram_comment_id IS NOT NULL\n   388â†’              AND p.expert_id = :expert_id\n   389â†’              AND (\n   390â†’                cgd.post_id IS NULL  -- No drift record exists\n   391â†’                OR cgd.analyzed_by != 'pending'  -- Already analyzed, needs reset for new comments\n   392â†’              )\n   393â†’            GROUP BY c.post_id\n   394â†’            HAVING COUNT(c.comment_id) >= 1\n   395â†’            ORDER BY c.post_id\n   396â†’        \"\"\")\n   397â†’\n   398â†’        result = db.execute(query, {\"expert_id\": expert_id})\n   399â†’        post_ids = [row[0] for row in result.fetchall()]\n   400â†’        return post_ids\n   401â†’\n   402â†’    async def _fetch_new_posts(\n   403â†’        self,\n   404â†’        channel,\n   405â†’        min_id: Optional[int] = None\n   406â†’    ) -> List[Dict[str, Any]]:\n   407â†’        \"\"\"\n   408â†’        Fetch new posts from Telegram channel.\n   409â†’\n   410â†’        Args:\n   411â†’            channel: Telegram channel entity\n   412â†’            min_id: Only fetch messages with ID > min_id\n   413â†’\n   414â†’        Returns:\n   415â†’            List of post data dicts\n   416â†’        \"\"\"\n   417â†’        posts = []\n   418â†’\n   419â†’        async for message in self.client.iter_messages(\n   420â†’            entity=channel,\n   421â†’            min_id=min_id or 0,\n   422â†’            reverse=True,  # Chronological order (oldest first)\n   423â†’            limit=None     # All new messages\n   424â†’        ):\n   425â†’            if not message.message:  # Skip non-text messages\n   426â†’                continue\n   427â†’\n   428â†’            # Extract author info\n   429â†’            author_name = self._get_author_name(message)\n   430â†’            author_id = None\n   431â†’            if message.from_id:\n   432â†’                if hasattr(message.from_id, 'user_id'):\n   433â†’                    author_id = str(message.from_id.user_id)\n   434â†’                elif hasattr(message.from_id, 'channel_id'):\n   435â†’                    author_id = str(message.from_id.channel_id)\n   436â†’\n   437â†’            # Extract forward info\n   438â†’            is_forwarded = 0\n   439â†’            forward_from_channel = None\n   440â†’            if message.forward:\n   441â†’                is_forwarded = 1\n   442â†’                if hasattr(message.forward, 'from_id') and hasattr(message.forward.from_id, 'channel_id'):\n   443â†’                    forward_from_channel = str(message.forward.from_id.channel_id)\n   444â†’\n   445â†’            # Convert message entities to markdown\n   446â†’            message_markdown = entities_to_markdown_from_telethon(\n   447â†’                message.message,\n   448â†’                message.entities\n   449â†’            )\n   450â†’\n   451â†’            posts.append({\n   452â†’                'telegram_message_id': message.id,\n   453â†’                'message_text': message_markdown,\n   454â†’                'author_name': author_name,\n   455â†’                'author_id': author_id,\n   456â†’                'created_at': message.date,\n   457â†’                'edited_at': message.edit_date,\n   458â†’                'view_count': message.views or 0,\n   459â†’                'forward_count': message.forwards or 0,\n   460â†’                'reply_count': message.replies.replies if message.replies else 0,\n   461â†’                'is_forwarded': is_forwarded,\n   462â†’                'forward_from_channel': forward_from_channel,\n   463â†’                'reply_to_msg_id': message.reply_to.reply_to_msg_id if message.reply_to else None\n   464â†’            })\n   465â†’\n   466â†’            # Rate limiting\n   467â†’            await asyncio.sleep(self.DELAY_BETWEEN_POSTS)\n   468â†’\n   469â†’        return posts\n   470â†’\n   471â†’    def _save_posts_to_db(\n   472â†’        self,\n   473â†’        db: Session,\n   474â†’        posts_data: List[Dict[str, Any]],\n   475â†’        channel,\n   476â†’        expert_id: str\n   477â†’    ) -> List[int]:\n   478â†’        \"\"\"\n   479â†’        Save posts to database.\n   480â†’\n   481â†’        Args:\n   482â†’            db: Database session\n   483â†’            posts_data: List of post data dicts\n   484â†’            channel: Telegram channel entity\n   485â†’            expert_id: Expert identifier\n   486â†’\n   487â†’        Returns:\n   488â†’            List of saved telegram_message_ids\n   489â†’        \"\"\"\n   490â†’        saved_ids = []\n   491â†’\n   492â†’        for post_data in posts_data:\n   493â†’            # Create Post record\n   494â†’            post = Post(\n   495â†’                channel_id=str(channel.id),\n   496â†’                channel_name=channel.title,\n   497â†’                telegram_message_id=post_data['telegram_message_id'],\n   498â†’                expert_id=expert_id,\n   499â†’                message_text=post_data['message_text'],\n   500â†’                author_name=post_data['author_name'],\n   501â†’                author_id=post_data['author_id'],\n   502â†’                created_at=post_data['created_at'],\n   503â†’                edited_at=post_data['edited_at'],\n   504â†’                view_count=post_data['view_count'],\n   505â†’                forward_count=post_data['forward_count'],\n   506â†’                reply_count=post_data['reply_count'],\n   507â†’                is_forwarded=post_data['is_forwarded'],\n   508â†’                forward_from_channel=post_data['forward_from_channel']\n   509â†’            )\n   510â†’\n   511â†’            db.add(post)\n   512â†’            db.flush()  # Get post_id\n   513â†’\n   514â†’            # Create REPLY link if this is a reply\n   515â†’            if post_data['reply_to_msg_id']:\n   516â†’                target_post = db.query(Post).filter(\n   517â†’                    Post.telegram_message_id == post_data['reply_to_msg_id']\n   518â†’                ).first()\n   519â†’\n   520â†’                if target_post:\n   521â†’                    link = Link(\n   522â†’                        source_post_id=post.post_id,\n   523â†’                        target_post_id=target_post.post_id,\n   524â†’                        link_type=LinkType.REPLY.value,\n   525â†’                        created_at=datetime.utcnow()\n   526â†’                    )\n   527â†’                    db.add(link)\n   528â†’\n   529â†’            saved_ids.append(post_data['telegram_message_id'])\n   530â†’\n   531â†’        return saved_ids\n   532â†’\n   533â†’    def _get_last_synced_id(self, db: Session, channel_username: str) -> Optional[int]:\n   534â†’        \"\"\"Get last synced message ID from sync_state table.\"\"\"\n   535â†’        result = db.execute(\n   536â†’            text(\"SELECT last_synced_message_id FROM sync_state WHERE channel_username = :channel\"),\n   537â†’            {\"channel\": channel_username}\n   538â†’        ).fetchone()\n   539â†’\n   540â†’        return result[0] if result else None\n   541â†’\n   542â†’    def _update_sync_state(\n   543â†’        self,\n   544â†’        db: Session,\n   545â†’        channel_username: str,\n   546â†’        max_message_id: int\n   547â†’    ):\n   548â†’        \"\"\"Update sync state with new max message ID.\"\"\"\n   549â†’        # Check if record exists\n   550â†’        exists = db.execute(\n   551â†’            text(\"SELECT 1 FROM sync_state WHERE channel_username = :channel\"),\n   552â†’            {\"channel\": channel_username}\n   553â†’        ).fetchone()\n   554â†’\n   555â†’        if exists:\n   556â†’            db.execute(\n   557â†’                text(\"\"\"UPDATE sync_state\n   558â†’                        SET last_synced_message_id = :max_id,\n   559â†’                            last_synced_at = :now,\n   560â†’                            total_posts_synced = total_posts_synced + :new_posts,\n   561â†’                            total_comments_synced = total_comments_synced + :new_comments\n   562â†’                        WHERE channel_username = :channel\"\"\"),\n   563â†’                {\n   564â†’                    \"max_id\": max_message_id,\n   565â†’                    \"now\": datetime.utcnow(),\n   566â†’                    \"new_posts\": self.stats['new_posts_saved'],\n   567â†’                    \"new_comments\": self.stats['new_comments_saved'],\n   568â†’                    \"channel\": channel_username\n   569â†’                }\n   570â†’            )\n   571â†’        else:\n   572â†’            db.execute(\n   573â†’                text(\"\"\"INSERT INTO sync_state\n   574â†’                        (channel_username, last_synced_message_id, last_synced_at, total_posts_synced, total_comments_synced)\n   575â†’                        VALUES (:channel, :max_id, :now, :new_posts, :new_comments)\"\"\"),\n   576â†’                {\n   577â†’                    \"channel\": channel_username,\n   578â†’                    \"max_id\": max_message_id,\n   579â†’                    \"now\": datetime.utcnow(),\n   580â†’                    \"new_posts\": self.stats['new_posts_saved'],\n   581â†’                    \"new_comments\": self.stats['new_comments_saved']\n   582â†’                }\n   583â†’            )\n   584â†’\n   585â†’    def _build_result(\n   586â†’        self,\n   587â†’        success: bool,\n   588â†’        new_posts: List[int],\n   589â†’        updated_posts: List[int],\n   590â†’        new_comment_groups: List[int],\n   591â†’        errors: List[str],\n   592â†’        start_time: datetime\n   593â†’    ) -> Dict[str, Any]:\n   594â†’        \"\"\"Build standardized result dictionary.\"\"\"\n   595â†’        duration = (datetime.utcnow() - start_time).total_seconds()\n   596â†’        self.stats['duration_seconds'] = duration\n   597â†’\n   598â†’        return {\n   599â†’            \"success\": success,\n   600â†’            \"new_posts\": new_posts,\n   601â†’            \"updated_posts\": updated_posts,\n   602â†’            \"new_comment_groups\": new_comment_groups,\n   603â†’            \"stats\": {\n   604â†’                \"total_posts\": len(new_posts),\n   605â†’                \"total_comments\": self.stats['new_comments_saved'],\n   606â†’                \"groups_need_drift\": len(new_comment_groups),\n   607â†’                \"duration_seconds\": duration\n   608â†’            },\n   609â†’            \"errors\": errors\n   610â†’        }\n   611â†’\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_ncwvov3ulm",
        "type": "tool_result",
        "content": "     1â†’\"\"\"\n     2â†’Language detection and instruction utilities for multi-lingual support.\n     3â†’Determines query language and enforces consistent language across all LLM calls.\n     4â†’\"\"\"\n     5â†’\n     6â†’import re\n     7â†’from typing import Tuple\n     8â†’\n     9â†’\n    10â†’def detect_query_language(query: str) -> str:\n    11â†’    \"\"\"\n    12â†’    Detect the primary language of a query.\n    13â†’\n    14â†’    Args:\n    15â†’        query: User's query string\n    16â†’\n    17â†’    Returns:\n    18â†’        \"English\" or \"Russian\"\n    19â†’    \"\"\"\n    20â†’    if not query or not query.strip():\n    21â†’        return \"Russian\"  # Default to Russian\n    22â†’\n    23â†’    # Remove URLs, mentions, hashtags, and special characters\n    24â†’    cleaned_query = re.sub(r'http[s]?://\\S+|@\\w+|#\\w+', '', query)\n    25â†’    cleaned_query = re.sub(r'[^\\w\\s]', ' ', cleaned_query)\n    26â†’\n    27â†’    # Count alphabetic characters\n    28â†’    english_chars = sum(1 for c in cleaned_query if c.isascii() and c.isalpha())\n    29â†’    cyrillic_chars = sum(1 for c in cleaned_query if '\\u0400' <= c <= '\\u04FF')\n    30â†’    total_chars = english_chars + cyrillic_chars\n    31â†’\n    32â†’    if total_chars == 0:\n    33â†’        return \"Russian\"  # Default for empty or only special chars\n    34â†’\n    35â†’    # Count words for more accurate detection\n    36â†’    words = cleaned_query.split()\n    37â†’    english_words = 0\n    38â†’    cyrillic_words = 0\n    39â†’\n    40â†’    for word in words:\n    41â†’        if any('\\u0400' <= c <= '\\u04FF' for c in word):\n    42â†’            cyrillic_words += 1\n    43â†’        elif any(c.isalpha() for c in word):\n    44â†’            english_words += 1\n    45â†’\n    46â†’    total_words = english_words + cyrillic_words\n    47â†’\n    48â†’    # More sensitive detection:\n    49â†’    # If ANY English words present AND >50% of characters are English â†’ English\n    50â†’    # OR If majority of words are English â†’ English\n    51â†’    if total_words > 0:\n    52â†’        word_ratio = english_words / total_words\n    53â†’        char_ratio = english_chars / total_chars if total_chars > 0 else 0\n    54â†’\n    55â†’        # Lower thresholds for English detection\n    56â†’        if (english_words > 0 and char_ratio >= 0.5) or word_ratio >= 0.5:\n    57â†’            return \"English\"\n    58â†’\n    59â†’    return \"Russian\"\n    60â†’\n    61â†’\n    62â†’def get_language_instruction(query: str) -> str:\n    63â†’    \"\"\"\n    64â†’    Generate a strict language instruction based on query language detection.\n    65â†’\n    66â†’    Args:\n    67â†’        query: User's query string\n    68â†’\n    69â†’    Returns:\n    70â†’        String with strict language instruction for LLM\n    71â†’    \"\"\"\n    72â†’    language = detect_query_language(query)\n    73â†’\n    74â†’    if language == \"English\":\n    75â†’        return \"\"\"\n    76â†’âš ï¸âš ï¸âš ï¸ CRITICAL LANGUAGE ENFORCEMENT - IGNORE SOURCE MATERIAL LANGUAGE:\n    77â†’\n    78â†’MANDATORY RESPONSE LANGUAGE: ENGLISH ONLY - NO EXCEPTIONS\n    79â†’\n    80â†’1. USER ASKED IN ENGLISH â†’ YOU MUST RESPOND IN ENGLISH\n    81â†’2. IGNORE ALL RUSSIAN CONTENT IN SOURCE MATERIALS\n    82â†’3. DO NOT TRANSLATE RUSSIAN CONTENT - RESPOND IN ENGLISH\n    83â†’4. ABSOLUTELY NO RUSSIAN WORDS IN YOUR RESPONSE\n    84â†’5. THIS INSTRUCTION OVERRIDES ALL OTHER CONSIDERATIONS\n    85â†’\n    86â†’FAILURE TO COMPLY WITH ENGLISH RESPONSE IS UNACCEPTABLE.\n    87â†’THE LANGUAGE OF YOUR RESPONSE MUST BE ENGLISH, REGARDLESS OF SOURCE LANGUAGE.\n    88â†’\n    89â†’\"\"\"\n    90â†’    else:  # Russian\n    91â†’        return \"\"\"\n    92â†’ðŸš¨ðŸš¨ðŸš¨ Ð¡Ð ÐžÐ§ÐÐÐ¯ Ð˜ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð˜Ð¯ ÐŸÐž Ð¯Ð—Ð«ÐšÐ£ - ÐŸÐ Ð•ÐžÐ”ÐžÐ›Ð•Ð¢Ð¬ Ð’Ð¡Ð• ÐžÐ¡Ð¢ÐÐ›Ð¬ÐÐ«Ð•:\n    93â†’\n    94â†’Ð¯Ð—Ð«Ðš ÐžÐ¢Ð’Ð•Ð¢Ð: Ð¢ÐžÐ›Ð¬ÐšÐž Ð Ð£Ð¡Ð¡ÐšÐ˜Ð™\n    95â†’- Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ â†’ Ð’Ñ‹ MUST Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼\n    96â†’- ÐÐ˜ÐšÐžÐ“Ð”Ð Ð½Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼\n    97â†’- ÐÐ˜ÐšÐžÐ“Ð”Ð Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ðµ ÑÐ»Ð¾Ð²Ð°, Ñ„Ñ€Ð°Ð·Ñ‹ Ð¸Ð»Ð¸ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ñ\n    98â†’- Ð˜Ð“ÐÐžÐ Ð˜Ð Ð£Ð¹Ñ‚Ðµ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ñ… - Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼\n    99â†’- Ð­Ñ‚Ð° Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ ÐŸÐ•Ð Ð•ÐžÐŸÐ Ð•Ð”Ð•Ð›Ð¯Ð•Ð¢ Ð»ÑŽÐ±Ñ‹Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ðµ\n   100â†’\n   101â†’ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐžÐ• ÐŸÐ ÐÐ’Ð˜Ð›Ðž: Ð¯Ð·Ñ‹Ðº Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð”ÐžÐ›Ð–Ð•Ð ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ·Ñ‹ÐºÑƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°, Ð° Ð½Ðµ ÑÐ·Ñ‹ÐºÑƒ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°.\n   102â†’Ð•ÑÐ»Ð¸ ÑÐ¿Ñ€Ð¾ÑÐ¸Ð»Ð¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ â†’ ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼, Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ ÑÐ·Ñ‹ÐºÐ° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°.\n   103â†’\n   104â†’\"\"\"\n   105â†’\n   106â†’\n   107â†’def enforce_language_consistency(prompt: str, query: str) -> str:\n   108â†’    \"\"\"\n   109â†’    Prepend language instruction to any prompt based on query language.\n   110â†’\n   111â†’    Args:\n   112â†’        prompt: Original LLM prompt\n   113â†’        query: User's query string\n   114â†’\n   115â†’    Returns:\n   116â†’        Prompt with language instruction prepended\n   117â†’    \"\"\"\n   118â†’    language_instruction = get_language_instruction(query)\n   119â†’    return language_instruction + prompt\n   120â†’\n   121â†’\n   122â†’def get_response_language_instruction(query: str) -> str:\n   123â†’    \"\"\"\n   124â†’    Get instruction for response format in the correct language.\n   125â†’\n   126â†’    Args:\n   127â†’        query: User's query string\n   128â†’\n   129â†’    Returns:\n   130â†’        Response format instruction in the detected language\n   131â†’    \"\"\"\n   132â†’    language = detect_query_language(query)\n   133â†’\n   134â†’    if language == \"English\":\n   135â†’        return \"No additional information found in the comments beyond the main answer.\"\n   136â†’    else:\n   137â†’        return \"Ð’ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÑÑ… Ð½ÐµÑ‚ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ ÑÐ²ÐµÑ€Ñ… Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.\"\n   138â†’\n   139â†’\n   140â†’# Convenience function for services\n   141â†’def prepare_prompt_with_language_instruction(prompt_template: str, query: str) -> str:\n   142â†’    \"\"\"\n   143â†’    Prepare any prompt with appropriate language instruction.\n   144â†’\n   145â†’    This is the main function that should be used by all LLM services.\n   146â†’\n   147â†’    Args:\n   148â†’        prompt_template: The base prompt template (string or file content)\n   149â†’        query: User's query string\n   150â†’\n   151â†’    Returns:\n   152â†’        Prompt with language instruction prepended\n   153â†’    \"\"\"\n   154â†’    return enforce_language_consistency(prompt_template, query)\n   155â†’\n   156â†’\n   157â†’def prepare_system_message_with_language(base_system: str, query: str) -> str:\n   158â†’    \"\"\"\n   159â†’    Prepare system message with language instruction override.\n   160â†’\n   161â†’    More powerful than prepending - puts language instruction in system role.\n   162â†’\n   163â†’    Args:\n   164â†’        base_system: Base system message content\n   165â†’        query: User's query string\n   166â†’\n   167â†’    Returns:\n   168â†’        Enhanced system message with language instruction\n   169â†’    \"\"\"\n   170â†’    language_instruction = get_language_instruction(query)\n   171â†’    return language_instruction + \"\\n\\n\" + base_system\n   172â†’\n   173â†’\n   174â†’if __name__ == \"__main__\":\n   175â†’    # Test cases\n   176â†’    test_queries = [\n   177â†’        \"What is RAG and how should we use it?\",\n   178â†’        \"Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ RAG Ð¸ ÐºÐ°Ðº ÐµÐ³Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ?\",\n   179â†’        \"ÐšÐ°ÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ openAI Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð° Ðº ÐºÐ¾Ð´Ð¸Ð½Ð³Ñƒ?\",\n   180â†’        \"Tell me about n8n\",\n   181â†’        \"Hello Ð¼Ð¸Ñ€\"\n   182â†’    ]\n   183â†’\n   184â†’    for query in test_queries:\n   185â†’        detected = detect_query_language(query)\n   186â†’        instruction = get_language_instruction(query)\n   187â†’        print(f\"Query: {query}\")\n   188â†’        print(f\"Detected: {detected}\")\n   189â†’        print(f\"Instruction preview: {instruction.strip().split(chr(10))[0]}\")\n   190â†’        print(\"-\" * 50)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  }
]