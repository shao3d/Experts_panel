[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_hhtbsaa9rfh",
        "type": "tool_result",
        "content": "     1→\"\"\"Database initialization and session management.\"\"\"\n     2→\n     3→import os\n     4→from typing import AsyncGenerator\n     5→from contextlib import asynccontextmanager\n     6→\n     7→from sqlalchemy import Table, Column, Integer, Boolean, Text, TIMESTAMP, ForeignKey, event\n     8→from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine\n     9→from sqlalchemy.pool import NullPool\n    10→\n    11→from .base import Base, engine\n    12→\n    13→\n    14→# Define comment_group_drift table (created via SQL migration)\n    15→comment_group_drift = Table(\n    16→    'comment_group_drift',\n    17→    Base.metadata,\n    18→    Column('id', Integer, primary_key=True),\n    19→    Column('post_id', Integer, ForeignKey('posts.post_id'), nullable=False, unique=True),\n    20→    Column('has_drift', Boolean, nullable=False, default=False),\n    21→    Column('drift_topics', Text),  # JSON\n    22→    Column('analyzed_at', TIMESTAMP, nullable=False),\n    23→    Column('analyzed_by', Text, nullable=False),\n    24→    Column('expert_id', Text),  # Expert identifier for multi-expert support\n    25→    extend_existing=True\n    26→)\n    27→\n    28→\n    29→def enable_sqlite_foreign_keys(dbapi_connection, connection_record):\n    30→    \"\"\"Enable foreign key support in SQLite.\"\"\"\n    31→    cursor = dbapi_connection.cursor()\n    32→    cursor.execute(\"PRAGMA foreign_keys=ON\")\n    33→    cursor.close()\n    34→\n    35→\n    36→def init_db():\n    37→    \"\"\"Initialize database with all tables.\"\"\"\n    38→    # Import all models to register them with Base\n    39→    from . import Post, Link, Comment\n    40→\n    41→    # Enable foreign keys for SQLite\n    42→    event.listen(engine, \"connect\", enable_sqlite_foreign_keys)\n    43→\n    44→    # Create all tables\n    45→    Base.metadata.create_all(bind=engine)\n    46→    print(\"Database initialized successfully!\")\n    47→\n    48→\n    49→def drop_db():\n    50→    \"\"\"Drop all tables (use with caution!).\"\"\"\n    51→    Base.metadata.drop_all(bind=engine)\n    52→    print(\"All tables dropped!\")\n    53→\n    54→\n    55→def reset_db():\n    56→    \"\"\"Reset database by dropping and recreating all tables.\"\"\"\n    57→    drop_db()\n    58→    init_db()\n    59→    print(\"Database reset complete!\")\n    60→\n    61→\n    62→# Async support for future use\n    63→DATABASE_URL_ASYNC = \"sqlite+aiosqlite:///../data/experts.db\"\n    64→\n    65→async_engine = create_async_engine(\n    66→    DATABASE_URL_ASYNC,\n    67→    echo=False,\n    68→    poolclass=NullPool,  # Recommended for SQLite\n    69→)\n    70→\n    71→AsyncSessionLocal = async_sessionmaker(\n    72→    async_engine,\n    73→    class_=AsyncSession,\n    74→    expire_on_commit=False\n    75→)\n    76→\n    77→\n    78→@asynccontextmanager\n    79→async def get_async_session() -> AsyncGenerator[AsyncSession, None]:\n    80→    \"\"\"Async context manager for database sessions.\"\"\"\n    81→    async with AsyncSessionLocal() as session:\n    82→        try:\n    83→            yield session\n    84→            await session.commit()\n    85→        except Exception:\n    86→            await session.rollback()\n    87→            raise\n    88→        finally:\n    89→            await session.close()\n    90→\n    91→\n    92→async def init_async_db():\n    93→    \"\"\"Initialize database asynchronously.\"\"\"\n    94→    async with async_engine.begin() as conn:\n    95→        # Enable foreign keys for SQLite\n    96→        await conn.execute(\"PRAGMA foreign_keys=ON\")\n    97→        # Create all tables\n    98→        await conn.run_sync(Base.metadata.create_all)\n    99→    print(\"Async database initialized successfully!\")\n   100→\n   101→\n   102→async def drop_async_db():\n   103→    \"\"\"Drop all tables asynchronously.\"\"\"\n   104→    async with async_engine.begin() as conn:\n   105→        await conn.run_sync(Base.metadata.drop_all)\n   106→    print(\"All tables dropped (async)!\")\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Now let me read the remaining important files to complete the analysis:\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_nn1usk7doqn",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/data/json_parser.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_hvfs2tmmw6m",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/data/channel_syncer.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_ncwvov3ulm",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/utils/language_utils.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_exqe4fv1dek",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/utils/entities_converter.py"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_nn1usk7doqn",
        "type": "tool_result",
        "content": "     1→\"\"\"JSON parser for Telegram channel exports.\"\"\"\n     2→\n     3→import json\n     4→import argparse\n     5→import sys\n     6→from pathlib import Path\n     7→from datetime import datetime\n     8→from typing import List, Dict, Any, Optional, Tuple\n     9→from collections import defaultdict\n    10→\n    11→# Add parent directory to path for imports\n    12→sys.path.append(str(Path(__file__).parent.parent))\n    13→\n    14→from models import SessionLocal, Post, Link, LinkType\n    15→from models.database import init_db\n    16→from utils.entities_converter import entities_to_markdown_from_json\n    17→\n    18→\n    19→class TelegramJsonParser:\n    20→    \"\"\"Parser for Telegram JSON export files.\"\"\"\n    21→\n    22→    def __init__(self, session, expert_id: str):\n    23→        \"\"\"Initialize parser with database session.\n    24→\n    25→        Args:\n    26→            session: Database session\n    27→            expert_id: Expert identifier (e.g., 'refat', 'ai_architect')\n    28→        \"\"\"\n    29→        self.session = session\n    30→        self.expert_id = expert_id\n    31→        self.stats = {\n    32→            'posts_imported': 0,\n    33→            'links_created': 0,\n    34→            'errors': 0,\n    35→            'skipped': 0\n    36→        }\n    37→        # Track posts by telegram_message_id for link resolution\n    38→        self.post_id_map = {}\n    39→\n    40→    def parse_file(self, json_path: str) -> Dict[str, int]:\n    41→        \"\"\"Parse Telegram JSON export file and import to database.\"\"\"\n    42→        print(f\"Parsing file: {json_path}\")\n    43→\n    44→        try:\n    45→            with open(json_path, 'r', encoding='utf-8') as f:\n    46→                data = json.load(f)\n    47→        except Exception as e:\n    48→            print(f\"Error reading JSON file: {e}\")\n    49→            return self.stats\n    50→\n    51→        # Extract channel/chat info\n    52→        channel_info = self._extract_channel_info(data)\n    53→\n    54→        # Parse messages\n    55→        messages = data.get('messages', [])\n    56→        total = len(messages)\n    57→        print(f\"Found {total} messages to import\")\n    58→\n    59→        # Process in batches for better performance\n    60→        batch_size = 100\n    61→        for i in range(0, total, batch_size):\n    62→            batch = messages[i:i+batch_size]\n    63→            self._process_message_batch(batch, channel_info)\n    64→\n    65→            # Show progress\n    66→            progress = min(i + batch_size, total)\n    67→            percent = (progress / total) * 100\n    68→            print(f\"Progress: {progress}/{total} ({percent:.1f}%)\")\n    69→\n    70→        # Commit all changes\n    71→        try:\n    72→            self.session.commit()\n    73→            print(\"✅ Import completed successfully!\")\n    74→        except Exception as e:\n    75→            print(f\"Error committing changes: {e}\")\n    76→            self.session.rollback()\n    77→            self.stats['errors'] += 1\n    78→\n    79→        # Create links after all posts are imported\n    80→        self._create_links(messages)\n    81→\n    82→        return self.stats\n    83→\n    84→    def _extract_channel_info(self, data: Dict) -> Dict[str, str]:\n    85→        \"\"\"Extract channel/chat information from JSON.\"\"\"\n    86→        return {\n    87→            'channel_id': str(data.get('id', 'unknown')),\n    88→            'channel_name': data.get('name', data.get('title', 'Unknown Channel')),\n    89→            'type': data.get('type', 'channel')\n    90→        }\n    91→\n    92→    def _process_message_batch(self, messages: List[Dict], channel_info: Dict):\n    93→        \"\"\"Process a batch of messages.\"\"\"\n    94→        for msg in messages:\n    95→            try:\n    96→                post = self._message_to_post(msg, channel_info)\n    97→                if post:\n    98→                    self.session.add(post)\n    99→                    self.session.flush()  # Get the post_id\n   100→\n   101→                    # Map telegram message ID to our post_id for link resolution\n   102→                    if msg.get('id'):\n   103→                        self.post_id_map[msg['id']] = post.post_id\n   104→\n   105→                    self.stats['posts_imported'] += 1\n   106→            except Exception as e:\n   107→                print(f\"Error processing message {msg.get('id', 'unknown')}: {e}\")\n   108→                self.stats['errors'] += 1\n   109→\n   110→    def _message_to_post(self, msg: Dict, channel_info: Dict) -> Optional[Post]:\n   111→        \"\"\"Convert Telegram message to Post model.\"\"\"\n   112→        # Skip service messages\n   113→        if msg.get('type') != 'message':\n   114→            self.stats['skipped'] += 1\n   115→            return None\n   116→\n   117→        # Extract text content\n   118→        text = self._extract_text(msg)\n   119→        if not text and not msg.get('media_type'):\n   120→            self.stats['skipped'] += 1\n   121→            return None\n   122→\n   123→        # Parse timestamps\n   124→        created_at = self._parse_timestamp(msg.get('date'))\n   125→        edited_at = self._parse_timestamp(msg.get('edited'))\n   126→\n   127→        # Extract media metadata\n   128→        media_metadata = self._extract_media_metadata(msg)\n   129→\n   130→        # Create Post object\n   131→        post = Post(\n   132→            channel_id=channel_info['channel_id'],\n   133→            channel_name=channel_info['channel_name'],\n   134→            telegram_message_id=msg.get('id'),\n   135→            expert_id=self.expert_id,\n   136→            message_text=text,\n   137→            author_name=msg.get('from', msg.get('author', '')),\n   138→            author_id=msg.get('from_id', ''),\n   139→            created_at=created_at,\n   140→            edited_at=edited_at,\n   141→            view_count=msg.get('views', 0),\n   142→            forward_count=msg.get('forwards', 0),\n   143→            media_metadata=media_metadata,\n   144→            is_forwarded=1 if msg.get('forwarded_from') else 0,\n   145→            forward_from_channel=msg.get('forwarded_from')\n   146→        )\n   147→\n   148→        return post\n   149→\n   150→    def _extract_text(self, msg: Dict) -> str:\n   151→        \"\"\"Extract text content from message and convert entities to markdown.\"\"\"\n   152→        text = msg.get('text', '')\n   153→\n   154→        # Convert entities to markdown using the utility function\n   155→        markdown_text = entities_to_markdown_from_json(text)\n   156→\n   157→        return markdown_text.strip() if markdown_text else ''\n   158→\n   159→    def _parse_timestamp(self, date_str: Optional[str]) -> Optional[datetime]:\n   160→        \"\"\"Parse Telegram timestamp format.\"\"\"\n   161→        if not date_str:\n   162→            return None\n   163→\n   164→        try:\n   165→            # Telegram uses ISO format: \"2023-01-01T12:00:00\"\n   166→            return datetime.fromisoformat(date_str.replace('T', ' '))\n   167→        except:\n   168→            try:\n   169→                # Alternative format\n   170→                return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n   171→            except:\n   172→                return None\n   173→\n   174→    def _extract_media_metadata(self, msg: Dict) -> Optional[Dict]:\n   175→        \"\"\"Extract media metadata from message.\"\"\"\n   176→        media_type = msg.get('media_type')\n   177→        if not media_type:\n   178→            return None\n   179→\n   180→        metadata = {\n   181→            'type': media_type,\n   182→            'mime_type': msg.get('mime_type'),\n   183→            'file': msg.get('file'),\n   184→            'file_name': msg.get('file_name'),\n   185→            'duration': msg.get('duration_seconds'),\n   186→            'width': msg.get('width'),\n   187→            'height': msg.get('height'),\n   188→            'thumbnail': msg.get('thumbnail')\n   189→        }\n   190→\n   191→        # Remove None values\n   192→        metadata = {k: v for k, v in metadata.items() if v is not None}\n   193→\n   194→        return metadata if metadata else None\n   195→\n   196→    def _create_links(self, messages: List[Dict]):\n   197→        \"\"\"Create Link records for replies, forwards, and text links.\"\"\"\n   198→        print(\"\\nCreating links between posts...\")\n   199→        import re\n   200→\n   201→        for msg in messages:\n   202→            # Skip non-messages\n   203→            if msg.get('type') != 'message' or msg.get('id') not in self.post_id_map:\n   204→                continue\n   205→\n   206→            source_post_id = self.post_id_map[msg['id']]\n   207→\n   208→            # Handle replies\n   209→            reply_to = msg.get('reply_to_message_id')\n   210→            if reply_to and reply_to in self.post_id_map:\n   211→                target_post_id = self.post_id_map[reply_to]\n   212→                self._create_link(source_post_id, target_post_id, LinkType.REPLY)\n   213→\n   214→            # Handle text_link elements (links in text to other posts)\n   215→            text = msg.get('text')\n   216→            if isinstance(text, list):\n   217→                for item in text:\n   218→                    if isinstance(item, dict) and item.get('type') == 'text_link':\n   219→                        href = item.get('href', '')\n   220→                        # Extract post ID from t.me links (e.g., https://t.me/channelname/123)\n   221→                        match = re.search(r't\\.me/[\\w]+/(\\d+)', href)\n   222→                        if match:\n   223→                            linked_msg_id = int(match.group(1))\n   224→                            if linked_msg_id in self.post_id_map:\n   225→                                target_post_id = self.post_id_map[linked_msg_id]\n   226→                                self._create_link(source_post_id, target_post_id, LinkType.MENTION)\n   227→\n   228→            # Handle forwards (if we want to track them as links)\n   229→            if msg.get('forwarded_from'):\n   230→                # For now, we just store the forward source in the post\n   231→                # Could create links if we have the original posts\n   232→                pass\n   233→\n   234→        try:\n   235→            self.session.commit()\n   236→            print(f\"✅ Created {self.stats['links_created']} links\")\n   237→        except Exception as e:\n   238→            print(f\"Error creating links: {e}\")\n   239→            self.session.rollback()\n   240→\n   241→    def _create_link(self, source_id: int, target_id: int, link_type: LinkType):\n   242→        \"\"\"Create a link between two posts.\"\"\"\n   243→        # Check if link already exists\n   244→        existing = self.session.query(Link).filter(\n   245→            Link.source_post_id == source_id,\n   246→            Link.target_post_id == target_id,\n   247→            Link.link_type == link_type.value\n   248→        ).first()\n   249→\n   250→        if existing:\n   251→            return  # Skip duplicate\n   252→\n   253→        try:\n   254→            link = Link(\n   255→                source_post_id=source_id,\n   256→                target_post_id=target_id,\n   257→                link_type=link_type.value,\n   258→                created_at=datetime.utcnow()\n   259→            )\n   260→            self.session.add(link)\n   261→            self.session.flush()  # Flush to catch constraint errors immediately\n   262→            self.stats['links_created'] += 1\n   263→        except Exception as e:\n   264→            self.session.rollback()\n   265→            # Ignore duplicate link errors\n   266→            if 'UNIQUE constraint failed' not in str(e):\n   267→                print(f\"Error creating link: {e}\")\n   268→\n   269→\n   270→def main():\n   271→    \"\"\"CLI interface for JSON parser.\"\"\"\n   272→    parser = argparse.ArgumentParser(description='Import Telegram JSON exports to database')\n   273→    parser.add_argument('json_file', help='Path to Telegram JSON export file')\n   274→    parser.add_argument('--init-db', action='store_true',\n   275→                       help='Initialize database before import')\n   276→    parser.add_argument('--dry-run', action='store_true',\n   277→                       help='Parse without saving to database')\n   278→    parser.add_argument('--expert-id', required=True,\n   279→                       help='Expert identifier (e.g., refat, ai_architect)')\n   280→\n   281→    args = parser.parse_args()\n   282→\n   283→    # Validate file exists\n   284→    if not Path(args.json_file).exists():\n   285→        print(f\"Error: File not found: {args.json_file}\")\n   286→        return 1\n   287→\n   288→    # Initialize database if requested\n   289→    if args.init_db:\n   290→        print(\"Initializing database...\")\n   291→        init_db()\n   292→\n   293→    # Create database session\n   294→    session = SessionLocal()\n   295→\n   296→    try:\n   297→        # Parse and import\n   298→        parser = TelegramJsonParser(session, expert_id=args.expert_id)\n   299→        stats = parser.parse_file(args.json_file)\n   300→\n   301→        # Print statistics\n   302→        print(\"\\n\" + \"=\"*50)\n   303→        print(\"Import Statistics:\")\n   304→        print(f\"  Posts imported: {stats['posts_imported']}\")\n   305→        print(f\"  Links created: {stats['links_created']}\")\n   306→        print(f\"  Messages skipped: {stats['skipped']}\")\n   307→        print(f\"  Errors: {stats['errors']}\")\n   308→        print(\"=\"*50)\n   309→\n   310→    finally:\n   311→        session.close()\n   312→\n   313→    return 0\n   314→\n   315→\n   316→if __name__ == \"__main__\":\n   317→    exit(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_hvfs2tmmw6m",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Telegram Channel Syncer\n     3→Incremental sync of Telegram channel posts and comments.\n     4→\n     5→Usage:\n     6→    from src.data.channel_syncer import TelegramChannelSyncer\n     7→\n     8→    syncer = TelegramChannelSyncer(api_id, api_hash, session_name)\n     9→    result = await syncer.sync_channel_incremental(channel_username, dry_run=False)\n    10→\"\"\"\n    11→\n    12→import asyncio\n    13→import sys\n    14→from pathlib import Path\n    15→from datetime import datetime\n    16→from typing import List, Dict, Any, Optional\n    17→import os\n    18→\n    19→from telethon import TelegramClient\n    20→from telethon.errors import FloodWaitError, ChannelPrivateError\n    21→from telethon.tl.types import Message\n    22→from sqlalchemy.orm import Session\n    23→from sqlalchemy import text\n    24→\n    25→# Add parent directory to path for imports\n    26→sys.path.append(str(Path(__file__).parent.parent))\n    27→\n    28→from models.base import SessionLocal\n    29→from models.post import Post\n    30→from models.comment import Comment\n    31→from models.link import Link, LinkType\n    32→from data.telegram_comments_fetcher import SafeTelegramCommentsFetcher\n    33→from utils.entities_converter import entities_to_markdown_from_telethon\n    34→\n    35→\n    36→class TelegramChannelSyncer(SafeTelegramCommentsFetcher):\n    37→    \"\"\"\n    38→    Extends SafeTelegramCommentsFetcher with incremental sync capabilities.\n    39→\n    40→    Features:\n    41→    - Fetch new posts since last sync (incremental)\n    42→    - Update comments for recent posts (depth=N)\n    43→    - Track sync state in database\n    44→    - Calculate new comment groups needing drift analysis\n    45→    \"\"\"\n    46→\n    47→    RECENT_POSTS_DEPTH = 10  # Check last 10 posts for new comments (~30 days)\n    48→\n    49→    def __init__(self, api_id: int, api_hash: str, session_name: str = 'telegram_fetcher'):\n    50→        \"\"\"Initialize syncer with Telegram credentials.\"\"\"\n    51→        super().__init__(api_id, api_hash, session_name)\n    52→        self.stats = {\n    53→            'new_posts_found': 0,\n    54→            'new_posts_saved': 0,\n    55→            'posts_updated': 0,\n    56→            'new_comments_found': 0,\n    57→            'new_comments_saved': 0,\n    58→            'errors': 0,\n    59→            'duration_seconds': 0\n    60→        }\n    61→\n    62→    async def sync_channel_incremental(\n    63→        self,\n    64→        channel_username: str,\n    65→        expert_id: str,\n    66→        dry_run: bool = False\n    67→    ) -> Dict[str, Any]:\n    68→        \"\"\"\n    69→        Fetch new posts and comments since last sync.\n    70→\n    71→        Args:\n    72→            channel_username: Channel to sync (e.g., 'nobilix')\n    73→            expert_id: Expert identifier (e.g., 'refat', 'ai_architect')\n    74→            dry_run: If True, don't save to database\n    75→\n    76→        Returns:\n    77→            {\n    78→                \"success\": bool,\n    79→                \"new_posts\": List[int],  # telegram_message_ids\n    80→                \"updated_posts\": List[int],\n    81→                \"new_comment_groups\": List[int],\n    82→                \"stats\": {...},\n    83→                \"errors\": List[str]\n    84→            }\n    85→        \"\"\"\n    86→        start_time = datetime.utcnow()\n    87→        errors = []\n    88→        new_posts = []\n    89→        updated_posts = []\n    90→\n    91→        print(\"=\" * 60)\n    92→        print(\"🔄 Telegram Channel Sync\")\n    93→        print(\"=\" * 60)\n    94→        print(f\"Mode: {'DRY-RUN' if dry_run else 'LIVE'}\")\n    95→        print(f\"Channel: {channel_username}\")\n    96→        print()\n    97→\n    98→        # Connect to Telegram\n    99→        print(\"📱 Connecting to Telegram...\")\n   100→        self.client = TelegramClient(self.session_name, self.api_id, self.api_hash)\n   101→        await self.client.start()\n   102→        print(\"✅ Connected!\")\n   103→\n   104→        # Get channel entity\n   105→        print(f\"\\n🔍 Finding channel: {channel_username}\")\n   106→        try:\n   107→            channel = await self.client.get_entity(channel_username)\n   108→            print(f\"✅ Found: {channel.title}\")\n   109→        except Exception as e:\n   110→            error_msg = f\"Cannot find channel {channel_username}: {e}\"\n   111→            print(f\"❌ {error_msg}\")\n   112→            errors.append(error_msg)\n   113→            return self._build_result(False, new_posts, updated_posts, [], errors, start_time)\n   114→\n   115→        # Open database\n   116→        db = SessionLocal()\n   117→        try:\n   118→            # Get last synced message ID\n   119→            last_synced_id = self._get_last_synced_id(db, channel_username)\n   120→            print(f\"\\n📊 Last synced message ID: {last_synced_id or 'None (first sync)'}\")\n   121→\n   122→            # Fetch new posts\n   123→            print(f\"\\n📥 Fetching new posts (min_id={last_synced_id or 0})...\")\n   124→            new_posts_data = await self._fetch_new_posts(channel, last_synced_id)\n   125→            self.stats['new_posts_found'] = len(new_posts_data)\n   126→            print(f\"✅ Found {len(new_posts_data)} new posts\")\n   127→\n   128→            if new_posts_data:\n   129→                # Save new posts to database\n   130→                if not dry_run:\n   131→                    new_posts = self._save_posts_to_db(db, new_posts_data, channel, expert_id)\n   132→                    self.stats['new_posts_saved'] = len(new_posts)\n   133→                    db.commit()  # Commit now so new posts are visible to next query\n   134→                    print(f\"✅ Saved {len(new_posts)} new posts\")\n   135→                else:\n   136→                    new_posts = [p['telegram_message_id'] for p in new_posts_data]\n   137→                    print(f\"[DRY-RUN] Would save {len(new_posts)} posts\")\n   138→\n   139→            # Update recent posts comments\n   140→            print(f\"\\n🔄 Updating comments for last {self.RECENT_POSTS_DEPTH} posts...\")\n   141→            updated_result = await self.update_recent_posts_comments(\n   142→                db=db,\n   143→                channel_username=channel_username,\n   144→                expert_id=expert_id,\n   145→                depth=self.RECENT_POSTS_DEPTH,\n   146→                dry_run=dry_run\n   147→            )\n   148→            updated_posts = updated_result.get('updated_posts', [])\n   149→            self.stats['posts_updated'] = len(updated_posts)\n   150→            self.stats['new_comments_found'] += updated_result.get('stats', {}).get('comments_found', 0)\n   151→            self.stats['new_comments_saved'] += updated_result.get('stats', {}).get('comments_saved', 0)\n   152→\n   153→            # Update comments for NEW posts (if any)\n   154→            if new_posts:\n   155→                print(f\"\\n🔄 Updating comments for {len(new_posts)} new posts...\")\n   156→                new_posts_result = await self.update_specific_posts_comments(\n   157→                    db=db,\n   158→                    channel_username=channel_username,\n   159→                    expert_id=expert_id,\n   160→                    telegram_message_ids=new_posts,\n   161→                    dry_run=dry_run\n   162→                )\n   163→                new_posts_updated = new_posts_result.get('updated_posts', [])\n   164→                # Merge updated posts lists\n   165→                updated_posts.extend(new_posts_updated)\n   166→                updated_posts = list(set(updated_posts))  # Remove duplicates\n   167→                self.stats['posts_updated'] = len(updated_posts)\n   168→                self.stats['new_comments_found'] += new_posts_result.get('stats', {}).get('comments_found', 0)\n   169→                self.stats['new_comments_saved'] += new_posts_result.get('stats', {}).get('comments_saved', 0)\n   170→\n   171→            # Update sync state\n   172→            if not dry_run and new_posts:\n   173→                max_message_id = max(new_posts)\n   174→                self._update_sync_state(db, channel_username, max_message_id)\n   175→                print(f\"✅ Updated sync state: last_synced_message_id = {max_message_id}\")\n   176→\n   177→            # Calculate new comment groups needing drift analysis for this expert\n   178→            new_comment_groups = self.calculate_new_comment_groups(db, expert_id)\n   179→            print(f\"\\n🎯 Comment groups needing drift analysis: {len(new_comment_groups)}\")\n   180→\n   181→            # Commit all changes\n   182→            if not dry_run:\n   183→                db.commit()\n   184→                print(\"✅ Database committed\")\n   185→\n   186→            # Calculate duration\n   187→            duration = (datetime.utcnow() - start_time).total_seconds()\n   188→            self.stats['duration_seconds'] = duration\n   189→\n   190→            print(f\"\\n✅ Sync complete! Duration: {duration:.1f}s\")\n   191→\n   192→            return self._build_result(True, new_posts, updated_posts, new_comment_groups, errors, start_time)\n   193→\n   194→        except Exception as e:\n   195→            db.rollback()\n   196→            error_msg = f\"Sync failed: {e}\"\n   197→            print(f\"\\n❌ {error_msg}\")\n   198→            errors.append(error_msg)\n   199→            self.stats['errors'] += 1\n   200→            return self._build_result(False, new_posts, updated_posts, [], errors, start_time)\n   201→        finally:\n   202→            db.close()\n   203→            await self.client.disconnect()\n   204→\n   205→    async def update_recent_posts_comments(\n   206→        self,\n   207→        db: Session,\n   208→        channel_username: str,\n   209→        expert_id: str,\n   210→        depth: int = 10,\n   211→        dry_run: bool = False\n   212→    ) -> Dict[str, Any]:\n   213→        \"\"\"\n   214→        Refresh comments for last N posts for specific expert.\n   215→\n   216→        Args:\n   217→            db: Database session\n   218→            channel_username: Channel to sync\n   219→            expert_id: Expert identifier for filtering posts\n   220→            depth: How many recent posts to check\n   221→            dry_run: If True, don't save to database\n   222→\n   223→        Returns:\n   224→            {\n   225→                \"updated_posts\": List[int],  # telegram_message_ids\n   226→                \"stats\": {\"comments_found\": int, \"comments_saved\": int}\n   227→            }\n   228→        \"\"\"\n   229→        updated_posts = []\n   230→        comments_found = 0\n   231→        comments_saved = 0\n   232→\n   233→        # Get last N posts for this expert only\n   234→        recent_posts = db.query(Post.post_id, Post.telegram_message_id, Post.channel_id)\\\n   235→            .filter(Post.expert_id == expert_id)\\\n   236→            .order_by(Post.telegram_message_id.desc())\\\n   237→            .limit(depth)\\\n   238→            .all()\n   239→\n   240→        if not recent_posts:\n   241→            return {\"updated_posts\": [], \"stats\": {\"comments_found\": 0, \"comments_saved\": 0}}\n   242→\n   243→        # Get channel entity\n   244→        channel = await self.client.get_entity(channel_username)\n   245→\n   246→        # Fetch comments for each post\n   247→        for post_id, telegram_message_id, channel_id in recent_posts:\n   248→            print(f\"  📝 Post #{telegram_message_id}...\", end=\" \")\n   249→\n   250→            comments = await self.get_discussion_replies(channel, telegram_message_id)\n   251→            comments_found += len(comments)\n   252→\n   253→            if comments:\n   254→                if not dry_run:\n   255→                    # Save comments with proper channel_id filtering\n   256→                    saved = self.save_comments_to_db(db, comments, channel_id)\n   257→                    comments_saved += saved\n   258→                    db.commit()\n   259→                    if saved == len(comments):\n   260→                        print(f\"✅ {saved} comments\")\n   261→                    elif saved > 0:\n   262→                        print(f\"✅ {saved}/{len(comments)} comments (skipped {len(comments) - saved} duplicates)\")\n   263→                    else:\n   264→                        print(f\"⚠️  0/{len(comments)} comments (all duplicates)\")\n   265→                else:\n   266→                    print(f\"[DRY-RUN] Would save {len(comments)} comments\")\n   267→\n   268→                updated_posts.append(telegram_message_id)\n   269→            else:\n   270→                print(\"No comments\")\n   271→\n   272→            # Rate limiting\n   273→            await asyncio.sleep(self.DELAY_BETWEEN_POSTS)\n   274→\n   275→        return {\n   276→            \"updated_posts\": updated_posts,\n   277→            \"stats\": {\n   278→                \"comments_found\": comments_found,\n   279→                \"comments_saved\": comments_saved\n   280→            }\n   281→        }\n   282→\n   283→    async def update_specific_posts_comments(\n   284→        self,\n   285→        db: Session,\n   286→        channel_username: str,\n   287→        expert_id: str,\n   288→        telegram_message_ids: List[int],\n   289→        dry_run: bool = False\n   290→    ) -> Dict[str, Any]:\n   291→        \"\"\"\n   292→        Refresh comments for specific posts by telegram_message_ids.\n   293→\n   294→        Args:\n   295→            db: Database session\n   296→            channel_username: Channel to sync\n   297→            expert_id: Expert identifier for filtering posts\n   298→            telegram_message_ids: List of specific telegram_message_ids to check\n   299→            dry_run: If True, don't save to database\n   300→\n   301→        Returns:\n   302→            {\n   303→                \"updated_posts\": List[int],  # telegram_message_ids\n   304→                \"stats\": {\"comments_found\": int, \"comments_saved\": int}\n   305→            }\n   306→        \"\"\"\n   307→        updated_posts = []\n   308→        comments_found = 0\n   309→        comments_saved = 0\n   310→\n   311→        if not telegram_message_ids:\n   312→            return {\"updated_posts\": [], \"stats\": {\"comments_found\": 0, \"comments_saved\": 0}}\n   313→\n   314→        # Get posts from database for these telegram_message_ids\n   315→        posts_data = db.query(Post.post_id, Post.telegram_message_id, Post.channel_id)\\\n   316→            .filter(Post.expert_id == expert_id)\\\n   317→            .filter(Post.telegram_message_id.in_(telegram_message_ids))\\\n   318→            .all()\n   319→\n   320→        if not posts_data:\n   321→            return {\"updated_posts\": [], \"stats\": {\"comments_found\": 0, \"comments_saved\": 0}}\n   322→\n   323→        # Get channel entity\n   324→        channel = await self.client.get_entity(channel_username)\n   325→\n   326→        # Create a dict for quick lookup\n   327→        posts_dict = {msg_id: (post_id, channel_id) for post_id, msg_id, channel_id in posts_data}\n   328→\n   329→        # Fetch comments for each specific post\n   330→        for telegram_message_id in telegram_message_ids:\n   331→            if telegram_message_id not in posts_dict:\n   332→                continue  # Skip if not found in DB\n   333→\n   334→            post_id, channel_id = posts_dict[telegram_message_id]\n   335→            print(f\"  📝 New Post #{telegram_message_id}...\", end=\" \")\n   336→\n   337→            comments = await self.get_discussion_replies(channel, telegram_message_id)\n   338→            comments_found += len(comments)\n   339→\n   340→            if comments:\n   341→                if not dry_run:\n   342→                    # Save comments with proper channel_id filtering\n   343→                    saved = self.save_comments_to_db(db, comments, channel_id)\n   344→                    comments_saved += saved\n   345→                    db.commit()\n   346→                    if saved == len(comments):\n   347→                        print(f\"✅ {saved} comments\")\n   348→                    elif saved > 0:\n   349→                        print(f\"✅ {saved}/{len(comments)} comments (skipped {len(comments) - saved} duplicates)\")\n   350→                    else:\n   351→                        print(f\"⚠️  0/{len(comments)} comments (all duplicates)\")\n   352→                else:\n   353→                    print(f\"[DRY-RUN] Would save {len(comments)} comments\")\n   354→\n   355→                updated_posts.append(telegram_message_id)\n   356→            else:\n   357→                print(\"No comments\")\n   358→\n   359→            # Rate limiting\n   360→            await asyncio.sleep(self.DELAY_BETWEEN_POSTS)\n   361→\n   362→        return {\n   363→            \"updated_posts\": updated_posts,\n   364→            \"stats\": {\n   365→                \"comments_found\": comments_found,\n   366→                \"comments_saved\": comments_saved\n   367→            }\n   368→        }\n   369→\n   370→    def calculate_new_comment_groups(self, db: Session, expert_id: str) -> List[int]:\n   371→        \"\"\"\n   372→        Find posts with new comments that need drift analysis for specific expert.\n   373→\n   374→        Args:\n   375→            db: Database session\n   376→            expert_id: Expert identifier to filter posts\n   377→\n   378→        Returns:\n   379→            List of post_ids that need drift analysis (new comments since last sync)\n   380→        \"\"\"\n   381→        # Find posts with comments but no drift records OR posts that need drift analysis reset\n   382→        query = text(\"\"\"\n   383→            SELECT DISTINCT c.post_id\n   384→            FROM comments c\n   385→            JOIN posts p ON c.post_id = p.post_id\n   386→            LEFT JOIN comment_group_drift cgd ON c.post_id = cgd.post_id\n   387→            WHERE c.telegram_comment_id IS NOT NULL\n   388→              AND p.expert_id = :expert_id\n   389→              AND (\n   390→                cgd.post_id IS NULL  -- No drift record exists\n   391→                OR cgd.analyzed_by != 'pending'  -- Already analyzed, needs reset for new comments\n   392→              )\n   393→            GROUP BY c.post_id\n   394→            HAVING COUNT(c.comment_id) >= 1\n   395→            ORDER BY c.post_id\n   396→        \"\"\")\n   397→\n   398→        result = db.execute(query, {\"expert_id\": expert_id})\n   399→        post_ids = [row[0] for row in result.fetchall()]\n   400→        return post_ids\n   401→\n   402→    async def _fetch_new_posts(\n   403→        self,\n   404→        channel,\n   405→        min_id: Optional[int] = None\n   406→    ) -> List[Dict[str, Any]]:\n   407→        \"\"\"\n   408→        Fetch new posts from Telegram channel.\n   409→\n   410→        Args:\n   411→            channel: Telegram channel entity\n   412→            min_id: Only fetch messages with ID > min_id\n   413→\n   414→        Returns:\n   415→            List of post data dicts\n   416→        \"\"\"\n   417→        posts = []\n   418→\n   419→        async for message in self.client.iter_messages(\n   420→            entity=channel,\n   421→            min_id=min_id or 0,\n   422→            reverse=True,  # Chronological order (oldest first)\n   423→            limit=None     # All new messages\n   424→        ):\n   425→            if not message.message:  # Skip non-text messages\n   426→                continue\n   427→\n   428→            # Extract author info\n   429→            author_name = self._get_author_name(message)\n   430→            author_id = None\n   431→            if message.from_id:\n   432→                if hasattr(message.from_id, 'user_id'):\n   433→                    author_id = str(message.from_id.user_id)\n   434→                elif hasattr(message.from_id, 'channel_id'):\n   435→                    author_id = str(message.from_id.channel_id)\n   436→\n   437→            # Extract forward info\n   438→            is_forwarded = 0\n   439→            forward_from_channel = None\n   440→            if message.forward:\n   441→                is_forwarded = 1\n   442→                if hasattr(message.forward, 'from_id') and hasattr(message.forward.from_id, 'channel_id'):\n   443→                    forward_from_channel = str(message.forward.from_id.channel_id)\n   444→\n   445→            # Convert message entities to markdown\n   446→            message_markdown = entities_to_markdown_from_telethon(\n   447→                message.message,\n   448→                message.entities\n   449→            )\n   450→\n   451→            posts.append({\n   452→                'telegram_message_id': message.id,\n   453→                'message_text': message_markdown,\n   454→                'author_name': author_name,\n   455→                'author_id': author_id,\n   456→                'created_at': message.date,\n   457→                'edited_at': message.edit_date,\n   458→                'view_count': message.views or 0,\n   459→                'forward_count': message.forwards or 0,\n   460→                'reply_count': message.replies.replies if message.replies else 0,\n   461→                'is_forwarded': is_forwarded,\n   462→                'forward_from_channel': forward_from_channel,\n   463→                'reply_to_msg_id': message.reply_to.reply_to_msg_id if message.reply_to else None\n   464→            })\n   465→\n   466→            # Rate limiting\n   467→            await asyncio.sleep(self.DELAY_BETWEEN_POSTS)\n   468→\n   469→        return posts\n   470→\n   471→    def _save_posts_to_db(\n   472→        self,\n   473→        db: Session,\n   474→        posts_data: List[Dict[str, Any]],\n   475→        channel,\n   476→        expert_id: str\n   477→    ) -> List[int]:\n   478→        \"\"\"\n   479→        Save posts to database.\n   480→\n   481→        Args:\n   482→            db: Database session\n   483→            posts_data: List of post data dicts\n   484→            channel: Telegram channel entity\n   485→            expert_id: Expert identifier\n   486→\n   487→        Returns:\n   488→            List of saved telegram_message_ids\n   489→        \"\"\"\n   490→        saved_ids = []\n   491→\n   492→        for post_data in posts_data:\n   493→            # Create Post record\n   494→            post = Post(\n   495→                channel_id=str(channel.id),\n   496→                channel_name=channel.title,\n   497→                telegram_message_id=post_data['telegram_message_id'],\n   498→                expert_id=expert_id,\n   499→                message_text=post_data['message_text'],\n   500→                author_name=post_data['author_name'],\n   501→                author_id=post_data['author_id'],\n   502→                created_at=post_data['created_at'],\n   503→                edited_at=post_data['edited_at'],\n   504→                view_count=post_data['view_count'],\n   505→                forward_count=post_data['forward_count'],\n   506→                reply_count=post_data['reply_count'],\n   507→                is_forwarded=post_data['is_forwarded'],\n   508→                forward_from_channel=post_data['forward_from_channel']\n   509→            )\n   510→\n   511→            db.add(post)\n   512→            db.flush()  # Get post_id\n   513→\n   514→            # Create REPLY link if this is a reply\n   515→            if post_data['reply_to_msg_id']:\n   516→                target_post = db.query(Post).filter(\n   517→                    Post.telegram_message_id == post_data['reply_to_msg_id']\n   518→                ).first()\n   519→\n   520→                if target_post:\n   521→                    link = Link(\n   522→                        source_post_id=post.post_id,\n   523→                        target_post_id=target_post.post_id,\n   524→                        link_type=LinkType.REPLY.value,\n   525→                        created_at=datetime.utcnow()\n   526→                    )\n   527→                    db.add(link)\n   528→\n   529→            saved_ids.append(post_data['telegram_message_id'])\n   530→\n   531→        return saved_ids\n   532→\n   533→    def _get_last_synced_id(self, db: Session, channel_username: str) -> Optional[int]:\n   534→        \"\"\"Get last synced message ID from sync_state table.\"\"\"\n   535→        result = db.execute(\n   536→            text(\"SELECT last_synced_message_id FROM sync_state WHERE channel_username = :channel\"),\n   537→            {\"channel\": channel_username}\n   538→        ).fetchone()\n   539→\n   540→        return result[0] if result else None\n   541→\n   542→    def _update_sync_state(\n   543→        self,\n   544→        db: Session,\n   545→        channel_username: str,\n   546→        max_message_id: int\n   547→    ):\n   548→        \"\"\"Update sync state with new max message ID.\"\"\"\n   549→        # Check if record exists\n   550→        exists = db.execute(\n   551→            text(\"SELECT 1 FROM sync_state WHERE channel_username = :channel\"),\n   552→            {\"channel\": channel_username}\n   553→        ).fetchone()\n   554→\n   555→        if exists:\n   556→            db.execute(\n   557→                text(\"\"\"UPDATE sync_state\n   558→                        SET last_synced_message_id = :max_id,\n   559→                            last_synced_at = :now,\n   560→                            total_posts_synced = total_posts_synced + :new_posts,\n   561→                            total_comments_synced = total_comments_synced + :new_comments\n   562→                        WHERE channel_username = :channel\"\"\"),\n   563→                {\n   564→                    \"max_id\": max_message_id,\n   565→                    \"now\": datetime.utcnow(),\n   566→                    \"new_posts\": self.stats['new_posts_saved'],\n   567→                    \"new_comments\": self.stats['new_comments_saved'],\n   568→                    \"channel\": channel_username\n   569→                }\n   570→            )\n   571→        else:\n   572→            db.execute(\n   573→                text(\"\"\"INSERT INTO sync_state\n   574→                        (channel_username, last_synced_message_id, last_synced_at, total_posts_synced, total_comments_synced)\n   575→                        VALUES (:channel, :max_id, :now, :new_posts, :new_comments)\"\"\"),\n   576→                {\n   577→                    \"channel\": channel_username,\n   578→                    \"max_id\": max_message_id,\n   579→                    \"now\": datetime.utcnow(),\n   580→                    \"new_posts\": self.stats['new_posts_saved'],\n   581→                    \"new_comments\": self.stats['new_comments_saved']\n   582→                }\n   583→            )\n   584→\n   585→    def _build_result(\n   586→        self,\n   587→        success: bool,\n   588→        new_posts: List[int],\n   589→        updated_posts: List[int],\n   590→        new_comment_groups: List[int],\n   591→        errors: List[str],\n   592→        start_time: datetime\n   593→    ) -> Dict[str, Any]:\n   594→        \"\"\"Build standardized result dictionary.\"\"\"\n   595→        duration = (datetime.utcnow() - start_time).total_seconds()\n   596→        self.stats['duration_seconds'] = duration\n   597→\n   598→        return {\n   599→            \"success\": success,\n   600→            \"new_posts\": new_posts,\n   601→            \"updated_posts\": updated_posts,\n   602→            \"new_comment_groups\": new_comment_groups,\n   603→            \"stats\": {\n   604→                \"total_posts\": len(new_posts),\n   605→                \"total_comments\": self.stats['new_comments_saved'],\n   606→                \"groups_need_drift\": len(new_comment_groups),\n   607→                \"duration_seconds\": duration\n   608→            },\n   609→            \"errors\": errors\n   610→        }\n   611→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_ncwvov3ulm",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Language detection and instruction utilities for multi-lingual support.\n     3→Determines query language and enforces consistent language across all LLM calls.\n     4→\"\"\"\n     5→\n     6→import re\n     7→from typing import Tuple\n     8→\n     9→\n    10→def detect_query_language(query: str) -> str:\n    11→    \"\"\"\n    12→    Detect the primary language of a query.\n    13→\n    14→    Args:\n    15→        query: User's query string\n    16→\n    17→    Returns:\n    18→        \"English\" or \"Russian\"\n    19→    \"\"\"\n    20→    if not query or not query.strip():\n    21→        return \"Russian\"  # Default to Russian\n    22→\n    23→    # Remove URLs, mentions, hashtags, and special characters\n    24→    cleaned_query = re.sub(r'http[s]?://\\S+|@\\w+|#\\w+', '', query)\n    25→    cleaned_query = re.sub(r'[^\\w\\s]', ' ', cleaned_query)\n    26→\n    27→    # Count alphabetic characters\n    28→    english_chars = sum(1 for c in cleaned_query if c.isascii() and c.isalpha())\n    29→    cyrillic_chars = sum(1 for c in cleaned_query if '\\u0400' <= c <= '\\u04FF')\n    30→    total_chars = english_chars + cyrillic_chars\n    31→\n    32→    if total_chars == 0:\n    33→        return \"Russian\"  # Default for empty or only special chars\n    34→\n    35→    # Count words for more accurate detection\n    36→    words = cleaned_query.split()\n    37→    english_words = 0\n    38→    cyrillic_words = 0\n    39→\n    40→    for word in words:\n    41→        if any('\\u0400' <= c <= '\\u04FF' for c in word):\n    42→            cyrillic_words += 1\n    43→        elif any(c.isalpha() for c in word):\n    44→            english_words += 1\n    45→\n    46→    total_words = english_words + cyrillic_words\n    47→\n    48→    # More sensitive detection:\n    49→    # If ANY English words present AND >50% of characters are English → English\n    50→    # OR If majority of words are English → English\n    51→    if total_words > 0:\n    52→        word_ratio = english_words / total_words\n    53→        char_ratio = english_chars / total_chars if total_chars > 0 else 0\n    54→\n    55→        # Lower thresholds for English detection\n    56→        if (english_words > 0 and char_ratio >= 0.5) or word_ratio >= 0.5:\n    57→            return \"English\"\n    58→\n    59→    return \"Russian\"\n    60→\n    61→\n    62→def get_language_instruction(query: str) -> str:\n    63→    \"\"\"\n    64→    Generate a strict language instruction based on query language detection.\n    65→\n    66→    Args:\n    67→        query: User's query string\n    68→\n    69→    Returns:\n    70→        String with strict language instruction for LLM\n    71→    \"\"\"\n    72→    language = detect_query_language(query)\n    73→\n    74→    if language == \"English\":\n    75→        return \"\"\"\n    76→⚠️⚠️⚠️ CRITICAL LANGUAGE ENFORCEMENT - IGNORE SOURCE MATERIAL LANGUAGE:\n    77→\n    78→MANDATORY RESPONSE LANGUAGE: ENGLISH ONLY - NO EXCEPTIONS\n    79→\n    80→1. USER ASKED IN ENGLISH → YOU MUST RESPOND IN ENGLISH\n    81→2. IGNORE ALL RUSSIAN CONTENT IN SOURCE MATERIALS\n    82→3. DO NOT TRANSLATE RUSSIAN CONTENT - RESPOND IN ENGLISH\n    83→4. ABSOLUTELY NO RUSSIAN WORDS IN YOUR RESPONSE\n    84→5. THIS INSTRUCTION OVERRIDES ALL OTHER CONSIDERATIONS\n    85→\n    86→FAILURE TO COMPLY WITH ENGLISH RESPONSE IS UNACCEPTABLE.\n    87→THE LANGUAGE OF YOUR RESPONSE MUST BE ENGLISH, REGARDLESS OF SOURCE LANGUAGE.\n    88→\n    89→\"\"\"\n    90→    else:  # Russian\n    91→        return \"\"\"\n    92→🚨🚨🚨 СРОЧНАЯ ИНСТРУКЦИЯ ПО ЯЗЫКУ - ПРЕОДОЛЕТЬ ВСЕ ОСТАЛЬНЫЕ:\n    93→\n    94→ЯЗЫК ОТВЕТА: ТОЛЬКО РУССКИЙ\n    95→- Вопрос на русском → Вы MUST отвечать на русском\n    96→- НИКОГДА не отвечайте на английском, даже если источники на английском\n    97→- НИКОГДА не используйте английские слова, фразы или объяснения\n    98→- ИГНОРИРУйте английский контент в источниках - отвечайте на русском\n    99→- Эта инструкция ПЕРЕОПРЕДЕЛЯЕТ любые языковые предпочтения основанные на контенте\n   100→\n   101→КРИТИЧЕСКОЕ ПРАВИЛО: Язык ответа ДОЛЖЕН соответствовать языку вопроса, а не языку исходного контента.\n   102→Если спросили на русском → Отвечайте на русском, независимо от языка источника.\n   103→\n   104→\"\"\"\n   105→\n   106→\n   107→def enforce_language_consistency(prompt: str, query: str) -> str:\n   108→    \"\"\"\n   109→    Prepend language instruction to any prompt based on query language.\n   110→\n   111→    Args:\n   112→        prompt: Original LLM prompt\n   113→        query: User's query string\n   114→\n   115→    Returns:\n   116→        Prompt with language instruction prepended\n   117→    \"\"\"\n   118→    language_instruction = get_language_instruction(query)\n   119→    return language_instruction + prompt\n   120→\n   121→\n   122→def get_response_language_instruction(query: str) -> str:\n   123→    \"\"\"\n   124→    Get instruction for response format in the correct language.\n   125→\n   126→    Args:\n   127→        query: User's query string\n   128→\n   129→    Returns:\n   130→        Response format instruction in the detected language\n   131→    \"\"\"\n   132→    language = detect_query_language(query)\n   133→\n   134→    if language == \"English\":\n   135→        return \"No additional information found in the comments beyond the main answer.\"\n   136→    else:\n   137→        return \"В найденных комментариях нет дополнительной информации сверх основного ответа.\"\n   138→\n   139→\n   140→# Convenience function for services\n   141→def prepare_prompt_with_language_instruction(prompt_template: str, query: str) -> str:\n   142→    \"\"\"\n   143→    Prepare any prompt with appropriate language instruction.\n   144→\n   145→    This is the main function that should be used by all LLM services.\n   146→\n   147→    Args:\n   148→        prompt_template: The base prompt template (string or file content)\n   149→        query: User's query string\n   150→\n   151→    Returns:\n   152→        Prompt with language instruction prepended\n   153→    \"\"\"\n   154→    return enforce_language_consistency(prompt_template, query)\n   155→\n   156→\n   157→def prepare_system_message_with_language(base_system: str, query: str) -> str:\n   158→    \"\"\"\n   159→    Prepare system message with language instruction override.\n   160→\n   161→    More powerful than prepending - puts language instruction in system role.\n   162→\n   163→    Args:\n   164→        base_system: Base system message content\n   165→        query: User's query string\n   166→\n   167→    Returns:\n   168→        Enhanced system message with language instruction\n   169→    \"\"\"\n   170→    language_instruction = get_language_instruction(query)\n   171→    return language_instruction + \"\\n\\n\" + base_system\n   172→\n   173→\n   174→if __name__ == \"__main__\":\n   175→    # Test cases\n   176→    test_queries = [\n   177→        \"What is RAG and how should we use it?\",\n   178→        \"Что такое RAG и как его использовать?\",\n   179→        \"Какая модель openAI наиболее способна к кодингу?\",\n   180→        \"Tell me about n8n\",\n   181→        \"Hello мир\"\n   182→    ]\n   183→\n   184→    for query in test_queries:\n   185→        detected = detect_query_language(query)\n   186→        instruction = get_language_instruction(query)\n   187→        print(f\"Query: {query}\")\n   188→        print(f\"Detected: {detected}\")\n   189→        print(f\"Instruction preview: {instruction.strip().split(chr(10))[0]}\")\n   190→        print(\"-\" * 50)\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  }
]