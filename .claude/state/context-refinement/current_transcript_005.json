[
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Perfect! Now I have all the information I need. This is an analysis request, not a task to work on. Let me provide a comprehensive analysis document directly to the user.\n\nBased on my comprehensive investigation of the multi-expert system, here's the complete context manifest for adding a third expert:\n\n## CONTEXT MANIFEST: Multi-Expert System Architecture\n\n### Executive Summary\n\nThe Experts Panel system uses a **parallel multi-expert architecture** where each expert's content lives in completely isolated silos, connected only by `expert_id`. Adding a new expert requires careful handling of 8 critical tables, 4 unique constraints, and 3 foreign key relationships. The most dangerous pitfall is **channel_id conflicts** - different Telegram channels can have overlapping `telegram_message_id` ranges, which will cause silent data corruption if not handled correctly.\n\n---\n\n## 1. DATABASE ARCHITECTURE & CONSTRAINTS\n\n### Schema Evolution Through Migrations\n\nThe database underwent **8 migrations** to reach its current state. Understanding this history reveals critical constraints:\n\n#### Migration 005-007: The telegram_message_id Saga\n- **005**: Added UNIQUE constraint on `telegram_message_id` (WRONG - globally unique)\n- **006**: Same mistake with full column list\n- **007**: **FIXED** - Changed to composite UNIQUE `(telegram_message_id, channel_id)`\n\n**Why this matters:** `telegram_message_id` is LOCAL to each channel. Different channels restart from ID=1. Without the composite constraint, you can only import ONE expert total.\n\n#### Migration 008: The Comment Constraint Fix (CRITICAL)\n```sql\n-- OLD (broken for multi-expert):\nCREATE UNIQUE INDEX idx_telegram_comment_unique ON comments (telegram_comment_id);\n\n-- NEW (correct):\nCREATE UNIQUE INDEX idx_telegram_comment_post_unique ON comments (telegram_comment_id, post_id);\n```\n\n**Current state check:**\n```bash\nsqlite3 data/experts.db \"SELECT expert_id, channel_id, COUNT(*) FROM posts GROUP BY expert_id, channel_id;\"\n# refat|2273349814|156\n# ai_architect|2293112404|138\n```\n\n### Foreign Key Relationships & Cascade Behavior\n\n```sql\n-- comments table\nFOREIGN KEY(post_id) REFERENCES posts(post_id) ON DELETE CASCADE\n\n-- links table  \nFOREIGN KEY(source_post_id) REFERENCES posts(post_id) ON DELETE CASCADE\nFOREIGN KEY(target_post_id) REFERENCES posts(post_id) ON DELETE CASCADE\n\n-- comment_group_drift table\nFOREIGN KEY(post_id) REFERENCES posts(post_id) -- NO CASCADE!\n```\n\n**Deletion order (critical for rollback):**\n1. Delete from `comment_group_drift` (no cascade, must be manual)\n2. Delete from `comments` (CASCADE from posts)\n3. Delete from `links` (CASCADE from posts)\n4. Delete from `posts` (triggers all cascades)\n\n---\n\n## 2. THE CHANNEL_ID PROBLEM: Root Cause of Data Corruption\n\n### Why telegram_message_id Alone Is Not Enough\n\n```python\n# refat's channel (nobilix)\nchannel_id: 2273349814\ntelegram_message_id: 5, 6, 7, 8...  # Starts from 5\n\n# ai_architect's channel (the_ai_architect)  \nchannel_id: 2293112404\ntelegram_message_id: 2, 3, 4, 5...  # OVERLAPS with refat!\n```\n\n### The Bug in telegram_comments_fetcher.py\n\n**Lines 185-186** (hardcoded channel_id):\n```python\ndef get_posts_from_db(self, db: Session) -> List[tuple]:\n    posts = db.query(Post.post_id, Post.telegram_message_id).filter(\n        Post.channel_id == '2293112404'  # ← HARDCODED!\n    ).order_by(Post.created_at).all()\n```\n\n**Lines 207-210** (critical filter missing):\n```python\npost = db.query(Post).filter(\n    Post.telegram_message_id == comment_data['parent_telegram_message_id'],\n    Post.channel_id == '2293112404'  # ← MUST FILTER BY channel_id!\n).first()\n```\n\n**What happens without channel_id filter:**\n1. You fetch comments for ai_architect (channel 2293112404)\n2. Comment with telegram_comment_id=5 says \"parent_message_id=5\"\n3. Query finds post with telegram_message_id=5 WITHOUT channel_id filter\n4. **Returns refat's post #5** (channel 2273349814) instead of ai_architect's post #5!\n5. Comment gets saved to wrong expert's post\n6. **Data corruption is invisible** - no error, just wrong relationships\n\n### The Solution: Temporary Script Per Expert\n\nThe playbook creates a **temporary modified script** for each expert:\n\n```bash\nCHANNEL_ID=$(cat /tmp/new_expert_channel_id.txt)\ncp src/data/telegram_comments_fetcher.py src/data/telegram_comments_fetcher_temp.py\nsed -i \"s/Post.channel_id == '2293112404'/Post.channel_id == '$CHANNEL_ID'/g\" \\\n    src/data/telegram_comments_fetcher_temp.py\n```\n\nThis is hacky but **necessary** until the script accepts channel_id as a parameter.\n\n---\n\n## 3. DATA IMPORT PIPELINE: Multi-Step Process\n\n### Step 1: JSON Import (json_parser.py)\n\n**Key features:**\n- Accepts `--expert-id` parameter (line 278)\n- **Automatically converts** Telegram entities to markdown via `entities_to_markdown_from_json()` (lines 154-157)\n- Stores `channel_id` extracted from JSON export (line 132)\n- Creates `Post` records with `expert_id` (line 135)\n- Builds `post_id_map` for link resolution (line 103)\n- Creates `Link` records for REPLY/MENTION/FORWARD (lines 196-240)\n\n**Entity conversion** (critical feature added 2025-10-11):\n```python\n# backend/src/utils/entities_converter.py\ndef entities_to_markdown_from_json(text: Union[str, List]) -> str:\n    # Converts Telegram entity types to markdown:\n    # - text_link → [text](url)\n    # - url → bare URL (react-markdown with remark-gfm makes clickable)\n    # - mention → @username\n    # - bold → **text**\n    # - code → `code`\n```\n\n**Output:**\n- Posts in database with `expert_id` field populated\n- Channel_id stored for filtering\n- Markdown-formatted text (NO raw Telegram entities)\n\n### Step 2: Comments Import (telegram_comments_fetcher.py or channel_syncer.py)\n\n**Two paths:**\n\n**A. One-time import** via `telegram_comments_fetcher.py`:\n- Fetches ALL comments for ALL posts via Telegram API\n- Rate-limited: 2 seconds between posts\n- Converts entities to markdown via `entities_to_markdown_from_telethon()` (lines 132-135)\n- **MUST** filter by `channel_id` to avoid cross-expert pollution\n\n**B. Incremental sync** via `channel_syncer.py`:\n- Part of the `/sync` workflow\n- Only fetches new posts since last sync\n- Updates comments for last 20 posts (SYNC_DEPTH constant)\n- Tracks state in `sync_state` table\n- Also filters by `channel_id` (lines 213-217)\n\n**Verification query** (detect wrong assignments):\n```sql\nSELECT COUNT(*) as wrong_comments\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE p.expert_id != 'new_expert_id'\n  AND c.created_at > datetime('now', '-1 hour');\n-- MUST return 0!\n```\n\n### Step 3: Drift Analysis Setup\n\n**Creates empty records** for later analysis:\n```sql\nINSERT INTO comment_group_drift (post_id, has_drift, drift_topics, analyzed_at, analyzed_by, expert_id)\nSELECT\n    p.post_id,\n    0 as has_drift,\n    NULL as drift_topics,\n    datetime('now') as analyzed_at,\n    'pending' as analyzed_by,\n    p.expert_id  -- ← Critical: auto-detected from post\nFROM posts p\nWHERE p.expert_id = 'new_expert'\n  AND p.post_id IN (SELECT DISTINCT post_id FROM comments)\n  AND p.post_id NOT IN (SELECT post_id FROM comment_group_drift);\n```\n\n**Drift agent** (`.claude/agents/drift-extraction.md` line 122):\n```sql\n-- ✅ CORRECT (auto-detects from posts table)\nexpert_id = (SELECT expert_id FROM posts WHERE post_id = <ID>)\n\n-- ❌ WRONG (hardcoded value)\nexpert_id = 'refat'\n```\n\nThe agent uses Claude Sonnet 4.5 to analyze comment groups and extract `drift_topics` JSON.\n\n---\n\n## 4. BACKEND QUERY PIPELINE: Expert Isolation at Every Phase\n\n### Phase 1: Expert Detection (simplified_query_endpoint.py lines 307-315)\n\n```python\nif request.expert_filter:\n    expert_ids = request.expert_filter  # User-specified\nelse:\n    # AUTO-DETECT all experts from database\n    expert_rows = db.query(Post.expert_id).distinct().filter(\n        Post.expert_id.isnot(None)\n    ).all()\n    expert_ids = [row[0] for row in expert_rows if row[0]]\n```\n\n**Default behavior:** Process ALL experts in parallel.\n\n### Phase 2: Parallel Pipeline Per Expert (lines 352-379)\n\n```python\ntasks = []\nfor expert_id in expert_ids:\n    task = asyncio.create_task(\n        process_expert_pipeline(\n            expert_id=expert_id,  # ← Passed to ALL phases\n            request=request,\n            db=db,\n            api_key=api_key,\n            progress_callback=expert_progress_callback\n        )\n    )\n    tasks.append((expert_id, task))\n```\n\nEach expert runs through **6 phases independently**:\n\n#### 1. **Map Phase** (map_service.py)\n- Model: Qwen 2.5-72B Instruct ($0.08/$0.33 per 1M tokens)\n- **Filter:** `Post.expert_id == expert_id` (line 105)\n- Returns posts with relevance: HIGH/MEDIUM/LOW\n\n#### 2. **Filter Phase** (simplified_query_endpoint.py lines 144-148)\n- **NEW (2025-10-14):** Keeps ONLY HIGH relevance posts\n- Was HIGH+MEDIUM, changed for precision\n- Reduces dataset by 60-70%\n\n#### 3. **Resolve Phase** (simple_resolve_service.py lines 75-78)\n- **No GPT** - trusts author's links from `links` table\n- **Filter:** `Post.expert_id == expert_id` (line 77)\n- Depth=1 expansion only\n- Adds posts as \"CONTEXT\" relevance\n\n#### 4. **Reduce Phase** (reduce_service.py lines 19-346)\n- Model: Gemini 2.0 Flash ($0.10/$0.40 per 1M tokens)\n- **Does NOT load expert comments** (removed 2025-10-08)\n- Sorts posts: HIGH → MEDIUM → CONTEXT\n- Limits to 50 posts\n- **Fact validation:** Verifies all [post:ID] references (lines 209-225)\n\n#### 5. **Comment Groups Phase** (comment_group_map_service.py, optional)\n- Model: GPT-4o-mini (fast matching)\n- **NEW (2025-10-14):** Returns only HIGH relevance (was HIGH+MEDIUM)\n- **Filter:** `comment_group_drift.expert_id == expert_id` (line 113)\n- Uses pre-analyzed `drift_topics` from database\n- Excludes `main_sources` from Reduce (8 posts)\n\n#### 6. **Comment Synthesis Phase** (comment_synthesis_service.py, optional)\n- Model: Gemini 2.0 Flash\n- **Filter:** Only processes HIGH comment groups (lines 200-208)\n- Runs AFTER Comment Groups completes\n- **Critical constraint:** NO [post:ID] references in output\n\n### Phase 3: Response Assembly (lines 260-271)\n\n```python\nreturn ExpertResponse(\n    expert_id=expert_id,\n    expert_name=get_expert_name(expert_id),  # ← Helper function\n    channel_username=get_channel_username(expert_id),  # ← Helper function\n    answer=sanitize_for_json(reduce_results.get(\"answer\", \"\")),\n    main_sources=reduce_results.get(\"main_sources\", []),\n    confidence=reduce_results.get(\"confidence\", ConfidenceLevel.MEDIUM),\n    processing_time_ms=processing_time,\n    relevant_comment_groups=sanitize_for_json(comment_groups),\n    comment_groups_synthesis=sanitize_for_json(comment_synthesis)\n)\n```\n\n---\n\n## 5. CONFIGURATION LAYER: Helper Functions\n\n### backend/src/api/models.py (lines 480-495)\n\n```python\ndef get_expert_name(expert_id: str) -> str:\n    \"\"\"Get display name for expert.\"\"\"\n    names = {\n        'refat': 'Refat (Tech & AI)',\n        'ai_architect': 'AI Архитектор'\n    }\n    return names.get(expert_id, expert_id)\n\ndef get_channel_username(expert_id: str) -> str:\n    \"\"\"Get Telegram channel username for expert.\"\"\"\n    channels = {\n        'refat': 'nobilix',\n        'ai_architect': 'the_ai_architect'\n    }\n    return channels.get(expert_id, expert_id)\n```\n\n**Usage throughout backend:**\n- `simplified_query_endpoint.py` lines 118, 229, 262, 551\n- `comment_group_map_service.py` line 95\n- Frontend receives `channel_username` in API response\n\n**Why this pattern:**\n- Avoids hardcoding channel names in database\n- Single source of truth for display names\n- Easy to add new experts (just update these functions)\n\n---\n\n## 6. FRONTEND MULTI-EXPERT UI\n\n### Architecture: Accordion Pattern (ExpertAccordion.tsx)\n\n```typescript\ninterface ExpertAccordionProps {\n  expert: ExpertResponseType;  // From API\n  isExpanded: boolean;\n  onToggle: () => void;\n}\n```\n\n**Key features:**\n- Header shows: expert_name, @channel_username, stats, confidence badge\n- Collapsible body with two columns (50/50 split)\n- Left: Expert answer + comment synthesis\n- Right: Source posts + comment groups\n- Posts loaded lazily when expanded via `apiClient.getPostsByIds()`\n\n### Data Flow\n\n1. **App.tsx** receives `MultiExpertQueryResponse` from SSE stream:\n```typescript\ninterface MultiExpertQueryResponse {\n  query: string;\n  expert_responses: ExpertResponse[];  // Array of expert results\n  total_processing_time_ms: number;\n  request_id: string;\n}\n```\n\n2. **App state** manages expanded experts:\n```typescript\nconst [expandedExperts, setExpandedExperts] = useState<Set<string>>(\n  new Set(['refat', 'ai_architect'])  // ← ADD NEW EXPERT HERE\n);\n```\n\n3. **ExpertAccordion** fetches posts with `expert_id` parameter:\n```typescript\napiClient.getPostsByIds(expert.main_sources, expert.expert_id)\n```\n\n**Why expert_id in getPostsByIds:**\n- Post endpoint filters: `Post.telegram_message_id == post_id AND Post.expert_id == expert_id` (line 529)\n- Prevents conflicts when different experts have same telegram_message_id\n\n---\n\n## 7. ENVIRONMENT & API CONFIGURATION\n\n### Critical Environment Variables (.env)\n\n```bash\n# Backend (backend/.env)\nOPENAI_API_KEY=sk-or-v1-...  # OpenRouter API key\nTELEGRAM_API_ID=12345678      # From https://my.telegram.org\nTELEGRAM_API_HASH=abcdef...   # From https://my.telegram.org\nTELEGRAM_CHANNEL=nobilix      # Used by sync_channel.py\n\n# Optional\nPRODUCTION_ORIGIN=https://...  # CORS for production\n```\n\n### load_dotenv() Placement (backend/src/api/main.py lines 11-12)\n\n```python\nfrom dotenv import load_dotenv\nload_dotenv()  # ← MUST be before other imports!\n```\n\n**Why this is critical:**\n- `uv run` does NOT automatically load .env\n- Environment variables MUST be loaded in Python code\n- If missing: API key not found, health check fails\n\n**Server startup** (from CLAUDE.md):\n```bash\ncd backend && uv run uvicorn src.api.main:app --reload --port 8000\n```\n\n**Health check verification:**\n```bash\ncurl -s http://localhost:8000/health | jq '.openai_configured'\n# Should return: true\n```\n\n---\n\n## 8. MODEL SELECTION STRATEGY (Updated 2025-10-14)\n\n### Multi-Model Architecture\n\nDifferent models optimized for specific tasks:\n\n```python\n# Map Phase (map_service.py line 30)\nDEFAULT_MODEL = \"qwen/qwen-2.5-72b-instruct\"  \n# Cost: $0.08/$0.33 per 1M tokens\n# Why: Excellent at document ranking, fast\n\n# Reduce Phase (reduce_service.py line 26)\nDEFAULT_MODEL = \"gemini-2.0-flash\"\n# Cost: $0.10/$0.40 per 1M tokens\n# Why: Better synthesis quality vs GPT-4o-mini, personal style support\n\n# Comment Synthesis (comment_synthesis_service.py line 24)\nDEFAULT_MODEL = \"gemini-2.0-flash\"\n# Why: Handles context better than GPT-4o-mini\n\n# Comment Groups (comment_group_map_service.py line 34)\nDEFAULT_MODEL = \"gpt-4o-mini\"\n# Why: Fast, cost-effective for keyword matching\n```\n\n### Model Name Conversion (openrouter_adapter.py lines 25-62)\n\n```python\ndef convert_model_name(model: str) -> str:\n    \"\"\"Convert internal model names to OpenRouter format.\"\"\"\n    mapping = {\n        \"gpt-4o-mini\": \"openai/gpt-4o-mini\",\n        \"gemini-2.0-flash\": \"google/gemini-2.0-flash-001\",\n        \"qwen-2.5-72b-instruct\": \"qwen/qwen-2.5-72b-instruct\",\n        \"claude-sonnet-4-5\": \"anthropic/claude-sonnet-4-5\"\n    }\n    return mapping.get(model, model)\n```\n\n---\n\n## 9. DATA INTEGRITY & ORPHAN PREVENTION\n\n### Foreign Key Checks\n\n```sql\n-- Check if foreign keys are enabled\nPRAGMA foreign_keys;  -- Should return: 1\n\n-- View foreign key definitions\nPRAGMA foreign_key_list(comments);\n-- 0|0|posts|post_id|post_id|NO ACTION|CASCADE|NONE\n\nPRAGMA foreign_key_list(links);\n-- 0|0|posts|post_id|post_id|NO ACTION|CASCADE|NONE (source)\n-- 1|0|posts|target_post_id|post_id|NO ACTION|CASCADE|NONE (target)\n\nPRAGMA foreign_key_list(comment_group_drift);\n-- 0|0|posts|post_id|post_id|NO ACTION|NO ACTION|NONE  -- ← NO CASCADE!\n```\n\n### Orphan Detection Queries\n\n```sql\n-- Orphaned comments (should be 0)\nSELECT COUNT(*) FROM comments c\nLEFT JOIN posts p ON c.post_id = p.post_id\nWHERE p.post_id IS NULL;\n\n-- Orphaned links (should be 0)\nSELECT COUNT(*) FROM links l\nLEFT JOIN posts p1 ON l.source_post_id = p1.post_id\nLEFT JOIN posts p2 ON l.target_post_id = p2.post_id\nWHERE p1.post_id IS NULL OR p2.post_id IS NULL;\n\n-- Orphaned drift records (should be 0)\nSELECT COUNT(*) FROM comment_group_drift cgd\nLEFT JOIN posts p ON cgd.post_id = p.post_id\nWHERE p.post_id IS NULL;\n```\n\n### Safe Deletion Order\n\n```sql\n-- Manual cleanup (e.g., removing an expert)\nDELETE FROM comment_group_drift WHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert');\nDELETE FROM comments WHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert');\nDELETE FROM links WHERE source_post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert')\n    OR target_post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert');\nDELETE FROM posts WHERE expert_id='bad_expert';\nVACUUM;  -- Reclaim space\n```\n\n---\n\n## 10. COMMON PITFALLS & SOLUTIONS\n\n### Issue 1: \"MsgIdInvalidError\" During Comment Import\n\n**Root cause:** Using wrong channel_id in query filter.\n\n**Symptom:**\n```\n❌ Error: MsgIdInvalid (post #123)\n```\n\n**Why it happens:**\n- Telegram API requires channel-specific message IDs\n- Querying refat's channel with ai_architect's post IDs = invalid\n\n**Solution:**\n```python\n# WRONG\nposts = db.query(Post.post_id, Post.telegram_message_id).all()\n\n# RIGHT\nposts = db.query(Post.post_id, Post.telegram_message_id).filter(\n    Post.channel_id == '2293112404'\n).all()\n```\n\n### Issue 2: Comments Saved to Wrong Expert's Posts\n\n**Root cause:** Missing channel_id filter when looking up parent post.\n\n**Symptom:** (Silent - no error!)\n```sql\n-- ai_architect comment appears under refat's post\nSELECT p.expert_id, c.author_name, c.comment_text\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE c.created_at > datetime('now', '-1 day')\n  AND p.expert_id != 'ai_architect';  -- Returns rows (BAD!)\n```\n\n**Solution:** Always filter by channel_id when finding parent post:\n```python\npost = db.query(Post).filter(\n    Post.telegram_message_id == parent_msg_id,\n    Post.channel_id == CORRECT_CHANNEL_ID  # ← CRITICAL\n).first()\n```\n\n### Issue 3: Migration 008 Not Applied\n\n**Root cause:** Running migration 006/007 but not 008.\n\n**Symptom:**\n```\nIntegrityError: UNIQUE constraint failed: comments.telegram_comment_id\n```\n\n**Check current state:**\n```bash\nsqlite3 data/experts.db \"PRAGMA index_list('comments');\" | grep unique\n# Should show: idx_telegram_comment_post_unique (compound)\n```\n\n**Fix:**\n```bash\nsqlite3 data/experts.db < backend/migrations/008_fix_comment_unique_constraint.sql\n```\n\n### Issue 4: API Key Not Loading\n\n**Root cause:** Missing `load_dotenv()` in main.py.\n\n**Symptom:**\n```bash\ncurl http://localhost:8000/health | jq '.openai_configured'\n# Returns: false\n```\n\n**Fix:** Ensure lines 11-12 of backend/src/api/main.py:\n```python\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n### Issue 5: Drift Analysis Lost After Reimport\n\n**Root cause:** Drift records have `post_id` foreign key, but posts get deleted/recreated.\n\n**Symptom:**\n```sql\nSELECT COUNT(*) FROM comment_group_drift WHERE has_drift=1;\n-- Returns: 0 (all lost!)\n```\n\n**Prevention:** Backup drift analysis before reimporting:\n```bash\nsqlite3 data/experts.db \".dump comment_group_drift\" > drift_backup.sql\n# After reimport:\nsqlite3 data/experts.db < drift_backup.sql\n```\n\n---\n\n## 11. TESTING & VERIFICATION CHECKLIST\n\n### Pre-Flight Checks (Before Adding Expert)\n\n```bash\n# 1. Migration 008 applied?\nsqlite3 data/experts.db \"PRAGMA index_list('comments');\" | grep -c idx_telegram_comment_post_unique\n# Expected: 1\n\n# 2. load_dotenv() present?\ngrep -c \"load_dotenv()\" backend/src/api/main.py\n# Expected: 1\n\n# 3. Backend/frontend stopped?\nlsof -ti:8000  # Should be empty\nlsof -ti:5173  # Should be empty\n\n# 4. Current experts & their channel_ids?\nsqlite3 data/experts.db \"SELECT expert_id, channel_id, COUNT(*) FROM posts GROUP BY expert_id, channel_id;\"\n```\n\n### Post-Import Verification\n\n```bash\n# 1. Data imported correctly?\nsqlite3 data/experts.db \"\nSELECT\n    expert_id,\n    COUNT(DISTINCT post_id) as posts,\n    COUNT(DISTINCT channel_id) as channels,\n    MIN(telegram_message_id) as first_msg,\n    MAX(telegram_message_id) as last_msg\nFROM posts\nWHERE expert_id='new_expert'\nGROUP BY expert_id;\"\n\n# 2. No duplicate telegram_message_ids within same channel?\nsqlite3 data/experts.db \"\nSELECT telegram_message_id, channel_id, COUNT(*) as cnt\nFROM posts\nWHERE expert_id='new_expert'\nGROUP BY telegram_message_id, channel_id\nHAVING cnt > 1;\"\n# Expected: empty\n\n# 3. Comments linked correctly?\nsqlite3 data/experts.db \"\nSELECT\n    p.expert_id,\n    COUNT(DISTINCT c.post_id) as posts_with_comments,\n    COUNT(c.comment_id) as total_comments\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE p.expert_id='new_expert'\nGROUP BY p.expert_id;\"\n\n# 4. No cross-expert comment pollution?\nsqlite3 data/experts.db \"\nSELECT COUNT(*) as wrong_assignments\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE p.expert_id != 'new_expert'\n  AND c.created_at > datetime('now', '-2 hours');\"\n# Expected: 0\n```\n\n### API Testing\n\n```bash\n# 1. Health check\ncurl -s http://localhost:8000/health | jq '{status, database, openai_configured}'\n\n# 2. Single expert query\ncurl -X POST http://localhost:8000/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"test\", \"expert_filter\": [\"new_expert\"], \"stream_progress\": false}' \\\n  -o /tmp/test.json\n\n# 3. Verify response\ncat /tmp/test.json | jq -r '.expert_responses[] | \"\\(.expert_id): \\(.posts_analyzed) posts, \\(.main_sources | length) sources\"'\n\n# 4. Check post retrieval\ncurl -s \"http://localhost:8000/api/v1/posts/5?expert_id=new_expert\" | jq '.telegram_message_id, .channel_name'\n```\n\n### Frontend Testing\n\n```typescript\n// 1. Check expert appears in expandedExperts (App.tsx)\nconst [expandedExperts, setExpandedExperts] = useState<Set<string>>(\n  new Set(['refat', 'ai_architect', 'new_expert'])  // ← Should be present\n);\n\n// 2. Verify helper functions work\n// backend/src/api/models.py\nget_expert_name('new_expert')  // Should return display name\nget_channel_username('new_expert')  // Should return Telegram username\n\n// 3. Test UI rendering\nnpm run dev\n// Open http://localhost:5173\n// Submit query, check new expert accordion appears\n```\n\n---\n\n## 12. ROLLBACK PROCEDURES\n\n### Full Rollback to Backup\n\n```bash\n# Restore from backup\nBACKUP_NAME=$(cat /tmp/last_backup.txt)\ncp \"data/backups/$BACKUP_NAME\" data/experts.db\necho \"✅ Database restored\"\n\n# Verify\nsqlite3 data/experts.db \"SELECT COUNT(*) FROM posts WHERE expert_id='new_expert';\"\n# Expected: 0\n```\n\n### Selective Expert Removal\n\n```bash\nsqlite3 data/experts.db <<SQL\nBEGIN TRANSACTION;\n\n-- Step 1: Remove drift records (no CASCADE)\nDELETE FROM comment_group_drift \nWHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert');\n\n-- Step 2: Remove comments (CASCADE works)\nDELETE FROM comments \nWHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert');\n\n-- Step 3: Remove links (CASCADE works)\nDELETE FROM links \nWHERE source_post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert')\n   OR target_post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert');\n\n-- Step 4: Remove posts (triggers remaining cascades)\nDELETE FROM posts WHERE expert_id='new_expert';\n\n-- Step 5: Reclaim space\nVACUUM;\n\nCOMMIT;\nSQL\n```\n\n### Code Rollback\n\n```bash\n# Revert helper functions\ngit checkout backend/src/api/models.py\n\n# Revert frontend\ngit checkout frontend/src/App.tsx\n\n# Verify\ngit status\n```\n\n---\n\n## 13. STEP-BY-STEP EXECUTION SEQUENCE\n\nBased on `docs/add-new-expert-playbook.md`, here's the recommended execution order with time estimates:\n\n| Step | Task | Time | Critical Checks |\n|------|------|------|----------------|\n| 0 | Prerequisites | 5 min | Migration 008, load_dotenv(), servers stopped |\n| 1 | Database backup | 2 min | Backup created, size verified |\n| 2 | Import posts from JSON | 5-10 min | Post count, channel_id uniqueness, no duplicates |\n| 3 | Update models.py | 2 min | Helper functions return correct values |\n| 4 | Import comments (optional) | 15-60 min | Comments linked correctly, no cross-expert pollution |\n| 5 | Create drift records | 2 min | Empty records created with correct expert_id |\n| 6 | Run drift analysis (optional) | 20-90 min | Analysis complete, drift_topics populated |\n| 7 | Final data verification | 5 min | All integrity checks pass |\n| 8 | API testing | 5 min | Health check, query test, post retrieval |\n| 9 | Update frontend | 2 min | expandedExperts updated, type-check passes |\n| 10 | E2E testing | 10 min | Full query to all experts, UI renders correctly |\n| 11 | Git commit | 5 min | Changes committed with proper message |\n| 12 | Documentation | 5 min | CLAUDE.md updated |\n\n**Total time:** 1.5-3 hours (depending on drift analysis)\n\n---\n\n## FINAL RECOMMENDATIONS\n\n### Must-Have Before Starting\n1. ✅ Migration 008 applied\n2. ✅ `load_dotenv()` in main.py\n3. ✅ Database backup created\n4. ✅ Understand channel_id vs telegram_message_id distinction\n5. ✅ Have Telegram API credentials ready\n\n### During Execution\n1. **Never skip channel_id filter** in any database query involving posts/comments\n2. **Always verify** channel_id is unique for new expert\n3. **Use temporary script** for comment import (playbook Step 4)\n4. **Test incrementally** - don't wait until the end to verify data\n5. **Check for cross-expert pollution** after every import step\n\n### After Completion\n1. **Run full verification suite** (Section 11)\n2. **Test query with `expert_filter`** to ensure isolation works\n3. **Check frontend** displays new expert correctly\n4. **Document** any deviations from playbook in CLAUDE.md\n5. **Keep backup** for at least 7 days\n\n---\n\nThis context manifest represents the complete understanding needed to add a third expert without issues. The key insight: **expert isolation happens at the database level via expert_id and channel_id filtering** - miss either filter in any query, and you get silent data corruption."
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01Y8JUyeqWUGSYPwTD1PLvPt",
        "type": "tool_result",
        "content": [
          {
            "type": "text",
            "text": "Perfect! Now I have all the information I need. This is an analysis request, not a task to work on. Let me provide a comprehensive analysis document directly to the user.\n\nBased on my comprehensive investigation of the multi-expert system, here's the complete context manifest for adding a third expert:\n\n## CONTEXT MANIFEST: Multi-Expert System Architecture\n\n### Executive Summary\n\nThe Experts Panel system uses a **parallel multi-expert architecture** where each expert's content lives in completely isolated silos, connected only by `expert_id`. Adding a new expert requires careful handling of 8 critical tables, 4 unique constraints, and 3 foreign key relationships. The most dangerous pitfall is **channel_id conflicts** - different Telegram channels can have overlapping `telegram_message_id` ranges, which will cause silent data corruption if not handled correctly.\n\n---\n\n## 1. DATABASE ARCHITECTURE & CONSTRAINTS\n\n### Schema Evolution Through Migrations\n\nThe database underwent **8 migrations** to reach its current state. Understanding this history reveals critical constraints:\n\n#### Migration 005-007: The telegram_message_id Saga\n- **005**: Added UNIQUE constraint on `telegram_message_id` (WRONG - globally unique)\n- **006**: Same mistake with full column list\n- **007**: **FIXED** - Changed to composite UNIQUE `(telegram_message_id, channel_id)`\n\n**Why this matters:** `telegram_message_id` is LOCAL to each channel. Different channels restart from ID=1. Without the composite constraint, you can only import ONE expert total.\n\n#### Migration 008: The Comment Constraint Fix (CRITICAL)\n```sql\n-- OLD (broken for multi-expert):\nCREATE UNIQUE INDEX idx_telegram_comment_unique ON comments (telegram_comment_id);\n\n-- NEW (correct):\nCREATE UNIQUE INDEX idx_telegram_comment_post_unique ON comments (telegram_comment_id, post_id);\n```\n\n**Current state check:**\n```bash\nsqlite3 data/experts.db \"SELECT expert_id, channel_id, COUNT(*) FROM posts GROUP BY expert_id, channel_id;\"\n# refat|2273349814|156\n# ai_architect|2293112404|138\n```\n\n### Foreign Key Relationships & Cascade Behavior\n\n```sql\n-- comments table\nFOREIGN KEY(post_id) REFERENCES posts(post_id) ON DELETE CASCADE\n\n-- links table  \nFOREIGN KEY(source_post_id) REFERENCES posts(post_id) ON DELETE CASCADE\nFOREIGN KEY(target_post_id) REFERENCES posts(post_id) ON DELETE CASCADE\n\n-- comment_group_drift table\nFOREIGN KEY(post_id) REFERENCES posts(post_id) -- NO CASCADE!\n```\n\n**Deletion order (critical for rollback):**\n1. Delete from `comment_group_drift` (no cascade, must be manual)\n2. Delete from `comments` (CASCADE from posts)\n3. Delete from `links` (CASCADE from posts)\n4. Delete from `posts` (triggers all cascades)\n\n---\n\n## 2. THE CHANNEL_ID PROBLEM: Root Cause of Data Corruption\n\n### Why telegram_message_id Alone Is Not Enough\n\n```python\n# refat's channel (nobilix)\nchannel_id: 2273349814\ntelegram_message_id: 5, 6, 7, 8...  # Starts from 5\n\n# ai_architect's channel (the_ai_architect)  \nchannel_id: 2293112404\ntelegram_message_id: 2, 3, 4, 5...  # OVERLAPS with refat!\n```\n\n### The Bug in telegram_comments_fetcher.py\n\n**Lines 185-186** (hardcoded channel_id):\n```python\ndef get_posts_from_db(self, db: Session) -> List[tuple]:\n    posts = db.query(Post.post_id, Post.telegram_message_id).filter(\n        Post.channel_id == '2293112404'  # ← HARDCODED!\n    ).order_by(Post.created_at).all()\n```\n\n**Lines 207-210** (critical filter missing):\n```python\npost = db.query(Post).filter(\n    Post.telegram_message_id == comment_data['parent_telegram_message_id'],\n    Post.channel_id == '2293112404'  # ← MUST FILTER BY channel_id!\n).first()\n```\n\n**What happens without channel_id filter:**\n1. You fetch comments for ai_architect (channel 2293112404)\n2. Comment with telegram_comment_id=5 says \"parent_message_id=5\"\n3. Query finds post with telegram_message_id=5 WITHOUT channel_id filter\n4. **Returns refat's post #5** (channel 2273349814) instead of ai_architect's post #5!\n5. Comment gets saved to wrong expert's post\n6. **Data corruption is invisible** - no error, just wrong relationships\n\n### The Solution: Temporary Script Per Expert\n\nThe playbook creates a **temporary modified script** for each expert:\n\n```bash\nCHANNEL_ID=$(cat /tmp/new_expert_channel_id.txt)\ncp src/data/telegram_comments_fetcher.py src/data/telegram_comments_fetcher_temp.py\nsed -i \"s/Post.channel_id == '2293112404'/Post.channel_id == '$CHANNEL_ID'/g\" \\\n    src/data/telegram_comments_fetcher_temp.py\n```\n\nThis is hacky but **necessary** until the script accepts channel_id as a parameter.\n\n---\n\n## 3. DATA IMPORT PIPELINE: Multi-Step Process\n\n### Step 1: JSON Import (json_parser.py)\n\n**Key features:**\n- Accepts `--expert-id` parameter (line 278)\n- **Automatically converts** Telegram entities to markdown via `entities_to_markdown_from_json()` (lines 154-157)\n- Stores `channel_id` extracted from JSON export (line 132)\n- Creates `Post` records with `expert_id` (line 135)\n- Builds `post_id_map` for link resolution (line 103)\n- Creates `Link` records for REPLY/MENTION/FORWARD (lines 196-240)\n\n**Entity conversion** (critical feature added 2025-10-11):\n```python\n# backend/src/utils/entities_converter.py\ndef entities_to_markdown_from_json(text: Union[str, List]) -> str:\n    # Converts Telegram entity types to markdown:\n    # - text_link → [text](url)\n    # - url → bare URL (react-markdown with remark-gfm makes clickable)\n    # - mention → @username\n    # - bold → **text**\n    # - code → `code`\n```\n\n**Output:**\n- Posts in database with `expert_id` field populated\n- Channel_id stored for filtering\n- Markdown-formatted text (NO raw Telegram entities)\n\n### Step 2: Comments Import (telegram_comments_fetcher.py or channel_syncer.py)\n\n**Two paths:**\n\n**A. One-time import** via `telegram_comments_fetcher.py`:\n- Fetches ALL comments for ALL posts via Telegram API\n- Rate-limited: 2 seconds between posts\n- Converts entities to markdown via `entities_to_markdown_from_telethon()` (lines 132-135)\n- **MUST** filter by `channel_id` to avoid cross-expert pollution\n\n**B. Incremental sync** via `channel_syncer.py`:\n- Part of the `/sync` workflow\n- Only fetches new posts since last sync\n- Updates comments for last 20 posts (SYNC_DEPTH constant)\n- Tracks state in `sync_state` table\n- Also filters by `channel_id` (lines 213-217)\n\n**Verification query** (detect wrong assignments):\n```sql\nSELECT COUNT(*) as wrong_comments\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE p.expert_id != 'new_expert_id'\n  AND c.created_at > datetime('now', '-1 hour');\n-- MUST return 0!\n```\n\n### Step 3: Drift Analysis Setup\n\n**Creates empty records** for later analysis:\n```sql\nINSERT INTO comment_group_drift (post_id, has_drift, drift_topics, analyzed_at, analyzed_by, expert_id)\nSELECT\n    p.post_id,\n    0 as has_drift,\n    NULL as drift_topics,\n    datetime('now') as analyzed_at,\n    'pending' as analyzed_by,\n    p.expert_id  -- ← Critical: auto-detected from post\nFROM posts p\nWHERE p.expert_id = 'new_expert'\n  AND p.post_id IN (SELECT DISTINCT post_id FROM comments)\n  AND p.post_id NOT IN (SELECT post_id FROM comment_group_drift);\n```\n\n**Drift agent** (`.claude/agents/drift-extraction.md` line 122):\n```sql\n-- ✅ CORRECT (auto-detects from posts table)\nexpert_id = (SELECT expert_id FROM posts WHERE post_id = <ID>)\n\n-- ❌ WRONG (hardcoded value)\nexpert_id = 'refat'\n```\n\nThe agent uses Claude Sonnet 4.5 to analyze comment groups and extract `drift_topics` JSON.\n\n---\n\n## 4. BACKEND QUERY PIPELINE: Expert Isolation at Every Phase\n\n### Phase 1: Expert Detection (simplified_query_endpoint.py lines 307-315)\n\n```python\nif request.expert_filter:\n    expert_ids = request.expert_filter  # User-specified\nelse:\n    # AUTO-DETECT all experts from database\n    expert_rows = db.query(Post.expert_id).distinct().filter(\n        Post.expert_id.isnot(None)\n    ).all()\n    expert_ids = [row[0] for row in expert_rows if row[0]]\n```\n\n**Default behavior:** Process ALL experts in parallel.\n\n### Phase 2: Parallel Pipeline Per Expert (lines 352-379)\n\n```python\ntasks = []\nfor expert_id in expert_ids:\n    task = asyncio.create_task(\n        process_expert_pipeline(\n            expert_id=expert_id,  # ← Passed to ALL phases\n            request=request,\n            db=db,\n            api_key=api_key,\n            progress_callback=expert_progress_callback\n        )\n    )\n    tasks.append((expert_id, task))\n```\n\nEach expert runs through **6 phases independently**:\n\n#### 1. **Map Phase** (map_service.py)\n- Model: Qwen 2.5-72B Instruct ($0.08/$0.33 per 1M tokens)\n- **Filter:** `Post.expert_id == expert_id` (line 105)\n- Returns posts with relevance: HIGH/MEDIUM/LOW\n\n#### 2. **Filter Phase** (simplified_query_endpoint.py lines 144-148)\n- **NEW (2025-10-14):** Keeps ONLY HIGH relevance posts\n- Was HIGH+MEDIUM, changed for precision\n- Reduces dataset by 60-70%\n\n#### 3. **Resolve Phase** (simple_resolve_service.py lines 75-78)\n- **No GPT** - trusts author's links from `links` table\n- **Filter:** `Post.expert_id == expert_id` (line 77)\n- Depth=1 expansion only\n- Adds posts as \"CONTEXT\" relevance\n\n#### 4. **Reduce Phase** (reduce_service.py lines 19-346)\n- Model: Gemini 2.0 Flash ($0.10/$0.40 per 1M tokens)\n- **Does NOT load expert comments** (removed 2025-10-08)\n- Sorts posts: HIGH → MEDIUM → CONTEXT\n- Limits to 50 posts\n- **Fact validation:** Verifies all [post:ID] references (lines 209-225)\n\n#### 5. **Comment Groups Phase** (comment_group_map_service.py, optional)\n- Model: GPT-4o-mini (fast matching)\n- **NEW (2025-10-14):** Returns only HIGH relevance (was HIGH+MEDIUM)\n- **Filter:** `comment_group_drift.expert_id == expert_id` (line 113)\n- Uses pre-analyzed `drift_topics` from database\n- Excludes `main_sources` from Reduce (8 posts)\n\n#### 6. **Comment Synthesis Phase** (comment_synthesis_service.py, optional)\n- Model: Gemini 2.0 Flash\n- **Filter:** Only processes HIGH comment groups (lines 200-208)\n- Runs AFTER Comment Groups completes\n- **Critical constraint:** NO [post:ID] references in output\n\n### Phase 3: Response Assembly (lines 260-271)\n\n```python\nreturn ExpertResponse(\n    expert_id=expert_id,\n    expert_name=get_expert_name(expert_id),  # ← Helper function\n    channel_username=get_channel_username(expert_id),  # ← Helper function\n    answer=sanitize_for_json(reduce_results.get(\"answer\", \"\")),\n    main_sources=reduce_results.get(\"main_sources\", []),\n    confidence=reduce_results.get(\"confidence\", ConfidenceLevel.MEDIUM),\n    processing_time_ms=processing_time,\n    relevant_comment_groups=sanitize_for_json(comment_groups),\n    comment_groups_synthesis=sanitize_for_json(comment_synthesis)\n)\n```\n\n---\n\n## 5. CONFIGURATION LAYER: Helper Functions\n\n### backend/src/api/models.py (lines 480-495)\n\n```python\ndef get_expert_name(expert_id: str) -> str:\n    \"\"\"Get display name for expert.\"\"\"\n    names = {\n        'refat': 'Refat (Tech & AI)',\n        'ai_architect': 'AI Архитектор'\n    }\n    return names.get(expert_id, expert_id)\n\ndef get_channel_username(expert_id: str) -> str:\n    \"\"\"Get Telegram channel username for expert.\"\"\"\n    channels = {\n        'refat': 'nobilix',\n        'ai_architect': 'the_ai_architect'\n    }\n    return channels.get(expert_id, expert_id)\n```\n\n**Usage throughout backend:**\n- `simplified_query_endpoint.py` lines 118, 229, 262, 551\n- `comment_group_map_service.py` line 95\n- Frontend receives `channel_username` in API response\n\n**Why this pattern:**\n- Avoids hardcoding channel names in database\n- Single source of truth for display names\n- Easy to add new experts (just update these functions)\n\n---\n\n## 6. FRONTEND MULTI-EXPERT UI\n\n### Architecture: Accordion Pattern (ExpertAccordion.tsx)\n\n```typescript\ninterface ExpertAccordionProps {\n  expert: ExpertResponseType;  // From API\n  isExpanded: boolean;\n  onToggle: () => void;\n}\n```\n\n**Key features:**\n- Header shows: expert_name, @channel_username, stats, confidence badge\n- Collapsible body with two columns (50/50 split)\n- Left: Expert answer + comment synthesis\n- Right: Source posts + comment groups\n- Posts loaded lazily when expanded via `apiClient.getPostsByIds()`\n\n### Data Flow\n\n1. **App.tsx** receives `MultiExpertQueryResponse` from SSE stream:\n```typescript\ninterface MultiExpertQueryResponse {\n  query: string;\n  expert_responses: ExpertResponse[];  // Array of expert results\n  total_processing_time_ms: number;\n  request_id: string;\n}\n```\n\n2. **App state** manages expanded experts:\n```typescript\nconst [expandedExperts, setExpandedExperts] = useState<Set<string>>(\n  new Set(['refat', 'ai_architect'])  // ← ADD NEW EXPERT HERE\n);\n```\n\n3. **ExpertAccordion** fetches posts with `expert_id` parameter:\n```typescript\napiClient.getPostsByIds(expert.main_sources, expert.expert_id)\n```\n\n**Why expert_id in getPostsByIds:**\n- Post endpoint filters: `Post.telegram_message_id == post_id AND Post.expert_id == expert_id` (line 529)\n- Prevents conflicts when different experts have same telegram_message_id\n\n---\n\n## 7. ENVIRONMENT & API CONFIGURATION\n\n### Critical Environment Variables (.env)\n\n```bash\n# Backend (backend/.env)\nOPENAI_API_KEY=sk-or-v1-...  # OpenRouter API key\nTELEGRAM_API_ID=12345678      # From https://my.telegram.org\nTELEGRAM_API_HASH=abcdef...   # From https://my.telegram.org\nTELEGRAM_CHANNEL=nobilix      # Used by sync_channel.py\n\n# Optional\nPRODUCTION_ORIGIN=https://...  # CORS for production\n```\n\n### load_dotenv() Placement (backend/src/api/main.py lines 11-12)\n\n```python\nfrom dotenv import load_dotenv\nload_dotenv()  # ← MUST be before other imports!\n```\n\n**Why this is critical:**\n- `uv run` does NOT automatically load .env\n- Environment variables MUST be loaded in Python code\n- If missing: API key not found, health check fails\n\n**Server startup** (from CLAUDE.md):\n```bash\ncd backend && uv run uvicorn src.api.main:app --reload --port 8000\n```\n\n**Health check verification:**\n```bash\ncurl -s http://localhost:8000/health | jq '.openai_configured'\n# Should return: true\n```\n\n---\n\n## 8. MODEL SELECTION STRATEGY (Updated 2025-10-14)\n\n### Multi-Model Architecture\n\nDifferent models optimized for specific tasks:\n\n```python\n# Map Phase (map_service.py line 30)\nDEFAULT_MODEL = \"qwen/qwen-2.5-72b-instruct\"  \n# Cost: $0.08/$0.33 per 1M tokens\n# Why: Excellent at document ranking, fast\n\n# Reduce Phase (reduce_service.py line 26)\nDEFAULT_MODEL = \"gemini-2.0-flash\"\n# Cost: $0.10/$0.40 per 1M tokens\n# Why: Better synthesis quality vs GPT-4o-mini, personal style support\n\n# Comment Synthesis (comment_synthesis_service.py line 24)\nDEFAULT_MODEL = \"gemini-2.0-flash\"\n# Why: Handles context better than GPT-4o-mini\n\n# Comment Groups (comment_group_map_service.py line 34)\nDEFAULT_MODEL = \"gpt-4o-mini\"\n# Why: Fast, cost-effective for keyword matching\n```\n\n### Model Name Conversion (openrouter_adapter.py lines 25-62)\n\n```python\ndef convert_model_name(model: str) -> str:\n    \"\"\"Convert internal model names to OpenRouter format.\"\"\"\n    mapping = {\n        \"gpt-4o-mini\": \"openai/gpt-4o-mini\",\n        \"gemini-2.0-flash\": \"google/gemini-2.0-flash-001\",\n        \"qwen-2.5-72b-instruct\": \"qwen/qwen-2.5-72b-instruct\",\n        \"claude-sonnet-4-5\": \"anthropic/claude-sonnet-4-5\"\n    }\n    return mapping.get(model, model)\n```\n\n---\n\n## 9. DATA INTEGRITY & ORPHAN PREVENTION\n\n### Foreign Key Checks\n\n```sql\n-- Check if foreign keys are enabled\nPRAGMA foreign_keys;  -- Should return: 1\n\n-- View foreign key definitions\nPRAGMA foreign_key_list(comments);\n-- 0|0|posts|post_id|post_id|NO ACTION|CASCADE|NONE\n\nPRAGMA foreign_key_list(links);\n-- 0|0|posts|post_id|post_id|NO ACTION|CASCADE|NONE (source)\n-- 1|0|posts|target_post_id|post_id|NO ACTION|CASCADE|NONE (target)\n\nPRAGMA foreign_key_list(comment_group_drift);\n-- 0|0|posts|post_id|post_id|NO ACTION|NO ACTION|NONE  -- ← NO CASCADE!\n```\n\n### Orphan Detection Queries\n\n```sql\n-- Orphaned comments (should be 0)\nSELECT COUNT(*) FROM comments c\nLEFT JOIN posts p ON c.post_id = p.post_id\nWHERE p.post_id IS NULL;\n\n-- Orphaned links (should be 0)\nSELECT COUNT(*) FROM links l\nLEFT JOIN posts p1 ON l.source_post_id = p1.post_id\nLEFT JOIN posts p2 ON l.target_post_id = p2.post_id\nWHERE p1.post_id IS NULL OR p2.post_id IS NULL;\n\n-- Orphaned drift records (should be 0)\nSELECT COUNT(*) FROM comment_group_drift cgd\nLEFT JOIN posts p ON cgd.post_id = p.post_id\nWHERE p.post_id IS NULL;\n```\n\n### Safe Deletion Order\n\n```sql\n-- Manual cleanup (e.g., removing an expert)\nDELETE FROM comment_group_drift WHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert');\nDELETE FROM comments WHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert');\nDELETE FROM links WHERE source_post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert')\n    OR target_post_id IN (SELECT post_id FROM posts WHERE expert_id='bad_expert');\nDELETE FROM posts WHERE expert_id='bad_expert';\nVACUUM;  -- Reclaim space\n```\n\n---\n\n## 10. COMMON PITFALLS & SOLUTIONS\n\n### Issue 1: \"MsgIdInvalidError\" During Comment Import\n\n**Root cause:** Using wrong channel_id in query filter.\n\n**Symptom:**\n```\n❌ Error: MsgIdInvalid (post #123)\n```\n\n**Why it happens:**\n- Telegram API requires channel-specific message IDs\n- Querying refat's channel with ai_architect's post IDs = invalid\n\n**Solution:**\n```python\n# WRONG\nposts = db.query(Post.post_id, Post.telegram_message_id).all()\n\n# RIGHT\nposts = db.query(Post.post_id, Post.telegram_message_id).filter(\n    Post.channel_id == '2293112404'\n).all()\n```\n\n### Issue 2: Comments Saved to Wrong Expert's Posts\n\n**Root cause:** Missing channel_id filter when looking up parent post.\n\n**Symptom:** (Silent - no error!)\n```sql\n-- ai_architect comment appears under refat's post\nSELECT p.expert_id, c.author_name, c.comment_text\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE c.created_at > datetime('now', '-1 day')\n  AND p.expert_id != 'ai_architect';  -- Returns rows (BAD!)\n```\n\n**Solution:** Always filter by channel_id when finding parent post:\n```python\npost = db.query(Post).filter(\n    Post.telegram_message_id == parent_msg_id,\n    Post.channel_id == CORRECT_CHANNEL_ID  # ← CRITICAL\n).first()\n```\n\n### Issue 3: Migration 008 Not Applied\n\n**Root cause:** Running migration 006/007 but not 008.\n\n**Symptom:**\n```\nIntegrityError: UNIQUE constraint failed: comments.telegram_comment_id\n```\n\n**Check current state:**\n```bash\nsqlite3 data/experts.db \"PRAGMA index_list('comments');\" | grep unique\n# Should show: idx_telegram_comment_post_unique (compound)\n```\n\n**Fix:**\n```bash\nsqlite3 data/experts.db < backend/migrations/008_fix_comment_unique_constraint.sql\n```\n\n### Issue 4: API Key Not Loading\n\n**Root cause:** Missing `load_dotenv()` in main.py.\n\n**Symptom:**\n```bash\ncurl http://localhost:8000/health | jq '.openai_configured'\n# Returns: false\n```\n\n**Fix:** Ensure lines 11-12 of backend/src/api/main.py:\n```python\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n### Issue 5: Drift Analysis Lost After Reimport\n\n**Root cause:** Drift records have `post_id` foreign key, but posts get deleted/recreated.\n\n**Symptom:**\n```sql\nSELECT COUNT(*) FROM comment_group_drift WHERE has_drift=1;\n-- Returns: 0 (all lost!)\n```\n\n**Prevention:** Backup drift analysis before reimporting:\n```bash\nsqlite3 data/experts.db \".dump comment_group_drift\" > drift_backup.sql\n# After reimport:\nsqlite3 data/experts.db < drift_backup.sql\n```\n\n---\n\n## 11. TESTING & VERIFICATION CHECKLIST\n\n### Pre-Flight Checks (Before Adding Expert)\n\n```bash\n# 1. Migration 008 applied?\nsqlite3 data/experts.db \"PRAGMA index_list('comments');\" | grep -c idx_telegram_comment_post_unique\n# Expected: 1\n\n# 2. load_dotenv() present?\ngrep -c \"load_dotenv()\" backend/src/api/main.py\n# Expected: 1\n\n# 3. Backend/frontend stopped?\nlsof -ti:8000  # Should be empty\nlsof -ti:5173  # Should be empty\n\n# 4. Current experts & their channel_ids?\nsqlite3 data/experts.db \"SELECT expert_id, channel_id, COUNT(*) FROM posts GROUP BY expert_id, channel_id;\"\n```\n\n### Post-Import Verification\n\n```bash\n# 1. Data imported correctly?\nsqlite3 data/experts.db \"\nSELECT\n    expert_id,\n    COUNT(DISTINCT post_id) as posts,\n    COUNT(DISTINCT channel_id) as channels,\n    MIN(telegram_message_id) as first_msg,\n    MAX(telegram_message_id) as last_msg\nFROM posts\nWHERE expert_id='new_expert'\nGROUP BY expert_id;\"\n\n# 2. No duplicate telegram_message_ids within same channel?\nsqlite3 data/experts.db \"\nSELECT telegram_message_id, channel_id, COUNT(*) as cnt\nFROM posts\nWHERE expert_id='new_expert'\nGROUP BY telegram_message_id, channel_id\nHAVING cnt > 1;\"\n# Expected: empty\n\n# 3. Comments linked correctly?\nsqlite3 data/experts.db \"\nSELECT\n    p.expert_id,\n    COUNT(DISTINCT c.post_id) as posts_with_comments,\n    COUNT(c.comment_id) as total_comments\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE p.expert_id='new_expert'\nGROUP BY p.expert_id;\"\n\n# 4. No cross-expert comment pollution?\nsqlite3 data/experts.db \"\nSELECT COUNT(*) as wrong_assignments\nFROM comments c\nJOIN posts p ON c.post_id = p.post_id\nWHERE p.expert_id != 'new_expert'\n  AND c.created_at > datetime('now', '-2 hours');\"\n# Expected: 0\n```\n\n### API Testing\n\n```bash\n# 1. Health check\ncurl -s http://localhost:8000/health | jq '{status, database, openai_configured}'\n\n# 2. Single expert query\ncurl -X POST http://localhost:8000/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"test\", \"expert_filter\": [\"new_expert\"], \"stream_progress\": false}' \\\n  -o /tmp/test.json\n\n# 3. Verify response\ncat /tmp/test.json | jq -r '.expert_responses[] | \"\\(.expert_id): \\(.posts_analyzed) posts, \\(.main_sources | length) sources\"'\n\n# 4. Check post retrieval\ncurl -s \"http://localhost:8000/api/v1/posts/5?expert_id=new_expert\" | jq '.telegram_message_id, .channel_name'\n```\n\n### Frontend Testing\n\n```typescript\n// 1. Check expert appears in expandedExperts (App.tsx)\nconst [expandedExperts, setExpandedExperts] = useState<Set<string>>(\n  new Set(['refat', 'ai_architect', 'new_expert'])  // ← Should be present\n);\n\n// 2. Verify helper functions work\n// backend/src/api/models.py\nget_expert_name('new_expert')  // Should return display name\nget_channel_username('new_expert')  // Should return Telegram username\n\n// 3. Test UI rendering\nnpm run dev\n// Open http://localhost:5173\n// Submit query, check new expert accordion appears\n```\n\n---\n\n## 12. ROLLBACK PROCEDURES\n\n### Full Rollback to Backup\n\n```bash\n# Restore from backup\nBACKUP_NAME=$(cat /tmp/last_backup.txt)\ncp \"data/backups/$BACKUP_NAME\" data/experts.db\necho \"✅ Database restored\"\n\n# Verify\nsqlite3 data/experts.db \"SELECT COUNT(*) FROM posts WHERE expert_id='new_expert';\"\n# Expected: 0\n```\n\n### Selective Expert Removal\n\n```bash\nsqlite3 data/experts.db <<SQL\nBEGIN TRANSACTION;\n\n-- Step 1: Remove drift records (no CASCADE)\nDELETE FROM comment_group_drift \nWHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert');\n\n-- Step 2: Remove comments (CASCADE works)\nDELETE FROM comments \nWHERE post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert');\n\n-- Step 3: Remove links (CASCADE works)\nDELETE FROM links \nWHERE source_post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert')\n   OR target_post_id IN (SELECT post_id FROM posts WHERE expert_id='new_expert');\n\n-- Step 4: Remove posts (triggers remaining cascades)\nDELETE FROM posts WHERE expert_id='new_expert';\n\n-- Step 5: Reclaim space\nVACUUM;\n\nCOMMIT;\nSQL\n```\n\n### Code Rollback\n\n```bash\n# Revert helper functions\ngit checkout backend/src/api/models.py\n\n# Revert frontend\ngit checkout frontend/src/App.tsx\n\n# Verify\ngit status\n```\n\n---\n\n## 13. STEP-BY-STEP EXECUTION SEQUENCE\n\nBased on `docs/add-new-expert-playbook.md`, here's the recommended execution order with time estimates:\n\n| Step | Task | Time | Critical Checks |\n|------|------|------|----------------|\n| 0 | Prerequisites | 5 min | Migration 008, load_dotenv(), servers stopped |\n| 1 | Database backup | 2 min | Backup created, size verified |\n| 2 | Import posts from JSON | 5-10 min | Post count, channel_id uniqueness, no duplicates |\n| 3 | Update models.py | 2 min | Helper functions return correct values |\n| 4 | Import comments (optional) | 15-60 min | Comments linked correctly, no cross-expert pollution |\n| 5 | Create drift records | 2 min | Empty records created with correct expert_id |\n| 6 | Run drift analysis (optional) | 20-90 min | Analysis complete, drift_topics populated |\n| 7 | Final data verification | 5 min | All integrity checks pass |\n| 8 | API testing | 5 min | Health check, query test, post retrieval |\n| 9 | Update frontend | 2 min | expandedExperts updated, type-check passes |\n| 10 | E2E testing | 10 min | Full query to all experts, UI renders correctly |\n| 11 | Git commit | 5 min | Changes committed with proper message |\n| 12 | Documentation | 5 min | CLAUDE.md updated |\n\n**Total time:** 1.5-3 hours (depending on drift analysis)\n\n---\n\n## FINAL RECOMMENDATIONS\n\n### Must-Have Before Starting\n1. ✅ Migration 008 applied\n2. ✅ `load_dotenv()` in main.py\n3. ✅ Database backup created\n4. ✅ Understand channel_id vs telegram_message_id distinction\n5. ✅ Have Telegram API credentials ready\n\n### During Execution\n1. **Never skip channel_id filter** in any database query involving posts/comments\n2. **Always verify** channel_id is unique for new expert\n3. **Use temporary script** for comment import (playbook Step 4)\n4. **Test incrementally** - don't wait until the end to verify data\n5. **Check for cross-expert pollution** after every import step\n\n### After Completion\n1. **Run full verification suite** (Section 11)\n2. **Test query with `expert_filter`** to ensure isolation works\n3. **Check frontend** displays new expert correctly\n4. **Document** any deviations from playbook in CLAUDE.md\n5. **Keep backup** for at least 7 days\n\n---\n\nThis context manifest represents the complete understanding needed to add a third expert without issues. The key insight: **expert isolation happens at the database level via expert_id and channel_id filtering** - miss either filter in any query, and you get silent data corruption."
          }
        ]
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "thinking",
        "thinking": "Агент context-gathering провёл отличную работу! Он выявил множество критических деталей, которые не были полностью учтены в playbook. Теперь я должен проанализировать найденные проблемы и обновить playbook с учётом всей этой информации.\n\nКлючевые находки, которые нужно внедрить в playbook:\n\n1. **КРИТИЧНО: channel_id фильтрация** - без неё комментарии попадут к неправильному эксперту из-за пересечения telegram_message_id между каналами\n\n2. **Миграции 005-007** - история ошибок с UNIQUE constraint показывает, что проблема была в том, что telegram_message_id сделали глобально уникальным, хотя он локален для каждого канала\n\n3. **Deletion order** - comment_group_drift не имеет CASCADE, поэтому нужно удалять в правильном порядке\n\n4. **telegram_comments_fetcher.py** - хардкод channel_id на строках 186 и 209 это критическая проблема, временный скрипт - правильное решение\n\n5. **Entity conversion** - автоматическая конвертация в markdown происходит при импорте\n\n6. **Frontend expert_id в getPostsByIds** - нужно передавать expert_id чтобы избежать конфликтов при одинаковых telegram_message_id\n\n7. **Drift agent** - правильно использует автоопределение expert_id из posts таблицы\n\n8. **load_dotenv()** - должен быть ДО других импортов\n\n9. **Orphan detection** - нужны проверки на orphaned записи\n \nНеобходимо тщательно проверить возможность перекрестного загрязнения данных между экспертами. Это критический момент, требующий немедленного внимания при импорте комментариев. Требуется разработать надёжный механизм изоляции данных каждого эксперта, чтобы предотвратить случайное смешивание информации.",
        "signature": "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"
      }
    ]
  }
]