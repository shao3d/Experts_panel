[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_x1ym2pxknlr",
        "type": "tool_result",
        "content": "     1→\"\"\"Comment Group Map service for finding relevant comment groups.\"\"\"\n     2→\n     3→import asyncio\n     4→import json\n     5→from typing import List, Dict, Any, Optional, Callable\n     6→from datetime import datetime\n     7→import re\n     8→import logging\n     9→from pathlib import Path\n    10→from string import Template\n    11→\n    12→from sqlalchemy.orm import Session\n    13→from openai import AsyncOpenAI\n    14→from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n    15→import httpx\n    16→\n    17→from ..models.post import Post\n    18→from ..models.comment import Comment\n    19→from ..models.database import comment_group_drift\n    20→from .openrouter_adapter import create_openrouter_client, convert_model_name\n    21→from ..api.models import get_channel_username\n    22→from ..utils.language_utils import prepare_prompt_with_language_instruction\n    23→\n    24→logger = logging.getLogger(__name__)\n    25→\n    26→\n    27→class CommentGroupMapService:\n    28→    \"\"\"Service for finding relevant GROUPS of comments.\n    29→\n    30→    Analyzes groups of Telegram comments to find discussions relevant to the query,\n    31→    even when the anchor post itself is not relevant.\n    32→    \"\"\"\n    33→\n    34→    DEFAULT_CHUNK_SIZE = 20  # Groups per chunk\n    35→    DEFAULT_MODEL = \"gpt-4o-mini\"\n    36→    DEFAULT_MAX_PARALLEL = 5  # Rate limiting for API calls\n    37→\n    38→    def __init__(\n    39→        self,\n    40→        api_key: str,\n    41→        chunk_size: int = DEFAULT_CHUNK_SIZE,\n    42→        model: str = DEFAULT_MODEL,\n    43→        max_parallel: int = DEFAULT_MAX_PARALLEL\n    44→    ):\n    45→        \"\"\"Initialize CommentGroupMapService.\n    46→\n    47→        Args:\n    48→            api_key: OpenAI API key\n    49→            chunk_size: Number of comment groups per chunk (default 20)\n    50→            model: Model to use (default gemini-2.0-flash)\n    51→            max_parallel: Maximum parallel API calls (default 5)\n    52→        \"\"\"\n    53→        self.client = create_openrouter_client(api_key=api_key)\n    54→        self.chunk_size = chunk_size\n    55→        self.model = convert_model_name(model)\n    56→        self.max_parallel = max_parallel\n    57→        self._prompt_template = self._load_prompt_template()\n    58→\n    59→    def _load_prompt_template(self) -> Template:\n    60→        \"\"\"Load the comment group drift prompt template.\"\"\"\n    61→        try:\n    62→            prompt_dir = Path(__file__).parent.parent.parent / \"prompts\"\n    63→            prompt_path = prompt_dir / \"comment_group_drift_prompt.txt\"\n    64→\n    65→            # SECURITY: Prevent path traversal attacks\n    66→            if not prompt_path.resolve().is_relative_to(prompt_dir.resolve()):\n    67→                raise ValueError(f\"Invalid prompt path: {prompt_path}\")\n    68→\n    69→            # SECURITY: Check file permissions (not world-writable)\n    70→            if prompt_path.stat().st_mode & 0o002:\n    71→                logger.error(f\"Prompt file is world-writable: {prompt_path}\")\n    72→                raise PermissionError(\"Unsafe prompt file permissions\")\n    73→\n    74→            with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n    75→                content = f.read()\n    76→\n    77→            # Validate required placeholders\n    78→            if \"$query\" not in content or \"$groups\" not in content:\n    79→                raise ValueError(\"Prompt template missing required placeholders\")\n    80→\n    81→            return Template(content)\n    82→        except FileNotFoundError:\n    83→            logger.error(f\"Comment group map prompt template not found at {prompt_path}\")\n    84→            raise\n    85→\n    86→    def _load_drift_groups(\n    87→        self,\n    88→        db: Session,\n    89→        expert_id: str,\n    90→        exclude_post_ids: Optional[List[int]] = None\n    91→    ) -> List[Dict[str, Any]]:\n    92→        \"\"\"Load comment groups with drift from database.\n    93→\n    94→        Args:\n    95→            db: Database session\n    96→            expert_id: Expert identifier to filter posts\n    97→            exclude_post_ids: List of telegram_message_ids to exclude\n    98→\n    99→        Returns:\n   100→            List of drift groups with anchor post info and drift_topics\n   101→        \"\"\"\n   102→        # Query drift groups with anchor posts\n   103→        query = db.query(\n   104→            comment_group_drift.c.post_id,\n   105→            comment_group_drift.c.drift_topics,\n   106→            Post.telegram_message_id,\n   107→            Post.message_text,\n   108→            Post.created_at,\n   109→            Post.author_name\n   110→        ).join(\n   111→            Post, comment_group_drift.c.post_id == Post.post_id\n   112→        ).filter(\n   113→            comment_group_drift.c.has_drift == True,\n   114→            comment_group_drift.c.expert_id == expert_id  # Direct filter, no need for Post.expert_id\n   115→        )\n   116→\n   117→        # Exclude posts from Pipeline A (relevant_post_ids)\n   118→        # SECURITY: Validate exclude_post_ids to prevent SQL injection\n   119→        if exclude_post_ids:\n   120→            validated_ids = []\n   121→            for post_id in exclude_post_ids:\n   122→                if not isinstance(post_id, int) or post_id <= 0:\n   123→                    logger.warning(f\"Invalid post_id in exclude_post_ids: {post_id}\")\n   124→                    continue\n   125→                validated_ids.append(post_id)\n   126→\n   127→            if validated_ids:\n   128→                query = query.filter(\n   129→                    Post.telegram_message_id.notin_(validated_ids)\n   130→                )\n   131→\n   132→        results = query.all()\n   133→\n   134→        # Format groups with drift_topics and load comments\n   135→        groups = []\n   136→        for post_id, drift_topics_json, telegram_msg_id, message_text, created_at, author_name in results:\n   137→            # Sanitize JSON from database before parsing\n   138→            if drift_topics_json:\n   139→                try:\n   140→                    # Convert bytes to string if needed\n   141→                    if isinstance(drift_topics_json, bytes):\n   142→                        drift_topics_json = drift_topics_json.decode(\"utf-8\")                    # Remove invalid escape sequences\n   143→                    sanitized = re.sub(r'\\\\(?![ntr\"\\\\/])', '', drift_topics_json)\n   144→                    parsed_drift = json.loads(sanitized)\n   145→\n   146→                    # Extract only the drift_topics array, not the wrapper\n   147→                    if isinstance(parsed_drift, dict) and 'drift_topics' in parsed_drift:\n   148→                        drift_topics = parsed_drift['drift_topics']\n   149→                    elif isinstance(parsed_drift, list):\n   150→                        # Handle old format (direct array)\n   151→                        drift_topics = parsed_drift\n   152→                    else:\n   153→                        drift_topics = []\n   154→\n   155→                except json.JSONDecodeError:\n   156→                    # If still fails, use json_repair\n   157→                    try:\n   158→                        from json_repair import repair_json\n   159→                        repaired = json.loads(repair_json(drift_topics_json))\n   160→                        if isinstance(repaired, dict) and 'drift_topics' in repaired:\n   161→                            drift_topics = repaired['drift_topics']\n   162→                        elif isinstance(repaired, list):\n   163→                            drift_topics = repaired\n   164→                        else:\n   165→                            drift_topics = []\n   166→                    except:\n   167→                        drift_topics = []\n   168→            else:\n   169→                drift_topics = []\n   170→\n   171→            # Load actual comments from database\n   172→            from ..models.comment import Comment\n   173→            comments_query = db.query(Comment).filter(Comment.post_id == post_id).all()\n   174→            comments = [\n   175→                {\n   176→                    \"comment_id\": c.comment_id,\n   177→                    \"comment_text\": c.comment_text,\n   178→                    \"author_name\": c.author_name,\n   179→                    \"created_at\": c.created_at.isoformat(),\n   180→                    \"updated_at\": c.updated_at.isoformat()\n   181→                }\n   182→                for c in comments_query\n   183→            ]\n   184→\n   185→            groups.append({\n   186→                \"anchor_post\": {\n   187→                    \"telegram_message_id\": telegram_msg_id,\n   188→                    \"message_text\": message_text or \"[Media only]\",\n   189→                    \"created_at\": created_at.isoformat(),\n   190→                    \"author_name\": author_name or \"Unknown\",\n   191→                    \"channel_username\": get_channel_username(expert_id)  # Dynamic based on expert\n   192→                },\n   193→                \"drift_topics\": drift_topics,\n   194→                \"comments_count\": len(comments),\n   195→                \"comments\": comments\n   196→            })\n   197→\n   198→        return groups\n   199→\n   200→    def _chunk_groups(self, groups: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:\n   201→        \"\"\"Split comment groups into chunks.\n   202→\n   203→        Args:\n   204→            groups: List of comment groups\n   205→\n   206→        Returns:\n   207→            List of group chunks\n   208→        \"\"\"\n   209→        chunks = []\n   210→        for i in range(0, len(groups), self.chunk_size):\n   211→            chunks.append(groups[i:i + self.chunk_size])\n   212→        return chunks\n   213→\n   214→    def _format_groups_for_prompt(self, groups: List[Dict[str, Any]]) -> str:\n   215→        \"\"\"Format drift groups for inclusion in the prompt.\n   216→\n   217→        Args:\n   218→            groups: List of drift groups with drift_topics\n   219→\n   220→        Returns:\n   221→            Formatted JSON string\n   222→        \"\"\"\n   223→        formatted_groups = []\n   224→        for group in groups:\n   225→            formatted_groups.append({\n   226→                \"parent_telegram_message_id\": group[\"anchor_post\"][\"telegram_message_id\"],\n   227→                \"drift_topics\": group[\"drift_topics\"],\n   228→                \"comments_count\": group[\"comments_count\"]\n   229→            })\n   230→\n   231→        return json.dumps(formatted_groups, ensure_ascii=False, indent=2)\n   232→\n   233→    @retry(\n   234→        stop=stop_after_attempt(5),\n   235→        wait=wait_exponential(multiplier=2, min=4, max=60),\n   236→        retry=retry_if_exception_type(httpx.HTTPStatusError),\n   237→        reraise=True\n   238→    )\n   239→    async def _process_chunk(\n   240→        self,\n   241→        chunk: List[Dict[str, Any]],\n   242→        query: str,\n   243→        chunk_index: int,\n   244→        progress_callback: Optional[Callable] = None\n   245→    ) -> Dict[str, Any]:\n   246→        \"\"\"Process a single chunk of comment groups.\n   247→\n   248→        Args:\n   249→            chunk: List of comment groups in this chunk\n   250→            query: User's query\n   251→            chunk_index: Index of this chunk\n   252→            progress_callback: Optional callback for progress updates\n   253→\n   254→        Returns:\n   255→            Dictionary with relevant comment groups\n   256→        \"\"\"\n   257→        try:\n   258→            if progress_callback:\n   259→                await progress_callback({\n   260→                    \"event_type\": \"progress\",\n   261→                    \"phase\": \"comment_groups\",\n   262→                    \"chunk\": chunk_index,\n   263→                    \"status\": \"processing\",\n   264→                    \"message\": f\"Analyzing comment groups chunk {chunk_index + 1}\"\n   265→                })\n   266→\n   267→            # Format the prompt\n   268→            groups_formatted = self._format_groups_for_prompt(chunk)\n   269→\n   270→            # Create base prompt\n   271→            base_prompt = self._prompt_template.substitute(\n   272→                query=query,\n   273→                groups=groups_formatted\n   274→            )\n   275→\n   276→            # Apply language instruction based on query language\n   277→            prompt = prepare_prompt_with_language_instruction(base_prompt, query)\n   278→\n   279→            # Call API\n   280→            response = await self.client.chat.completions.create(\n   281→                model=self.model,\n   282→                messages=[\n   283→                    {\"role\": \"system\", \"content\": \"You are analyzing comment groups from Telegram.\"},\n   284→                    {\"role\": \"user\", \"content\": prompt}\n   285→                ],\n   286→                temperature=0.3,\n   287→                response_format={\"type\": \"json_object\"}\n   288→            )\n   289→\n   290→            # Parse response\n   291→            result = json.loads(response.choices[0].message.content)\n   292→\n   293→            # Add anchor_post and comments back to each group from original chunk data\n   294→            # GPT only returns parent_telegram_message_id, we need full anchor_post + comments\n   295→            chunk_map = {\n   296→                group[\"anchor_post\"][\"telegram_message_id\"]: {\n   297→                    \"anchor_post\": group[\"anchor_post\"],\n   298→                    \"comments\": group[\"comments\"],\n   299→                    \"comments_count\": group[\"comments_count\"]\n   300→                }\n   301→                for group in chunk\n   302→            }\n   303→\n   304→            for group in result.get(\"relevant_groups\", []):\n   305→                telegram_msg_id = group.get(\"parent_telegram_message_id\")\n   306→                if telegram_msg_id and telegram_msg_id in chunk_map:\n   307→                    group[\"anchor_post\"] = chunk_map[telegram_msg_id][\"anchor_post\"]\n   308→                    group[\"comments\"] = chunk_map[telegram_msg_id][\"comments\"]\n   309→                    group[\"comments_count\"] = chunk_map[telegram_msg_id][\"comments_count\"]\n   310→\n   311→            # Add chunk metadata\n   312→            result[\"chunk_index\"] = chunk_index\n   313→            result[\"groups_processed\"] = len(chunk)\n   314→\n   315→            if progress_callback:\n   316→                await progress_callback({\n   317→                    \"event_type\": \"progress\",\n   318→                    \"phase\": \"comment_groups\",\n   319→                    \"chunk\": chunk_index,\n   320→                    \"status\": \"completed\",\n   321→                    \"message\": f\"Comment groups chunk {chunk_index + 1} completed\",\n   322→                    \"relevant_found\": len(result.get(\"relevant_groups\", []))\n   323→                })\n   324→\n   325→            return result\n   326→\n   327→        except Exception as e:\n   328→            logger.error(f\"Error processing comment groups chunk {chunk_index}: {str(e)}\")\n   329→            if progress_callback:\n   330→                await progress_callback({\n   331→                    \"event_type\": \"error\",\n   332→                    \"phase\": \"comment_groups\",\n   333→                    \"chunk\": chunk_index,\n   334→                    \"status\": \"error\",\n   335→                    \"message\": f\"Error in chunk {chunk_index + 1}: {str(e)}\"\n   336→                })\n   337→            raise\n   338→\n   339→    async def process(\n   340→        self,\n   341→        query: str,\n   342→        db: Session,  # NOTE: Caller MUST manage session lifecycle\n   343→        expert_id: str,\n   344→        exclude_post_ids: Optional[List[int]] = None,\n   345→        progress_callback: Optional[Callable] = None\n   346→    ) -> List[Dict[str, Any]]:\n   347→        \"\"\"Process drift groups to find relevant ones.\n   348→\n   349→        IMPORTANT: This method does NOT close the db session. The caller\n   350→        is responsible for session lifecycle management.\n   351→\n   352→        Args:\n   353→            query: User's query\n   354→            db: Database session (managed by caller)\n   355→            expert_id: Expert identifier to filter posts\n   356→            exclude_post_ids: List of telegram_message_ids to exclude (from Pipeline A)\n   357→            progress_callback: Optional callback for progress updates\n   358→\n   359→        Returns:\n   360→            List of relevant comment groups\n   361→        \"\"\"\n   362→        import time\n   363→        phase_start_time = time.time()\n   364→\n   365→        # Load drift groups from database\n   366→        all_groups = self._load_drift_groups(db, expert_id, exclude_post_ids)\n   367→\n   368→        if not all_groups:\n   369→            if progress_callback:\n   370→                await progress_callback({\n   371→                    \"event_type\": \"phase_complete\",\n   372→                    \"phase\": \"comment_groups\",\n   373→                    \"status\": \"completed\",\n   374→                    \"message\": \"No comment groups to analyze\"\n   375→                })\n   376→            return []\n   377→\n   378→        # Split into chunks\n   379→        chunks = self._chunk_groups(all_groups)\n   380→        total_chunks = len(chunks)\n   381→\n   382→        logger.info(f\"[{expert_id}] Comment Groups Phase START: Processing {len(all_groups)} comment groups in {total_chunks} chunks\")\n   383→\n   384→        if progress_callback:\n   385→            await progress_callback({\n   386→                \"event_type\": \"phase_start\",\n   387→                \"phase\": \"comment_groups\",\n   388→                \"status\": \"starting\",\n   389→                \"message\": f\"Starting comment groups analysis: {len(all_groups)} groups in {total_chunks} chunks\",\n   390→                \"total_chunks\": total_chunks\n   391→            })\n   392→\n   393→        # Process chunks in parallel\n   394→        parallel_limit = self.max_parallel if self.max_parallel is not None else total_chunks\n   395→        semaphore = asyncio.Semaphore(parallel_limit)\n   396→\n   397→        async def process_with_semaphore(chunk, index):\n   398→            async with semaphore:\n   399→                return await self._process_chunk(chunk, query, index, progress_callback)\n   400→\n   401→        tasks = [\n   402→            process_with_semaphore(chunk, i)\n   403→            for i, chunk in enumerate(chunks)\n   404→        ]\n   405→\n   406→        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)\n   407→\n   408→        # Aggregate results\n   409→        all_relevant_groups = []\n   410→        chunks_with_errors = []\n   411→\n   412→        for i, result in enumerate(chunk_results):\n   413→            if isinstance(result, Exception):\n   414→                logger.error(f\"Comment groups chunk {i} failed: {result}\")\n   415→                chunks_with_errors.append(i)\n   416→            else:\n   417→                # Validate each group has anchor_post before adding\n   418→                groups = result.get(\"relevant_groups\", [])\n   419→                for group in groups:\n   420→                    if not group.get(\"anchor_post\"):\n   421→                        logger.error(f\"Group missing anchor_post in chunk {i}: {group}\")\n   422→                        continue\n   423→                    all_relevant_groups.append(group)\n   424→\n   425→        # Filter HIGH relevance only\n   426→        relevant_groups = [\n   427→            g for g in all_relevant_groups\n   428→            if g.get(\"relevance\") == \"HIGH\"\n   429→        ]\n   430→\n   431→        # Sort by relevance\n   432→        relevance_order = {\"HIGH\": 0, \"MEDIUM\": 1}\n   433→        relevant_groups.sort(\n   434→            key=lambda x: (\n   435→                relevance_order.get(x.get(\"relevance\", \"LOW\"), 2),\n   436→                -x.get(\"comments_count\", 0)\n   437→            )\n   438→        )\n   439→\n   440→        # Log phase completion with timing\n   441→        duration_ms = int((time.time() - phase_start_time) * 1000)\n   442→        logger.info(\n   443→            f\"[{expert_id}] Comment Groups Phase END: {duration_ms}ms, \"\n   444→            f\"found {len(relevant_groups)} HIGH relevance groups from {len(all_groups)} total\"\n   445→        )\n   446→\n   447→        if progress_callback:\n   448→            await progress_callback({\n   449→                \"event_type\": \"phase_complete\",\n   450→                \"phase\": \"comment_groups\",\n   451→                \"status\": \"completed\",\n   452→                \"message\": f\"Found {len(relevant_groups)} relevant comment groups\"\n   453→            })\n   454→\n   455→        return relevant_groups\n   456→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Let me continue reading more files to get a complete picture:\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_y59zjwhsv7i",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/services/comment_synthesis_service.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_wmvrg9n99f",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/services/fact_validator.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_6fiibx9ocjg",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/services/log_service.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_j1izwo2t0l",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/models/base.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "call_hhtbsaa9rfh",
        "name": "Read",
        "input": {
          "file_path": "/Users/andreysazonov/Documents/Projects/Experts_panel/backend/src/models/database.py"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_y59zjwhsv7i",
        "type": "tool_result",
        "content": "     1→\"\"\"Comment synthesis service for extracting insights from comment discussions.\"\"\"\n     2→\n     3→import json\n     4→import logging\n     5→from typing import List, Dict, Any, Optional, Callable\n     6→from pathlib import Path\n     7→from string import Template\n     8→\n     9→from openai import AsyncOpenAI\n    10→from tenacity import retry, stop_after_attempt, wait_exponential\n    11→\n    12→from .openrouter_adapter import create_openrouter_client, convert_model_name\n    13→from ..utils.language_utils import prepare_prompt_with_language_instruction, prepare_system_message_with_language, get_response_language_instruction\n    14→\n    15→logger = logging.getLogger(__name__)\n    16→\n    17→\n    18→class CommentSynthesisService:\n    19→    \"\"\"Service for synthesizing insights from comment group discussions.\n    20→\n    21→    Takes relevant comment groups and extracts additional insights that\n    22→    complement the main answer from the Reduce phase.\n    23→    \"\"\"\n    24→\n    25→    DEFAULT_MODEL = \"gemini-2.0-flash\"  # Changed to Gemini for better context synthesis\n    26→\n    27→    def __init__(\n    28→        self,\n    29→        api_key: str,\n    30→        model: str = DEFAULT_MODEL\n    31→    ):\n    32→        \"\"\"Initialize CommentSynthesisService.\n    33→\n    34→        Args:\n    35→            api_key: OpenAI API key\n    36→            model: OpenAI model to use (default gpt-4o-mini)\n    37→        \"\"\"\n    38→        self.client = create_openrouter_client(api_key=api_key)\n    39→        self.model = convert_model_name(model)\n    40→        self._prompt_template = self._load_prompt_template()\n    41→\n    42→    def _load_prompt_template(self) -> Template:\n    43→        \"\"\"Load the comment synthesis prompt template.\"\"\"\n    44→        try:\n    45→            prompt_dir = Path(__file__).parent.parent.parent / \"prompts\"\n    46→            prompt_path = prompt_dir / \"comment_synthesis_prompt.txt\"\n    47→\n    48→            with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n    49→                return Template(f.read())\n    50→        except FileNotFoundError:\n    51→            logger.error(f\"Comment synthesis prompt template not found at {prompt_path}\")\n    52→            raise\n    53→\n    54→    def _format_comment_groups(\n    55→        self,\n    56→        comment_groups: List[Dict[str, Any]]\n    57→    ) -> str:\n    58→        \"\"\"Format comment groups for the synthesis prompt.\n    59→\n    60→        Args:\n    61→            comment_groups: List of comment group data\n    62→\n    63→        Returns:\n    64→            Formatted JSON string of comment groups\n    65→        \"\"\"\n    66→        formatted_groups = []\n    67→\n    68→        for group in comment_groups:\n    69→            # Extract only comments text (no anchor post)\n    70→            comments_text = []\n    71→            for comment in group.get(\"comments\", []):\n    72→                comment_text = comment.get(\"text\", \"\").strip()\n    73→                if comment_text:\n    74→                    comments_text.append(comment_text)\n    75→\n    76→            if comments_text:\n    77→                formatted_group = {\n    78→                    \"anchor_post_id\": group.get(\"anchor_post\", {}).get(\"id\"),\n    79→                    \"relevance\": group.get(\"relevance\", \"MEDIUM\"),\n    80→                    \"reason\": group.get(\"reason\", \"\"),\n    81→                    \"comments\": comments_text,\n    82→                    \"comment_count\": len(comments_text)\n    83→                }\n    84→                formatted_groups.append(formatted_group)\n    85→\n    86→        return json.dumps(formatted_groups, ensure_ascii=False, indent=2)\n    87→\n    88→    @retry(\n    89→        stop=stop_after_attempt(3),\n    90→        wait=wait_exponential(multiplier=1, min=4, max=10),\n    91→        reraise=True\n    92→    )\n    93→    async def _synthesize_insights(\n    94→        self,\n    95→        query: str,\n    96→        main_answer: str,\n    97→        comment_groups_json: str,\n    98→        progress_callback: Optional[Callable] = None\n    99→    ) -> Dict[str, Any]:\n   100→        \"\"\"Call GPT-4o-mini to synthesize insights from comments.\n   101→\n   102→        Args:\n   103→            query: User's original query\n   104→            main_answer: Main answer from Reduce phase\n   105→            comment_groups_json: JSON string of formatted comment groups\n   106→            progress_callback: Optional callback for progress updates\n   107→\n   108→        Returns:\n   109→            Synthesized insights response\n   110→        \"\"\"\n   111→        if progress_callback:\n   112→            await progress_callback({\n   113→                \"phase\": \"comment_synthesis\",\n   114→                \"status\": \"synthesizing\",\n   115→                \"message\": \"Synthesizing insights from comment discussions\"\n   116→            })\n   117→\n   118→        # Create base prompt\n   119→        base_prompt = self._prompt_template.substitute(\n   120→            query=query,\n   121→            main_answer=main_answer,\n   122→            comment_groups=comment_groups_json\n   123→        )\n   124→\n   125→        # Apply language instruction based on query language\n   126→        prompt = prepare_prompt_with_language_instruction(base_prompt, query)\n   127→\n   128→        # Prepare enhanced system message with language instruction\n   129→        enhanced_system = prepare_system_message_with_language(\n   130→            \"You are a helpful assistant that extracts insights from comment discussions.\",\n   131→            query\n   132→        )\n   133→\n   134→        # Call OpenAI API\n   135→        response = await self.client.chat.completions.create(\n   136→            model=self.model,\n   137→            messages=[\n   138→                {\"role\": \"system\", \"content\": enhanced_system},\n   139→                {\"role\": \"user\", \"content\": prompt}\n   140→            ],\n   141→            temperature=0.4,\n   142→            response_format={\"type\": \"json_object\"}\n   143→        )\n   144→\n   145→        # Parse response\n   146→        raw_content = response.choices[0].message.content\n   147→        try:\n   148→            result = json.loads(raw_content)\n   149→        except json.JSONDecodeError as e:\n   150→            logger.error(f\"JSON parsing failed: {e}\")\n   151→            logger.debug(f\"Raw content (first 500 chars): {raw_content[:500]}\")\n   152→            raise ValueError(f\"Failed to parse GPT response as JSON: {e}\") from e\n   153→\n   154→        # Handle both string and list formats from GPT\n   155→        if \"synthesis\" in result:\n   156→            synthesis = result[\"synthesis\"]\n   157→\n   158→            # If GPT returned a list of bullet points, join them into a string\n   159→            if isinstance(synthesis, list):\n   160→                synthesis = \"\\n\".join(synthesis)\n   161→                logger.info(f\"Converted synthesis from list ({len(result['synthesis'])} items) to string\")\n   162→            elif isinstance(synthesis, str):\n   163→                logger.info(\"Synthesis already in string format\")\n   164→            else:\n   165→                logger.warning(f\"Unexpected synthesis type: {type(synthesis)}\")\n   166→                synthesis = str(synthesis)\n   167→\n   168→            # Sanitize synthesis string (same as in ReduceService)\n   169→            import re\n   170→            # Remove invalid escape sequences\n   171→            synthesis = re.sub(r'\\\\(?![ntr\"\\\\/])', '', synthesis)\n   172→            result[\"synthesis\"] = synthesis\n   173→            logger.info(\"Sanitized synthesis string for safe JSON transmission\")\n   174→\n   175→        # Track token usage\n   176→        if response.usage:\n   177→            result[\"token_usage\"] = {\n   178→                \"prompt_tokens\": response.usage.prompt_tokens,\n   179→                \"completion_tokens\": response.usage.completion_tokens,\n   180→                \"total_tokens\": response.usage.total_tokens\n   181→            }\n   182→\n   183→        if progress_callback:\n   184→            await progress_callback({\n   185→                \"phase\": \"comment_synthesis\",\n   186→                \"status\": \"completed\",\n   187→                \"message\": \"Comment synthesis completed\"\n   188→            })\n   189→\n   190→        return result\n   191→\n   192→    async def process(\n   193→        self,\n   194→        query: str,\n   195→        main_answer: str,\n   196→        comment_groups: List[Dict[str, Any]],\n   197→        expert_id: str = \"unknown\",\n   198→        progress_callback: Optional[Callable] = None\n   199→    ) -> Optional[str]:\n   200→        \"\"\"Process and synthesize insights from comment groups.\n   201→\n   202→        Args:\n   203→            query: User's original query\n   204→            main_answer: Main answer from Reduce phase\n   205→            comment_groups: List of relevant comment groups (HIGH/MEDIUM only)\n   206→            expert_id: Expert identifier for logging\n   207→            progress_callback: Optional callback for progress updates\n   208→\n   209→        Returns:\n   210→            Synthesized insights string or None if no relevant groups\n   211→        \"\"\"\n   212→        import time\n   213→        phase_start_time = time.time()\n   214→\n   215→        # Filter to only HIGH/MEDIUM relevance\n   216→        relevant_groups = [\n   217→            g for g in comment_groups\n   218→            if g.get(\"relevance\") in [\"HIGH\", \"MEDIUM\"]\n   219→        ]\n   220→\n   221→        if not relevant_groups:\n   222→            logger.info(f\"[{expert_id}] No HIGH/MEDIUM comment groups found, skipping synthesis\")\n   223→            return None\n   224→\n   225→        logger.info(f\"[{expert_id}] Comment Synthesis Phase START: Synthesizing insights from {len(relevant_groups)} comment groups\")\n   226→\n   227→        if progress_callback:\n   228→            await progress_callback({\n   229→                \"phase\": \"comment_synthesis\",\n   230→                \"status\": \"starting\",\n   231→                \"message\": f\"Starting comment synthesis with {len(relevant_groups)} groups\"\n   232→            })\n   233→\n   234→        # Format comment groups\n   235→        comment_groups_json = self._format_comment_groups(relevant_groups)\n   236→\n   237→        # Synthesize insights\n   238→        synthesis = await self._synthesize_insights(\n   239→            query,\n   240→            main_answer,\n   241→            comment_groups_json,\n   242→            progress_callback\n   243→        )\n   244→\n   245→        # Log phase completion with timing\n   246→        duration_ms = int((time.time() - phase_start_time) * 1000)\n   247→        logger.info(\n   248→            f\"[{expert_id}] Comment Synthesis Phase END: {duration_ms}ms, \"\n   249→            f\"processed {len(relevant_groups)} comment groups\"\n   250→        )\n   251→\n   252→        return synthesis.get(\"synthesis\", \"\")\n   253→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_wmvrg9n99f",
        "type": "tool_result",
        "content": "     1→\"\"\"Fact validation service for verifying LLM-generated content against database.\"\"\"\n     2→\n     3→import re\n     4→import logging\n     5→from typing import Dict, List, Optional, Set, Tuple\n     6→from datetime import datetime\n     7→from sqlalchemy.orm import Session\n     8→\n     9→from ..models.post import Post\n    10→from ..models.link import Link\n    11→\n    12→logger = logging.getLogger(__name__)\n    13→\n    14→\n    15→class FactValidator:\n    16→    \"\"\"Validates facts extracted by LLM against the actual database content.\"\"\"\n    17→\n    18→    def __init__(self, db_session: Session):\n    19→        \"\"\"Initialize validator with database session.\n    20→\n    21→        Args:\n    22→            db_session: SQLAlchemy database session\n    23→        \"\"\"\n    24→        self.db = db_session\n    25→\n    26→    def extract_post_references(self, text: str) -> List[int]:\n    27→        \"\"\"Extract post IDs from [post:123] references in text.\n    28→\n    29→        Args:\n    30→            text: Text containing post references\n    31→\n    32→        Returns:\n    33→            List of extracted post IDs\n    34→        \"\"\"\n    35→        pattern = r'\\[post:(\\d+)\\]'\n    36→        matches = re.findall(pattern, text)\n    37→        return [int(match) for match in matches]\n    38→\n    39→    def validate_post_exists(self, post_id: int) -> bool:\n    40→        \"\"\"Check if a post with given ID exists in database.\n    41→\n    42→        Args:\n    43→            post_id: Telegram message ID to check\n    44→\n    45→        Returns:\n    46→            True if post exists, False otherwise\n    47→        \"\"\"\n    48→        post = self.db.query(Post).filter_by(telegram_message_id=post_id).first()\n    49→        return post is not None\n    50→\n    51→    def validate_post_content(self, post_id: int, claimed_content: str) -> Tuple[bool, Optional[str]]:\n    52→        \"\"\"Validate if claimed content matches actual post.\n    53→\n    54→        Args:\n    55→            post_id: Telegram message ID\n    56→            claimed_content: Content claimed to be in the post\n    57→\n    58→        Returns:\n    59→            Tuple of (is_valid, actual_content)\n    60→        \"\"\"\n    61→        post = self.db.query(Post).filter_by(telegram_message_id=post_id).first()\n    62→\n    63→        if not post:\n    64→            return False, None\n    65→\n    66→        actual_content = post.message_text or \"\"\n    67→\n    68→        # Normalize for comparison\n    69→        claimed_normalized = claimed_content.lower().strip()\n    70→        actual_normalized = actual_content.lower().strip()\n    71→\n    72→        # Check if claimed content is a substring or very similar\n    73→        is_valid = claimed_normalized in actual_normalized or \\\n    74→                   self._fuzzy_match(claimed_normalized, actual_normalized)\n    75→\n    76→        return is_valid, actual_content\n    77→\n    78→    def validate_dates(self, post_ids: List[int], claimed_period: Optional[str] = None) -> Dict[str, any]:\n    79→        \"\"\"Validate dates for given posts.\n    80→\n    81→        Args:\n    82→            post_ids: List of telegram message IDs\n    83→            claimed_period: Optional claimed time period (e.g., \"последние 2 месяца\")\n    84→\n    85→        Returns:\n    86→            Dictionary with validation results\n    87→        \"\"\"\n    88→        posts = self.db.query(Post).filter(\n    89→            Post.telegram_message_id.in_(post_ids)\n    90→        ).all()\n    91→\n    92→        if not posts:\n    93→            return {\"valid\": False, \"error\": \"No posts found\"}\n    94→\n    95→        dates = [post.created_at for post in posts]\n    96→        min_date = min(dates)\n    97→        max_date = max(dates)\n    98→\n    99→        result = {\n   100→            \"valid\": True,\n   101→            \"actual_period\": {\n   102→                \"start\": min_date.strftime(\"%Y-%m-%d\"),\n   103→                \"end\": max_date.strftime(\"%Y-%m-%d\"),\n   104→                \"months\": self._get_month_names(dates)\n   105→            },\n   106→            \"post_count\": len(posts)\n   107→        }\n   108→\n   109→        # Validate claimed period if provided\n   110→        if claimed_period:\n   111→            result[\"claimed_period\"] = claimed_period\n   112→            result[\"period_match\"] = self._validate_period_claim(\n   113→                claimed_period, min_date, max_date\n   114→            )\n   115→\n   116→        return result\n   117→\n   118→    def validate_links(self, post_id: int) -> Dict[str, any]:\n   119→        \"\"\"Validate links and references for a post.\n   120→\n   121→        Args:\n   122→            post_id: Telegram message ID\n   123→\n   124→        Returns:\n   125→            Dictionary with link validation results\n   126→        \"\"\"\n   127→        # Get links from this post\n   128→        outgoing = self.db.query(Link).filter_by(from_post_id=post_id).all()\n   129→        incoming = self.db.query(Link).filter_by(to_post_id=post_id).all()\n   130→\n   131→        return {\n   132→            \"post_id\": post_id,\n   133→            \"outgoing_links\": [\n   134→                {\n   135→                    \"to_post\": link.to_post_id,\n   136→                    \"type\": link.link_type.value\n   137→                }\n   138→                for link in outgoing\n   139→            ],\n   140→            \"incoming_links\": [\n   141→                {\n   142→                    \"from_post\": link.from_post_id,\n   143→                    \"type\": link.link_type.value\n   144→                }\n   145→                for link in incoming\n   146→            ],\n   147→            \"total_connections\": len(outgoing) + len(incoming)\n   148→        }\n   149→\n   150→    def generate_validation_report(self, answer: str, post_ids: List[int]) -> Dict[str, any]:\n   151→        \"\"\"Generate comprehensive validation report for an answer.\n   152→\n   153→        Args:\n   154→            answer: The answer text to validate\n   155→            post_ids: List of main source post IDs\n   156→\n   157→        Returns:\n   158→            Validation report dictionary\n   159→        \"\"\"\n   160→        report = {\n   161→            \"timestamp\": datetime.now().isoformat(),\n   162→            \"validations\": []\n   163→        }\n   164→\n   165→        # Extract and validate post references\n   166→        referenced_posts = self.extract_post_references(answer)\n   167→\n   168→        for post_id in referenced_posts:\n   169→            validation = {\n   170→                \"post_id\": post_id,\n   171→                \"exists\": self.validate_post_exists(post_id)\n   172→            }\n   173→\n   174→            if validation[\"exists\"]:\n   175→                # Get actual post content for fact checking\n   176→                post = self.db.query(Post).filter_by(\n   177→                    telegram_message_id=post_id\n   178→                ).first()\n   179→\n   180→                validation[\"actual_date\"] = post.created_at.strftime(\"%Y-%m-%d\")\n   181→                validation[\"actual_month\"] = self._get_month_name_ru(post.created_at)\n   182→\n   183→            report[\"validations\"].append(validation)\n   184→\n   185→        # Validate main sources\n   186→        report[\"main_sources_validation\"] = {\n   187→            \"all_exist\": all(self.validate_post_exists(pid) for pid in post_ids),\n   188→            \"date_range\": self.validate_dates(post_ids) if post_ids else None\n   189→        }\n   190→\n   191→        # Calculate accuracy score\n   192→        total_refs = len(referenced_posts)\n   193→        valid_refs = sum(1 for v in report[\"validations\"] if v[\"exists\"])\n   194→        report[\"accuracy_score\"] = valid_refs / total_refs if total_refs > 0 else 1.0\n   195→\n   196→        return report\n   197→\n   198→    def _fuzzy_match(self, text1: str, text2: str, threshold: float = 0.8) -> bool:\n   199→        \"\"\"Simple fuzzy matching for content validation.\n   200→\n   201→        Args:\n   202→            text1: First text\n   203→            text2: Second text\n   204→            threshold: Similarity threshold (0-1)\n   205→\n   206→        Returns:\n   207→            True if texts are similar enough\n   208→        \"\"\"\n   209→        # Simple implementation - can be enhanced with proper fuzzy matching\n   210→        words1 = set(text1.split())\n   211→        words2 = set(text2.split())\n   212→\n   213→        if not words1 or not words2:\n   214→            return False\n   215→\n   216→        intersection = words1.intersection(words2)\n   217→        union = words1.union(words2)\n   218→\n   219→        jaccard_similarity = len(intersection) / len(union)\n   220→        return jaccard_similarity >= threshold\n   221→\n   222→    def _get_month_names(self, dates: List[datetime]) -> List[str]:\n   223→        \"\"\"Get unique month names from dates in Russian.\n   224→\n   225→        Args:\n   226→            dates: List of datetime objects\n   227→\n   228→        Returns:\n   229→            List of unique month names\n   230→        \"\"\"\n   231→        months = set()\n   232→        for date in dates:\n   233→            months.add(self._get_month_name_ru(date))\n   234→        return sorted(list(months))\n   235→\n   236→    def _get_month_name_ru(self, date: datetime) -> str:\n   237→        \"\"\"Get Russian month name from date.\n   238→\n   239→        Args:\n   240→            date: Datetime object\n   241→\n   242→        Returns:\n   243→            Russian month name\n   244→        \"\"\"\n   245→        month_names = {\n   246→            1: \"январь\", 2: \"февраль\", 3: \"март\",\n   247→            4: \"апрель\", 5: \"май\", 6: \"июнь\",\n   248→            7: \"июль\", 8: \"август\", 9: \"сентябрь\",\n   249→            10: \"октябрь\", 11: \"ноябрь\", 12: \"декабрь\"\n   250→        }\n   251→        return f\"{month_names.get(date.month, 'unknown')} {date.year}\"\n   252→\n   253→    def _validate_period_claim(self, claimed: str, start: datetime, end: datetime) -> bool:\n   254→        \"\"\"Validate if claimed period matches actual dates.\n   255→\n   256→        Args:\n   257→            claimed: Claimed period string\n   258→            start: Actual start date\n   259→            end: Actual end date\n   260→\n   261→        Returns:\n   262→            True if claim is valid\n   263→        \"\"\"\n   264→        # Simple validation - can be enhanced\n   265→        claimed_lower = claimed.lower()\n   266→\n   267→        # Check for specific month mentions\n   268→        if \"сентябр\" in claimed_lower:\n   269→            return any(d.month == 9 for d in [start, end])\n   270→        if \"август\" in claimed_lower:\n   271→            return any(d.month == 8 for d in [start, end])\n   272→        if \"июл\" in claimed_lower:\n   273→            return any(d.month == 7 for d in [start, end])\n   274→\n   275→        # Check for \"последние X месяцев\"\n   276→        if \"последн\" in claimed_lower and \"месяц\" in claimed_lower:\n   277→            # Extract number\n   278→            import re\n   279→            numbers = re.findall(r'\\d+', claimed)\n   280→            if numbers:\n   281→                months_claimed = int(numbers[0])\n   282→                actual_months = (end.year - start.year) * 12 + end.month - start.month + 1\n   283→                return abs(actual_months - months_claimed) <= 1\n   284→\n   285→        return True  # Default to valid if can't parse\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_6fiibx9ocjg",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Simple Logging Service for Map-Resolve-Reduce Pipeline\n     3→\n     4→Provides structured logging with SSE integration for real-time updates.\n     5→\"\"\"\n     6→\n     7→import asyncio\n     8→import json\n     9→import time\n    10→from dataclasses import dataclass, asdict\n    11→from datetime import datetime\n    12→from enum import Enum\n    13→from typing import List, Dict, Any, Optional, Callable\n    14→from collections import deque\n    15→import logging\n    16→\n    17→logger = logging.getLogger(__name__)\n    18→\n    19→\n    20→class LogLevel(Enum):\n    21→    \"\"\"Log severity levels\"\"\"\n    22→    DEBUG = \"debug\"\n    23→    INFO = \"info\"\n    24→    WARNING = \"warning\"\n    25→    ERROR = \"error\"\n    26→    SUCCESS = \"success\"\n    27→\n    28→\n    29→class LogPhase(Enum):\n    30→    \"\"\"Pipeline phases for logging\"\"\"\n    31→    STARTUP = \"startup\"\n    32→    MAP = \"map\"\n    33→    RESOLVE = \"resolve\"\n    34→    REDUCE = \"reduce\"\n    35→    COMPLETE = \"complete\"\n    36→    ERROR = \"error\"\n    37→\n    38→\n    39→@dataclass\n    40→class LogEntry:\n    41→    \"\"\"Structured log entry\"\"\"\n    42→    timestamp: float\n    43→    level: LogLevel\n    44→    phase: LogPhase\n    45→    message: str\n    46→    data: Optional[Dict[str, Any]] = None\n    47→\n    48→    def to_dict(self) -> Dict[str, Any]:\n    49→        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n    50→        return {\n    51→            \"timestamp\": self.timestamp,\n    52→            \"datetime\": datetime.fromtimestamp(self.timestamp).isoformat(),\n    53→            \"level\": self.level.value,\n    54→            \"phase\": self.phase.value,\n    55→            \"message\": self.message,\n    56→            \"data\": self.data\n    57→        }\n    58→\n    59→\n    60→class LogService:\n    61→    \"\"\"\n    62→    Centralized logging service for pipeline execution\n    63→\n    64→    Features:\n    65→    - Structured logging with phases and levels\n    66→    - In-memory log storage for debugging\n    67→    - SSE event generation\n    68→    - Performance metrics tracking\n    69→    - Async-safe operations\n    70→    \"\"\"\n    71→\n    72→    def __init__(self, max_logs: int = 1000):\n    73→        \"\"\"\n    74→        Initialize the log service\n    75→\n    76→        Args:\n    77→            max_logs: Maximum number of logs to keep in memory\n    78→        \"\"\"\n    79→        self._logs: deque = deque(maxlen=max_logs)\n    80→        self._sse_callbacks: List[Callable] = []\n    81→        self._start_times: Dict[str, float] = {}\n    82→        self._metrics: Dict[str, Any] = {\n    83→            \"total_queries\": 0,\n    84→            \"successful_queries\": 0,\n    85→            \"failed_queries\": 0,\n    86→            \"average_processing_time\": 0,\n    87→            \"phase_timings\": {}\n    88→        }\n    89→        self._lock = asyncio.Lock()\n    90→\n    91→    async def log(\n    92→        self,\n    93→        level: LogLevel,\n    94→        phase: LogPhase,\n    95→        message: str,\n    96→        data: Optional[Dict[str, Any]] = None,\n    97→        emit_sse: bool = True\n    98→    ) -> LogEntry:\n    99→        \"\"\"\n   100→        Create and store a log entry\n   101→\n   102→        Args:\n   103→            level: Log severity level\n   104→            phase: Pipeline phase\n   105→            message: Log message\n   106→            data: Additional data to log\n   107→            emit_sse: Whether to emit SSE event\n   108→\n   109→        Returns:\n   110→            The created log entry\n   111→        \"\"\"\n   112→        async with self._lock:\n   113→            entry = LogEntry(\n   114→                timestamp=time.time(),\n   115→                level=level,\n   116→                phase=phase,\n   117→                message=message,\n   118→                data=data\n   119→            )\n   120→\n   121→            self._logs.append(entry)\n   122→\n   123→            # Log to Python logger\n   124→            log_method = getattr(logger, level.value, logger.info)\n   125→            log_method(f\"[{phase.value}] {message}\", extra={\"data\": data})\n   126→\n   127→            # Emit SSE event if requested\n   128→            if emit_sse:\n   129→                await self._emit_sse(entry)\n   130→\n   131→            return entry\n   132→\n   133→    async def log_phase_start(self, phase: LogPhase, message: str = None) -> None:\n   134→        \"\"\"Log the start of a pipeline phase\"\"\"\n   135→        phase_name = phase.value\n   136→        self._start_times[phase_name] = time.time()\n   137→\n   138→        if message is None:\n   139→            message = f\"Starting {phase_name} phase\"\n   140→\n   141→        await self.log(\n   142→            level=LogLevel.INFO,\n   143→            phase=phase,\n   144→            message=message,\n   145→            data={\"event\": \"phase_start\", \"phase\": phase_name}\n   146→        )\n   147→\n   148→    async def log_phase_complete(\n   149→        self,\n   150→        phase: LogPhase,\n   151→        message: str = None,\n   152→        data: Optional[Dict[str, Any]] = None\n   153→    ) -> None:\n   154→        \"\"\"Log the completion of a pipeline phase\"\"\"\n   155→        phase_name = phase.value\n   156→\n   157→        # Calculate duration if start time exists\n   158→        duration = None\n   159→        if phase_name in self._start_times:\n   160→            duration = time.time() - self._start_times[phase_name]\n   161→            self._metrics[\"phase_timings\"][phase_name] = duration\n   162→\n   163→        if message is None:\n   164→            message = f\"Completed {phase_name} phase\"\n   165→            if duration:\n   166→                message += f\" (took {duration:.2f}s)\"\n   167→\n   168→        log_data = {\"event\": \"phase_complete\", \"phase\": phase_name}\n   169→        if duration:\n   170→            log_data[\"duration\"] = duration\n   171→        if data:\n   172→            log_data.update(data)\n   173→\n   174→        await self.log(\n   175→            level=LogLevel.SUCCESS,\n   176→            phase=phase,\n   177→            message=message,\n   178→            data=log_data\n   179→        )\n   180→\n   181→    async def log_map_progress(\n   182→        self,\n   183→        current_chunk: int,\n   184→        total_chunks: int,\n   185→        posts_in_chunk: int\n   186→    ) -> None:\n   187→        \"\"\"Log progress during map phase\"\"\"\n   188→        message = f\"Processing chunk {current_chunk}/{total_chunks} ({posts_in_chunk} posts)\"\n   189→\n   190→        await self.log(\n   191→            level=LogLevel.INFO,\n   192→            phase=LogPhase.MAP,\n   193→            message=message,\n   194→            data={\n   195→                \"event\": \"map_progress\",\n   196→                \"chunk\": current_chunk,\n   197→                \"total\": total_chunks,\n   198→                \"posts_in_chunk\": posts_in_chunk,\n   199→                \"progress_percentage\": (current_chunk / total_chunks) * 100\n   200→            }\n   201→        )\n   202→\n   203→    async def log_resolve_progress(\n   204→        self,\n   205→        links_found: int,\n   206→        links_evaluated: int\n   207→    ) -> None:\n   208→        \"\"\"Log progress during resolve phase\"\"\"\n   209→        message = f\"Evaluating {links_evaluated} of {links_found} links found\"\n   210→\n   211→        await self.log(\n   212→            level=LogLevel.INFO,\n   213→            phase=LogPhase.RESOLVE,\n   214→            message=message,\n   215→            data={\n   216→                \"event\": \"resolve_progress\",\n   217→                \"links_found\": links_found,\n   218→                \"links_evaluated\": links_evaluated\n   219→            }\n   220→        )\n   221→\n   222→    async def log_error(\n   223→        self,\n   224→        phase: LogPhase,\n   225→        error: str,\n   226→        exception: Optional[Exception] = None\n   227→    ) -> None:\n   228→        \"\"\"Log an error\"\"\"\n   229→        data = {\"event\": \"error\", \"error\": error}\n   230→\n   231→        if exception:\n   232→            data[\"exception_type\"] = type(exception).__name__\n   233→            data[\"exception_message\"] = str(exception)\n   234→\n   235→        await self.log(\n   236→            level=LogLevel.ERROR,\n   237→            phase=phase,\n   238→            message=f\"Error in {phase.value}: {error}\",\n   239→            data=data\n   240→        )\n   241→\n   242→        self._metrics[\"failed_queries\"] += 1\n   243→\n   244→    async def log_query_complete(\n   245→        self,\n   246→        query: str,\n   247→        total_posts: int,\n   248→        relevant_posts: int,\n   249→        processing_time: float\n   250→    ) -> None:\n   251→        \"\"\"Log successful query completion\"\"\"\n   252→        self._metrics[\"successful_queries\"] += 1\n   253→        self._metrics[\"total_queries\"] += 1\n   254→\n   255→        # Update average processing time\n   256→        total = self._metrics[\"successful_queries\"]\n   257→        current_avg = self._metrics[\"average_processing_time\"]\n   258→        self._metrics[\"average_processing_time\"] = (\n   259→            (current_avg * (total - 1) + processing_time) / total\n   260→        )\n   261→\n   262→        await self.log(\n   263→            level=LogLevel.SUCCESS,\n   264→            phase=LogPhase.COMPLETE,\n   265→            message=f\"Query completed successfully in {processing_time:.2f}s\",\n   266→            data={\n   267→                \"event\": \"query_complete\",\n   268→                \"query\": query[:100],  # Truncate long queries\n   269→                \"total_posts_analyzed\": total_posts,\n   270→                \"relevant_posts_found\": relevant_posts,\n   271→                \"processing_time\": processing_time\n   272→            }\n   273→        )\n   274→\n   275→    def register_sse_callback(self, callback: Callable) -> None:\n   276→        \"\"\"Register a callback for SSE events\"\"\"\n   277→        self._sse_callbacks.append(callback)\n   278→\n   279→    def unregister_sse_callback(self, callback: Callable) -> None:\n   280→        \"\"\"Unregister an SSE callback\"\"\"\n   281→        if callback in self._sse_callbacks:\n   282→            self._sse_callbacks.remove(callback)\n   283→\n   284→    async def _emit_sse(self, entry: LogEntry) -> None:\n   285→        \"\"\"Emit SSE event for log entry\"\"\"\n   286→        # Map log levels to SSE event types\n   287→        event_type_map = {\n   288→            LogLevel.ERROR: \"error\",\n   289→            LogLevel.SUCCESS: \"phase_complete\",\n   290→            LogLevel.INFO: \"info\",\n   291→            LogLevel.WARNING: \"warning\",\n   292→            LogLevel.DEBUG: \"debug\"\n   293→        }\n   294→\n   295→        # Special handling for specific events\n   296→        if entry.data and \"event\" in entry.data:\n   297→            event_type = entry.data[\"event\"]\n   298→        else:\n   299→            event_type = event_type_map.get(entry.level, \"info\")\n   300→\n   301→        sse_data = {\n   302→            \"type\": event_type,\n   303→            \"phase\": entry.phase.value,\n   304→            \"message\": entry.message,\n   305→            \"timestamp\": entry.timestamp\n   306→        }\n   307→\n   308→        if entry.data:\n   309→            sse_data.update(entry.data)\n   310→\n   311→        # Call registered callbacks\n   312→        for callback in self._sse_callbacks:\n   313→            try:\n   314→                if asyncio.iscoroutinefunction(callback):\n   315→                    await callback(sse_data)\n   316→                else:\n   317→                    callback(sse_data)\n   318→            except Exception as e:\n   319→                logger.error(f\"Error in SSE callback: {e}\")\n   320→\n   321→    def get_logs(\n   322→        self,\n   323→        phase: Optional[LogPhase] = None,\n   324→        level: Optional[LogLevel] = None,\n   325→        limit: int = 100\n   326→    ) -> List[Dict[str, Any]]:\n   327→        \"\"\"\n   328→        Retrieve logs from memory\n   329→\n   330→        Args:\n   331→            phase: Filter by phase\n   332→            level: Filter by level\n   333→            limit: Maximum number of logs to return\n   334→\n   335→        Returns:\n   336→            List of log entries as dictionaries\n   337→        \"\"\"\n   338→        logs = list(self._logs)\n   339→\n   340→        if phase:\n   341→            logs = [l for l in logs if l.phase == phase]\n   342→\n   343→        if level:\n   344→            logs = [l for l in logs if l.level == level]\n   345→\n   346→        # Return most recent logs first\n   347→        return [l.to_dict() for l in reversed(logs[-limit:])]\n   348→\n   349→    def get_metrics(self) -> Dict[str, Any]:\n   350→        \"\"\"Get performance metrics\"\"\"\n   351→        return self._metrics.copy()\n   352→\n   353→    def clear_logs(self) -> None:\n   354→        \"\"\"Clear all logs from memory\"\"\"\n   355→        self._logs.clear()\n   356→        self._start_times.clear()\n   357→\n   358→    def get_current_phase_duration(self, phase: LogPhase) -> Optional[float]:\n   359→        \"\"\"Get duration of current or completed phase\"\"\"\n   360→        phase_name = phase.value\n   361→        if phase_name in self._start_times:\n   362→            if phase_name in self._metrics[\"phase_timings\"]:\n   363→                return self._metrics[\"phase_timings\"][phase_name]\n   364→            else:\n   365→                return time.time() - self._start_times[phase_name]\n   366→        return None\n   367→\n   368→\n   369→# Global instance for easy access\n   370→_log_service: Optional[LogService] = None\n   371→\n   372→\n   373→def get_log_service() -> LogService:\n   374→    \"\"\"Get or create the global log service instance\"\"\"\n   375→    global _log_service\n   376→    if _log_service is None:\n   377→        _log_service = LogService()\n   378→    return _log_service\n   379→\n   380→\n   381→def reset_log_service() -> None:\n   382→    \"\"\"Reset the global log service\"\"\"\n   383→    global _log_service\n   384→    _log_service = None\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "call_j1izwo2t0l",
        "type": "tool_result",
        "content": "     1→\"\"\"Database base configuration for SQLAlchemy models.\"\"\"\n     2→\n     3→from sqlalchemy import create_engine\n     4→from sqlalchemy.ext.declarative import declarative_base\n     5→from sqlalchemy.orm import sessionmaker\n     6→\n     7→# Database URL - using SQLite for this project\n     8→# Path is relative to backend directory when running\n     9→DATABASE_URL = \"sqlite:///../data/experts.db\"\n    10→\n    11→# Create engine\n    12→engine = create_engine(\n    13→    DATABASE_URL,\n    14→    connect_args={\"check_same_thread\": False}  # Needed for SQLite\n    15→)\n    16→\n    17→# Create session factory\n    18→SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n    19→\n    20→# Create declarative base\n    21→Base = declarative_base()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  }
]