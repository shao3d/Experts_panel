{"contexts":{"ExpertsPanel":{"c4_level":"Context","description":"An intelligent system for analyzing content from expert-led Telegram channels. It allows users to ask natural language questions and receive synthesized, context-aware answers. The core of the system is a custom 7-phase 'Map-Resolve-Reduce' pipeline that uses a hybrid multi-model AI strategy. It supports querying multiple experts (channels) in parallel with full data isolation.","actors":[{"name":"User","description":"An individual interested in understanding an expert's thoughts by asking natural language questions about their Telegram channel content."},{"name":"Administrator","description":"Manages the system, including importing and synchronizing data from Telegram channels."}]}},"containers":{"BackendAPI":{"name":"Backend API","description":"A Python-based container running a FastAPI/Uvicorn application. It serves the core business logic, handles the 'Map-Resolve-Reduce' pipeline, interacts with the database, and exposes a REST API for the frontend. It can be deployed as a standalone service.","technology":"Python 3.11, FastAPI, Uvicorn, SQLAlchemy","c4_level":"Container"},"FrontendWebUI":{"name":"Frontend Web UI","description":"A container that serves the user interface. The UI is built using Node.js, Vite, and React/TypeScript. It supports real-time, multi-expert query progress tracking and comprehensive error handling. In production deployments, the static assets are served by an Nginx web server. It communicates with the Backend API to fetch data and trigger analysis pipelines. It also includes an advanced debug logging system that batches and sends client-side events to the backend.","technology":"Node.js (build), React, TypeScript, Vite, Nginx (serve)","c4_level":"Container"}},"domain_glossary":{"Expert":"A single Telegram channel treated as a distinct, isolated data source, identified by an `expert_id`. The system can process multiple experts in parallel.","Map-Resolve-Reduce Pipeline":{"c4_level":"Context","description":"The core 7-phase process for answering user queries. It includes phases for: 1) Mapping relevant posts using a hybrid LLM strategy, 2) Scoring 'Medium' relevance posts, 3) Expanding context for 'High' relevance posts via database link traversal, 4) Reducing information into a synthesized answer using a hybrid LLM strategy, 5) Validating language consistency, 6) Analyzing comment groups for topic drift, and 7) Synthesizing additional insights from comments."},"Map Phase":"The first and most critical phase of the pipeline. It functions as a 'Chunked Listwise LLM Reranker' that evaluates chunks of posts against the user query to determine relevance (HIGH, MEDIUM, LOW), avoiding traditional vector search.","Comment Drift":"A phenomenon where comment discussions under a Telegram post deviate to a different topic ('off-topic'). The system has a dedicated parallel pipeline to find valuable insights from these discussions, even if the parent post is irrelevant to the query.","RAG (Retrieval-Augmented Generation)":"A common AI architecture pattern for question-answering. This project implements a custom, embedding-free version of RAG, arguing it provides higher precision for its specific use case (small, dense datasets).","Listwise LLM Reranker":"An AI technique where a Large Language Model is given a list of documents and a query, and is tasked with ranking all documents in the list for relevance. This is the core mechanism of the 'Map Phase'.","Anchor Post":"A potentially irrelevant post that serves as an 'anchor' for a valuable, on-topic comment discussion. This concept is central to the Comment Drift analysis."},"deployment_topology":{"production":{"name":"Production Deployment","description":"The production environment is deployed on Fly.io as a single application container running the FastAPI backend. This container also serves the static frontend assets. The deployment features auto-scaling (scaling to zero when idle), SSL termination, and data persistence via a mounted volume for the SQLite database.","c4_level":"Container"}},"data_schema":{"posts":{"name":"posts","description":"Stores individual posts from Telegram channels. This is the central table for content.","columns":[{"name":"post_id","type":"INTEGER","constraints":"PRIMARY KEY AUTOINCREMENT"},{"name":"channel_id","type":"VARCHAR(100)","constraints":"NOT NULL"},{"name":"channel_name","type":"VARCHAR(255)","constraints":null},{"name":"expert_id","type":"VARCHAR(50)","constraints":"Indexed. Added in migration 003."},{"name":"message_text","type":"TEXT","constraints":null},{"name":"author_name","type":"VARCHAR(255)","constraints":null},{"name":"author_id","type":"VARCHAR(100)","constraints":null},{"name":"created_at","type":"DATETIME","constraints":"NOT NULL, DEFAULT CURRENT_TIMESTAMP"},{"name":"edited_at","type":"DATETIME","constraints":null},{"name":"view_count","type":"INTEGER","constraints":"DEFAULT 0"},{"name":"forward_count","type":"INTEGER","constraints":"DEFAULT 0"},{"name":"reply_count","type":"INTEGER","constraints":"DEFAULT 0"},{"name":"media_metadata","type":"JSON","constraints":null},{"name":"telegram_message_id","type":"INTEGER","constraints":"UNIQUE per channel_id (composite key). Added in migration 007."},{"name":"is_forwarded","type":"INTEGER","constraints":"DEFAULT 0"},{"name":"forward_from_channel","type":"VARCHAR(255)","constraints":null},{"name":"channel_username","type":"VARCHAR(255)","constraints":"Added in migration 007."}],"relationships":[{"target_table":"comments","type":"One-to-Many","description":"A post can have many comments."},{"target_table":"links","type":"One-to-Many","description":"A post can be the source or target of many links (replies, forwards)."},{"target_table":"comment_group_drift","type":"One-to-One","description":"A post can have one associated drift analysis record."}],"c4_level":"Code"},"links":{"name":"links","description":"Stores relationships between posts, such as replies or forwards.","columns":[{"name":"link_id","type":"INTEGER","constraints":"PRIMARY KEY AUTOINCREMENT"},{"name":"source_post_id","type":"INTEGER","constraints":"NOT NULL, FOREIGN KEY to posts.post_id"},{"name":"target_post_id","type":"INTEGER","constraints":"NOT NULL, FOREIGN KEY to posts.post_id"},{"name":"link_type","type":"VARCHAR(20)","constraints":"NOT NULL (e.g., 'reply', 'forward')"},{"name":"created_at","type":"DATETIME","constraints":"NOT NULL, DEFAULT CURRENT_TIMESTAMP"}],"relationships":[{"target_table":"posts","type":"Many-to-One","description":"A link connects a source post."},{"target_table":"posts","type":"Many-to-One","description":"A link connects to a target post."}],"c4_level":"Code"},"comments":{"name":"comments","description":"Stores expert annotations or Telegram comments related to a specific post.","columns":[{"name":"comment_id","type":"INTEGER","constraints":"PRIMARY KEY AUTOINCREMENT"},{"name":"post_id","type":"INTEGER","constraints":"NOT NULL, FOREIGN KEY to posts.post_id"},{"name":"comment_text","type":"TEXT","constraints":"NOT NULL"},{"name":"author_name","type":"VARCHAR(255)","constraints":"NOT NULL"},{"name":"author_id","type":"VARCHAR(255)","constraints":"Nullable"},{"name":"created_at","type":"DATETIME","constraints":"NOT NULL, DEFAULT CURRENT_TIMESTAMP"},{"name":"updated_at","type":"DATETIME","constraints":"NOT NULL, DEFAULT CURRENT_TIMESTAMP"},{"name":"telegram_comment_id","type":"INTEGER","constraints":"Nullable. Unique per post_id (migration 008)."},{"name":"parent_telegram_message_id","type":"INTEGER","constraints":"Nullable"}],"relationships":[{"target_table":"posts","type":"Many-to-One","description":"A comment belongs to a single post."}],"c4_level":"Code"},"comment_group_drift":{"name":"comment_group_drift","description":"Stores pre-analyzed drift topics for comment groups to optimize query performance.","columns":[{"name":"id","type":"INTEGER","constraints":"PRIMARY KEY AUTOINCREMENT"},{"name":"post_id","type":"INTEGER","constraints":"NOT NULL, FOREIGN KEY to posts.post_id, UNIQUE"},{"name":"has_drift","type":"BOOLEAN","constraints":"NOT NULL, DEFAULT FALSE"},{"name":"drift_topics","type":"TEXT","constraints":"JSON array of topic objects"},{"name":"analyzed_at","type":"TIMESTAMP","constraints":"NOT NULL, DEFAULT CURRENT_TIMESTAMP"},{"name":"analyzed_by","type":"TEXT","constraints":"NOT NULL"},{"name":"expert_id","type":"VARCHAR(50)","constraints":"Added in migration 004."}],"relationships":[{"target_table":"posts","type":"One-to-One","description":"Each drift analysis corresponds to one anchor post."}],"c4_level":"Code"},"sync_state":{"name":"sync_state","description":"Tracks the last synced message ID for each channel to enable incremental data synchronization from Telegram.","columns":[{"name":"id","type":"INTEGER","constraints":"PRIMARY KEY AUTOINCREMENT"},{"name":"channel_username","type":"TEXT","constraints":"UNIQUE NOT NULL"},{"name":"last_synced_message_id","type":"INTEGER","constraints":null},{"name":"last_synced_at","type":"TIMESTAMP","constraints":null},{"name":"total_posts_synced","type":"INTEGER","constraints":"DEFAULT 0"},{"name":"total_comments_synced","type":"INTEGER","constraints":"DEFAULT 0"}],"relationships":[],"c4_level":"Code"}},"components":{"AppOrchestrator":{"name":"App Orchestrator","description":"The main React component (`App.tsx`) that manages the overall application state, including processing status, progress events, expert responses, and errors. It orchestrates the rendering of all other major UI components like the QueryForm, ProgressSection, ExpertSelectionBar, and ExpertAccordion. It now also manages the state of selected experts to be queried.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/App.tsx"]},"QueryForm":{"name":"Query Form","description":"A UI component (`QueryForm.tsx`) that provides a textarea for user input. It handles input validation (character limits), displays a character counter, and manages the disabled state of the submit button during query processing. Its validation logic now also requires at least one expert to be selected before a query can be submitted. It also displays the elapsed time while a query is running.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/components/QueryForm.tsx"]},"ProgressDisplay":{"name":"Progress Display","description":"A UI component (`ProgressSection.tsx`) that provides real-time feedback on query processing. It visualizes the different pipeline phases (Map, Resolve, Reduce, etc.) based on Server-Sent Events (SSE) from the backend, shows which experts are active, and displays final statistics upon completion.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/components/ProgressSection.tsx"]},"ResultsDisplay":{"name":"Results Display","description":"A major UI component (`ExpertAccordion.tsx`) that organizes results from multiple experts into a collapsible accordion layout. Each accordion panel contains the synthesized answer and the list of source posts for one expert, allowing for a clean, side-by-side comparison.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/components/ExpertAccordion.tsx"]},"AnswerRenderer":{"name":"Answer Renderer","description":"A UI component (`ExpertResponse.tsx`) responsible for rendering the Markdown-formatted answer from an expert. Its key feature is parsing special syntax like `[post:123]` and converting them into interactive buttons. Clicking these buttons triggers a scroll-to-post action in the Source Posts Viewer.","technology":"React, TypeScript, ReactMarkdown","c4_level":"Component","source_files":["frontend/src/components/ExpertResponse.tsx"]},"SourcePostsViewer":{"name":"Source Posts Viewer","description":"A set of UI components (`PostsList.tsx`, `PostCard.tsx`) that display the source Telegram posts used to generate an answer. The list can be automatically scrolled to a specific post when a user clicks a reference in the Answer Renderer. Each post card shows content, author, date, and associated comments.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/components/PostsList.tsx","frontend/src/components/PostCard.tsx"]},"CommentAnalysisViewer":{"name":"Comment Analysis Viewer","description":"A set of UI components (`CommentGroupsList.tsx`, `CommentSynthesis.tsx`) for displaying results from the 'Comment Drift' analysis pipeline. It lists relevant comment threads from potentially irrelevant posts and shows a synthesized summary of the insights found within those discussions.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/components/CommentGroupsList.tsx","frontend/src/components/CommentSynthesis.tsx"]},"APIClient":{"name":"API Client","description":"A service class (`api.ts`) that encapsulates all communication with the backend API. It handles submitting queries (including an optional `expert_filter`), parsing the Server-Sent Events (SSE) stream for real-time progress from multiple experts in parallel, and fetching post details. It includes a progressive, parallel post-fetching mechanism with retries and integrates with a frontend logging utility to report SSE events for debugging. Its error handling is enhanced to parse user-friendly error messages streamed from the backend. It also provides a `logBatch` method to send client-side logs to the server.","technology":"TypeScript, Fetch API","c4_level":"Component","source_files":["frontend/src/services/api.ts"]},"APITypes":{"name":"API Types","description":"A set of TypeScript interfaces (`api.ts`) that define the data contract between the frontend and the backend. These types match the backend's Pydantic models, ensuring type safety for all API requests and responses. The `QueryRequest` type now includes an `expert_filter` property. The `ProgressEvent` type includes an `expert_error` status for granular, per-expert error reporting during multi-expert queries.","technology":"TypeScript","c4_level":"Component","source_files":["frontend/src/types/api.ts"]},"LLMGateway":{"name":"LLM Gateway","description":"An adapter component (`openrouter_adapter.py`) that configures the OpenAI client to communicate with the OpenRouter.ai API gateway. It translates generic model names (e.g., 'gpt-4o-mini') into provider-specific names (e.g., 'openai/gpt-4o-mini'), decoupling the services from a specific LLM provider.","technology":"Python, OpenAI SDK","c4_level":"Component","source_files":["backend/src/services/openrouter_adapter.py"]},"MapService":{"name":"Map Service","description":{"description":"Implements the 'Map' phase of the pipeline. It takes all posts for an expert, splits them into chunks, and uses a hybrid model strategy for evaluation. It first attempts to use a primary, cost-effective model (e.g., `gemini-2.0-flash-lite`). If this model fails due to rate limits or other issues, the service is designed to fall back to a more powerful model (e.g., `qwen-2.5-72b-instruct`) to ensure the request succeeds. The service performs a 'listwise reranking' task in parallel, instructing the LLM to be highly selective in categorizing posts as HIGH, MEDIUM, or LOW relevance. It includes strict anti-hallucination rules and a robust two-layer retry mechanism.","c4_level":"Component"},"technology":"Python, asyncio, Tenacity, OpenRouter","c4_level":"Component","source_files":["backend/src/services/map_service.py"]},"SimpleResolveService":{"name":"Simple Resolve Service","description":"An alternative, non-LLM implementation of the 'Resolve' phase, now functioning as a 'Differential Resolve Phase'. It operates on the principle that an author's links are inherently relevant. It bypasses LLM evaluation and simply fetches all posts linked at depth 1 from the initial set of HIGH relevance posts only, providing a faster but potentially less precise context expansion. Medium relevance posts bypass this phase.","technology":"Python, SQLAlchemy","c4_level":"Component","source_files":["backend/src/services/simple_resolve_service.py"]},"MediumScoringService":{"name":"Medium Scoring Service","description":"A specialized re-ranking component that processes posts initially marked as 'MEDIUM' relevance by the Map Service. It provides the context of 'HIGH' relevance posts and asks a configurable LLM (defined by the `MODEL_ANALYSIS` environment variable) to score each medium post from 0.0 to 1.0 based on how well it complements the existing high-relevance context. The service then selects the top posts that score above a certain threshold (e.g., 0.7) to be included in the final context for the Reduce phase.","technology":"Python, OpenRouter","c4_level":"Component","source_files":["backend/src/services/medium_scoring_service.py"]},"ReduceService":{"name":"Reduce Service","description":{"description":"Implements the 'Reduce' phase. It takes the final, consolidated list of all relevant posts (HIGH posts with linked content and selected MEDIUM posts) and uses the `HybridLLMClient` to generate a comprehensive answer. It prioritizes a primary synthesis model (e.g., `gemini-2.0-flash`) and can fall back to a more powerful model on OpenRouter if needed. The prompt instructs the LLM to adopt the author's specific persona and writing style, responding in the first person. It enforces a strict citation format (`[post:123]`) and includes a fact-validation step to prevent hallucinated references.","c4_level":"Component"},"technology":"Python, OpenRouter","c4_level":"Component","source_files":["backend/src/services/reduce_service.py"],"notes":"There is a documented plan to migrate this service from 'Gemini 2.0 Flash' to 'Gemini 2.5 Flash-Lite' to improve the quality of synthesis and reduce hallucinations. This involves updating the model name and enhancing the prompt with instructions for adaptive response length based on the number of source posts."},"CommentAnalysisPipeline":{"name":"Comment Analysis Pipeline","description":{"description":"A parallel pipeline dedicated to analyzing 'Comment Drift'. It has two main LLM-driven stages: 1) A 'map' stage that finds relevant comment discussions using a configurable model (`MODEL_COMMENT_GROUPS`, which defaults to a Qwen model) while completely ignoring the content of the parent 'anchor' post. 2) A 'synthesis' stage that takes the main answer as context and uses the `HybridLLMClient` to extract only complementary, additional insights from the relevant comment threads. This synthesis step prioritizes a primary model (e.g., `gemini-2.0-flash`) and can fall back to a more powerful one, explicitly avoiding duplication of information already present in the main answer.","c4_level":"Component"},"technology":"Python, OpenRouter","c4_level":"Component","source_files":["backend/src/services/comment_group_map_service.py","backend/src/services/comment_synthesis_service.py"]},"FactValidationService":{"name":"Fact Validation Service","description":"A service that runs after the Reduce phase to validate the generated answer against the database. It parses `[post:123]` references from the answer text and verifies that these posts actually exist in the database, improving the reliability and accuracy of the output.","technology":"Python, SQLAlchemy","c4_level":"Component","source_files":["backend/src/services/fact_validator.py"]},"LanguageService":{"name":"Language Service","description":"A collection of components and utilities responsible for the system's multi-lingual capabilities. It handles query language detection, prompt injection, content translation, and final response validation to ensure language consistency. The translation and validation services use a configurable LLM defined by the `MODEL_ANALYSIS` environment variable.","technology":"Python","c4_level":"Component","source_files":["backend/src/utils/language_utils.py","backend/src/services/translation_service.py","backend/src/services/language_validation_service.py"]},"ObservabilityService":{"name":"Observability Service","description":"A centralized service (`log_service.py`) for structured logging and real-time progress reporting. It maintains an in-memory log, tracks performance metrics for each pipeline phase, and emits Server-Sent Events (SSE) to the frontend via a callback system.","technology":"Python, asyncio","c4_level":"Component","source_files":["backend/src/services/log_service.py"]},"TelegramEntityConverter":{"name":"Telegram Entity Converter","description":"A utility (`entities_converter.py`) that converts Telegram's rich text entity format (from JSON exports or the Telethon library) into standard Markdown. This ensures that formatting like bold, italics, links, and code blocks are preserved when displaying post content.","technology":"Python","c4_level":"Component","source_files":["backend/src/utils/entities_converter.py"]},"ExpertSelectionUI":{"name":"Expert Selection UI","description":"A set of UI components that allow the user to select which experts to include in a query. It consists of a primary horizontal bar (`ExpertSelectionBar.tsx`) integrated into the main layout and a reusable vertical selector (`ExpertSelector.tsx`). The selection is used to filter the query on the backend and to enable/disable the submit button in the `QueryForm`.","technology":"React, TypeScript","c4_level":"Component","source_files":["frontend/src/components/ExpertSelectionBar.tsx","frontend/src/components/ExpertSelector.tsx","frontend/src/components/StatsAndSelectors.tsx"]},"HybridLLMClient":{"name":"Hybrid LLM Client","description":{"description":"A strategic component that acts as a facade for multiple LLM providers. It is configured to first attempt API calls to Google AI Studio, leveraging a pool of API keys for automatic rotation to maximize free tier usage. If a call fails due to rate limits or other issues, it automatically falls back to using the OpenRouter gateway with a configurable fallback model (e.g., a more powerful Qwen model). This provides cost optimization, high availability, and performance resilience. It also includes a monitoring service to track API calls, success rates, and fallbacks.","c4_level":"Component"},"technology":"Python","c4_level":"Component","source_files":["backend/src/services/hybrid_llm_adapter.py","backend/src/services/google_ai_studio_client.py","backend/src/services/hybrid_llm_monitor.py"]},"QueryErrorHandler":{"name":"Query Error Handler","description":"A centralized utility that intercepts exceptions during query processing. It uses an `APIErrorDetector` to classify errors (e.g., rate limits, payment issues, server errors) and generates user-friendly, localized error messages that are streamed to the frontend via SSE. This improves user experience by providing clear feedback instead of generic error codes.","technology":"Python","c4_level":"Component","source_files":["backend/src/utils/error_handler.py","backend/src/utils/api_error_detector.py"]},"FrontendLogger":{"name":"Frontend Logger","description":"A client-side utility (`debugLogger.ts`) for enhanced development and debugging. It intercepts all console logs, network requests (fetch), and SSE events, preventing recursive logging loops. It buffers these events in a circular buffer (1000-event limit) and periodically sends them in batches to a dedicated backend endpoint (`/api/v1/log-batch`), allowing for server-side aggregation and analysis of client behavior and errors.","technology":"TypeScript","c4_level":"Component","source_files":["frontend/src/utils/debugLogger.ts"]}},"external_integrations":{"c4_level":"Container","description":"The system integrates with external Large Language Models (LLMs) via a single gateway service, OpenRouter.ai, to avoid vendor lock-in and leverage different models for specific tasks.","integrations":[{"name":"OpenRouter.ai","description":"Acts as a central API gateway for accessing various LLMs. The system uses the `openrouter_adapter.py` to map internal model names to OpenRouter's specific identifiers and to configure the OpenAI client to point to OpenRouter's base URL. This allows for flexible model selection without changing core service logic.","technology":"REST API","usage":"Used by all services that perform AI-based analysis, including Map, Resolve, Reduce, Comment Analysis, and Translation."},{"name":"Multiple LLM Providers (via OpenRouter)","description":"The system is configured to use models from several providers, each chosen for its strengths in specific tasks:","technology":"LLM APIs","usage":{"usage":"- **Alibaba Qwen:** Used for document ranking, analysis, and as a high-quality fallback model for synthesis tasks (`qwen-2.5-72b-instruct`). The specific model for analysis tasks is configurable via the `MODEL_ANALYSIS` environment variable.\n- **Google Gemini:** Used as the primary, cost-effective model for synthesis (`gemini-2.0-flash`) and mapping (`gemini-2.0-flash-lite`).","c4_level":"Container"}},{"name":"Google AI Studio (Gemini)","description":{"description":"The system can directly integrate with Google AI Studio to use Gemini models. This is the primary choice for synthesis tasks to leverage free tier usage. The integration is enhanced with an automatic API key rotation mechanism; if one key hits its daily rate limit, the system seamlessly switches to the next available key from a configured list. If all keys are exhausted or another error occurs, it automatically falls back to OpenRouter. This is managed by the `HybridLLMClient`.","c4_level":"Container"},"technology":"REST API","usage":"Used by the Reduce and Comment Synthesis services as the primary LLM provider."},{"name":"Google AI Studio (Gemini)","description":"The system directly integrates with Google AI Studio to use Gemini models as the primary choice for cost-effective synthesis and mapping tasks. This integration supports a list of API keys for automatic rotation to maximize free tier usage. If a call fails (e.g., due to rate limits), the system automatically falls back to using the OpenRouter gateway. This is managed by the `HybridLLMClient`.","technology":"REST API","usage":"Used by the Reduce, Map, and Comment Synthesis services as the primary LLM provider.","c4_level":"Container"}]},"testing_strategy":{"c4_level":"Container","description":"The project employs a multi-faceted testing strategy combining automated validation for functionality and performance with manual, qualitative assessment for answer quality and relevance.","pillars":[{"name":"Automated Functional & Query Validation","description":"An automated test suite, defined in `tests/test_queries.py`, validates the core functionality of the API. It runs a pre-defined set of queries from a configuration file (`test_queries.json`) and asserts the correctness of the responses. This includes checking HTTP status codes, response times against basic thresholds, the number of sources returned, the presence of expected keywords in the generated answer, and the proper sequence of Server-Sent Events (SSE) for streaming responses. This pillar serves as the primary regression test to ensure API stability.","artifacts":["tests/test_queries.py","backend/test_hybrid_llm.py","backend/test_hybrid_simple.py"]},{"name":"Automated Performance & Load Testing","description":"A dedicated performance testing script (`backend/tests/validation/performance_test.py`) validates non-functional requirements. It measures detailed metrics including response time (average, P95, P99), memory usage, and CPU load. The script supports various scenarios, including individual queries, concurrent requests, and load tests simulating multiple users. Results are benchmarked against defined performance targets (e.g., P95 response time < 180 seconds) and a detailed markdown report is generated. This pillar ensures the system is stable and performant under load.","artifacts":["backend/tests/validation/performance_test.py","backend/tests/validation/performance_report.md"]},{"name":"Manual Qualitative & Acceptance Testing","description":"This pillar focuses on assessing the quality and relevance of the AI-generated answers, which is difficult to automate. The process involves an expert defining a suite of representative, domain-specific queries (documented in `backend/tests/validation_context.md`). These queries are run against the system, and the detailed results (answer text, sources, metrics) are captured in markdown files within `backend/tests/outputs/`. An expert then performs a qualitative analysis, scoring the system on precision, recall, and synthesis quality, culminating in a summary report (`FINAL_TESTING_REPORT.md`). This process is also used for targeted regression testing to verify specific bug fixes and improvements.","artifacts":["backend/tests/validation_context.md","backend/tests/outputs/FINAL_TESTING_REPORT.md","backend/tests/outputs/improvements_test_result.md","Other files in backend/tests/outputs/"]}]},"api_surface":{"description":"The REST API exposed by the Backend container. It provides endpoints for querying the expert system, managing comments, and importing data. The API uses Server-Sent Events (SSE) for real-time progress on long-running tasks like queries. There is no authentication layer; all endpoints are public.","c4_level":"Component","groups":[{"name":"Query","description":"Core endpoints for querying the expert system.","endpoints":[{"method":"POST","path":"/api/v1/query","description":"Processes a user query via a parallel multi-expert pipeline, streaming progress and results using Server-Sent Events (SSE). The request can include an `expert_filter` to target specific experts.","request_body_model":"QueryRequest","response_body_model":"EventSourceResponse (streams ProgressEvent, final data is MultiExpertQueryResponse)"},{"method":"GET","path":"/api/v1/posts/{post_id}","description":"Retrieves detailed information for a specific post by its ID, with an option to translate the content based on a provided query context.","request_body_model":"None","response_body_model":"SimplifiedPostDetailResponse"},{"method":"POST","path":"/api/v1/posts/by-ids","description":"Retrieves detailed information for multiple posts in a single batch request.","request_body_model":"PostIdsRequest","response_body_model":"List[SimplifiedPostDetailResponse]"}]},{"name":"Comments","description":"Endpoints for managing expert comments on posts.","endpoints":[{"method":"GET","path":"/api/v1/posts/{post_id}/comments","description":"Retrieves all comments for a specific post.","request_body_model":"None","response_body_model":"CommentsListResponse"},{"method":"POST","path":"/api/v1/comments/collect","description":"Creates a new comment on a post. If a comment from the same author already exists, it updates it.","request_body_model":"CommentCreateRequest","response_body_model":"CommentResponse"},{"method":"PUT","path":"/api/v1/comments/{comment_id}","description":"Updates the text of an existing comment.","request_body_model":"CommentUpdateRequest","response_body_model":"CommentResponse"},{"method":"DELETE","path":"/api/v1/comments/{comment_id}","description":"Deletes a specific comment.","request_body_model":"None","response_body_model":"JSON message"},{"method":"GET","path":"/api/v1/comments/next-post","description":"Fetches the next post that requires expert review/commentary.","request_body_model":"None","response_body_model":"NextPostResponse"},{"method":"POST","path":"/api/v1/comments/batch","description":"Creates or updates multiple comments in a single batch request.","request_body_model":"List[CommentCreateRequest]","response_body_model":"List[CommentResponse]"}]},{"name":"Data Import","description":"Endpoints for importing Telegram channel data from JSON exports.","endpoints":[{"method":"POST","path":"/api/v1/api/import","description":"Accepts a Telegram JSON export file and starts a background import process. The path is a result of nested router prefixes ('/api/v1' + '/api').","request_body_model":"UploadFile","response_body_model":"JSON with job_id"},{"method":"GET","path":"/api/v1/api/import/status/{job_id}","description":"Polls the status of a background import job.","request_body_model":"None","response_body_model":"JSON with job status"},{"method":"GET","path":"/api/v1/api/import/jobs","description":"Lists recent import jobs.","request_body_model":"None","response_body_model":"JSON with list of jobs"},{"method":"DELETE","path":"/api/v1/api/import/jobs","description":"Clears the history of all import jobs.","request_body_model":"None","response_body_model":"JSON message"}]},{"name":"System & Health","description":"Endpoints for system health checks and general information.","endpoints":[{"method":"GET","path":"/health","description":"Performs a health check, verifying database connectivity and API key configuration.","request_body_model":"None","response_body_model":"JSON with system status"},{"method":"GET","path":"/api/info","description":"Provides basic information about the API, including its name, version, and features.","request_body_model":"None","response_body_model":"JSON with API info"}]},{"name":"Admin (Inactive)","description":"Admin endpoints for managing the database on the deployment volume. NOTE: These endpoints are defined but NOT registered in the main FastAPI application and are therefore inactive.","endpoints":[{"method":"POST","path":"/api/v1/admin/upload-database","description":"Uploads a SQLite database file to the persistent volume.","request_body_model":"UploadFile","response_body_model":"JSON message"},{"method":"GET","path":"/api/v1/admin/volume-status","description":"Checks the status of the persistent volume and the database file within it.","request_body_model":"None","response_body_model":"JSON with volume details"}]},{"name":"Logging","description":"Endpoints for receiving logs from client applications.","endpoints":[{"method":"POST","path":"/api/v1/log-batch","description":"Receives a batch of log events from the frontend and writes them to a dedicated server-side log file (`frontend.log`). This allows for server-side aggregation and analysis of client behavior and errors.","request_body_model":"List[LogEntry]","response_body_model":"JSON message"}]}]},"observability":{"c4_level":"Container","description":"The system employs a custom, structured logging and eventing mechanism for real-time progress tracking and performance monitoring.","features":[{"name":"Standard Logging","description":"Uses Python's built-in `logging` module for standard, file-based or console logging within the backend container."},{"name":"Custom LogService","description":"A dedicated service (`log_service.py`) provides structured, in-memory logging with distinct phases (Map, Resolve, etc.) and severity levels. It acts as a central hub for all pipeline events."},{"name":"Server-Sent Events (SSE) for Frontend","description":"The `LogService` is capable of emitting SSE events. Services throughout the pipeline use a `progress_callback` function which, when connected to the `LogService`, streams real-time progress updates to the frontend UI."},{"name":"Performance Metrics","description":"The `LogService` tracks key performance indicators, including timings for each pipeline phase, total queries processed, and success/failure rates."}]},"i18n_l10n":{"c4_level":"Container","description":"The system is designed to be multi-lingual, primarily supporting Russian and English. The strategy focuses on forcing the LLM to respond in the same language as the user's query.","features":[{"name":"Query Language Detection","description":"The `language_utils.py` module detects whether a user's query is primarily in English or Russian by analyzing character and word counts."},{"name":"Strict Prompt Injection","description":"Based on the detected language, a forceful, capitalized instruction block is prepended to every LLM prompt. This instruction commands the model to respond ONLY in the detected language, overriding any other context."},{"name":"Content Translation","description":"If a query is in English, the `TranslationService` is used to translate the Russian source posts into English before they are processed by the pipeline, ensuring the LLM has English context for an English question."},{"name":"Response Language Validation","description":"A final `LanguageValidationService` checks the generated answer. If the answer's language does not match the query's language (e.g., the LLM ignored the instruction), it will translate the final answer to ensure consistency."}]},"workflows_jobs":{"c4_level":"Component","description":"A collection of offline scripts and command-line tools for managing the system's data lifecycle, including data synchronization from Telegram, batch analysis, database migrations, and deployment tasks. These are typically run manually by an administrator or as part of a CI/CD pipeline.","groups":[{"name":"Data Synchronization","description":"Workflows for fetching and updating data from Telegram channels.","jobs":[{"name":"Multi-Expert Incremental Sync","script":"backend/sync_channel_multi_expert.py","description":"The primary data synchronization workflow. It fetches all registered experts (channels) from the database and runs an incremental sync for each one. It identifies new posts and new comments on recent posts, and creates 'pending' records for the drift analysis agent. The depth of the comment check is configurable via the `SYNC_DEPTH` environment variable.","trigger":"Manual execution by an administrator."},{"name":"Single-Channel Sync","script":"backend/sync_channel.py","description":"A CLI tool for running an incremental sync on a single, specified Telegram channel. Useful for debugging, testing, or manual intervention for a specific expert.","trigger":"Manual execution by an administrator."},{"name":"Initial Data Import","script":"backend/run_import.py / backend/import_interactive.py","description":"Scripts for performing an initial, bulk import of a channel's history. The interactive version guides the user through the Telegram authentication process (phone number, SMS code) to create a session file.","trigger":"Manual execution for initial system setup or adding a new channel from scratch."}]},{"name":"Data Processing & Analysis","description":"Batch jobs that perform analysis on the synchronized data.","jobs":[{"name":"Drift Analysis Agent","script":"backend/run_drift_on_synced.py","description":"A targeted analysis job that finds all comment groups marked as 'pending' by the sync workflow and runs a topic drift analysis on them. This is the main job for processing newly ingested data.","trigger":"Manual execution after a data synchronization run.","details":"The drift analysis process can be automated. The documentation outlines two strategies: a rapid parallel processing method requiring manual setup of multiple terminals, and a fully automated, self-reproducing sequential method that processes posts in batches within a single terminal."},{"name":"Full Drift Analysis","script":"backend/analyze_drift.py","description":"A full-batch analysis job that iterates through ALL posts with comments in the database and runs/re-runs the topic drift analysis using an LLM. Useful for initial analysis of the entire dataset or reprocessing after model/prompt changes.","trigger":"Manual execution for large-scale data processing."},{"name":"Drift Topic Extraction","script":"backend/analyze_drift.py (uses extract_drift_topics.txt prompt)","description":"A core offline analysis job that powers the 'Comment Drift' feature. It analyzes a post and its associated comments, instructing an LLM to identify and extract every topic discussed in the comments that was NOT present in the original anchor post. It generates structured data for each 'drift topic', including keywords, key phrases (direct quotes), and a summary, which is then stored in the `comment_group_drift` table for fast querying at runtime.","trigger":"Manual execution for large-scale data processing."},{"name":"Add New Expert Playbook","script":"docs/add-new-expert-playbook.md","description":"A comprehensive, step-by-step manual workflow for adding a new Telegram channel (expert) to the system. It covers data import, comment fetching, creating drift analysis records, and running critical data integrity checks to prevent 'cross-expert pollution' where data from one channel could be mistakenly associated with another due to overlapping message IDs.","trigger":"Manual execution by an administrator when adding a new data source."},{"name":"Targeted Drift Analysis/Correction Scripts","script":"backend/analyze_*.py, backend/run_drift_*.py, backend/update_drift.py, etc.","description":"A collection of specialized, single-use, or developer-oriented scripts for running, re-running, or manually correcting drift analysis for specific, individual posts. These are used for debugging, data correction, or targeted analysis outside the main batch processing workflows. They employ various methods, from simple heuristics to direct LLM API calls and even hardcoded results for data fixing (e.g., `update_drift.py`).","trigger":"Manual execution by a developer or administrator to address specific data issues.","c4_level":"Component"},{"name":"Heuristic Drift Analysis","script":"backend/drift_analysis.py","description":"A non-LLM, heuristic-based batch processing job that finds all comment groups marked as 'pending' and runs a drift analysis on them. It identifies topic shifts by looking for specific keywords (e.g., 'implementation', 'cost', 'timeline') and updates the database with the results. It is designed to be a fast, automated way to process newly ingested data.","trigger":"Manual execution after a data synchronization run.","c4_level":"Component"}]},{"name":"Database & Deployment","description":"Utilities for managing database schema and data between different environments.","jobs":[{"name":"PostgreSQL Migration Applier","script":"backend/apply_postgres_migrations.py","description":"Applies SQL migration files from the `migrations/` directory to a PostgreSQL database. It tracks which migrations have been applied in an `applied_migrations` table to prevent re-running them. Essential for production deployments.","trigger":"Manual or CI/CD execution during deployment."},{"name":"SQLite to PostgreSQL Sync","script":"backend/sync_to_postgres.py","description":"A data migration utility to copy all data from a local SQLite database (used for development) to a production PostgreSQL database. It truncates tables before copying.","trigger":"Manual execution as part of a deployment process (e.g., to Railway)."}]},{"name":"Developer Workflow & AI Management","description":"Tools and frameworks that structure and manage the interactive workflow between a developer and an AI coding assistant.","jobs":[{"name":"Sessions CLI Tool","script":"sessions/bin/sessions","description":"An opinionated, Node.js-based command-line tool and framework for managing the workflow of an AI coding assistant (e.g., Claude Code). It enforces a 'Discussion, Alignment, Implementation, Check' (DAIC) workflow by separating work into a read-only 'Discussion' mode and a write-enabled 'Implementation' mode. The framework is built around 'Tasks' (defined in Markdown files) which act as session boundaries for context management, persistence, and git operations. It integrates deeply into the AI assistant's environment via a system of hooks (e.g., `session_start`, `user_messages`) that load structured 'Protocols' to guide the AI through standardized processes like task creation, startup, and completion. The tool is highly configurable and includes an interactive 'Kickstart' onboarding tutorial and a concept of specialized 'subagents' for specific jobs.","trigger":"Manual execution by a developer via the `sessions` command or integrated `/sessions` slash commands within the Claude Code environment.","c4_level":"Component"}]},{"name":"Development & Maintenance Utilities","description":"Scripts for testing, data seeding, and one-off data correction tasks.","jobs":[{"name":"Test Data Population","script":"backend/populate_test_data.py","description":"A developer utility that clears the database and populates it with a small, consistent set of test data for posts, links, and comments.","trigger":"Manual execution by a developer to set up a clean test environment."},{"name":"Data Correction Scripts","script":"backend/fix_drift_topics.py, backend/fix_double_nested_drift.py, backend/refill_drift_topics.py","description":"A collection of one-off scripts created to fix specific data integrity issues, such as malformed JSON structures in the `drift_topics` column. These are run as needed to correct data errors.","trigger":"Manual execution by a developer to resolve known data issues."},{"name":"Log Cleanup Utility","script":"frontend/clean_logs.py","description":"A Python utility script located in the frontend directory to clean up old log files. It removes files based on age (default > 7 days) and total count (default > 5 files) to manage disk space.","trigger":"Manual execution by a developer.","c4_level":"Component"}]}]},"security_policy":{"c4_level":"Container","description":"The project has a formal security policy. Vulnerabilities should be reported privately to security@experts-panel.dev. The policy covers authentication, data protection, infrastructure, and application security. It explicitly recommends using environment variables for API keys, keeping dependencies updated, and enforcing HTTPS in production."},"project_processes":{"c4_level":"Context","code_of_conduct":"The project adheres to the Contributor Covenant Code of Conduct, outlining standards for a harassment-free and inclusive community.","license":"The project's source code is licensed under the MIT License. It also includes the JetBrains Mono font, which is licensed separately under the SIL Open Font License, Version 1.1 (see OFL.txt).","development_methodology":"The project follows a structured development methodology outlined in specification documents. This includes phases for Research, Design (data models, API contracts), and Task Planning before implementation begins. Development is guided by a 'Constitution' of architectural principles.","contribution_guidelines":{"description":"The project uses standardized templates for contributions, including Pull Requests, Bug Reports, and Feature Requests, stored in the .github/ directory."},"development_setup":{"description":"The development environment requires Python 3.11+ and Node.js 18+. The backend is run via `uvicorn src.api.main:app` on port 8000 and the frontend via `npm run dev` on port 3000. A comprehensive developer guide is available in the root `CLAUDE.md` file, with more detailed guides in `backend/CLAUDE.md` and `frontend/CLAUDE.md`.","environment_variables":[{"name":"OPENROUTER_API_KEY","description":"Required. The API key for multi-model access via OpenRouter.ai, used as a fallback."},{"name":"GOOGLE_AI_STUDIO_API_KEY","description":"Optional. A comma-separated list of API keys for Google AI Studio. Enables the primary, cost-effective LLM strategy."},{"name":"MODEL_MAP_PRIMARY","description":"Defines the primary model for the Map phase (e.g., `gemini-2.0-flash-lite`)."},{"name":"MODEL_MAP_FALLBACK","description":"Defines the fallback model for the Map phase (e.g., `qwen/qwen-2.5-72b-instruct`)."},{"name":"MODEL_SYNTHESIS_PRIMARY","description":"Defines the primary model for the Synthesis/Reduce phase (e.g., `gemini-2.0-flash`)."},{"name":"MODEL_SYNTHESIS_FALLBACK","description":"Defines the fallback model for the Synthesis/Reduce phase (e.g., `qwen/qwen-2.5-72b-instruct`)."},{"name":"MODEL_ANALYSIS","description":"Defines the model for general analysis tasks like Medium Scoring and Translation (e.g., `qwen/qwen-2.5-72b-instruct`)."},{"name":"MODEL_COMMENT_GROUPS","description":"Defines the model for the comment group drift analysis task."},{"name":"ENVIRONMENT","description":"Set to `production` to hide sensitive logs, or `development` for detailed debugging information."}]}},"architectural_decisions":{"c4_level":"Context","description":"Key foundational decisions made during the project's research phase.","decisions":[{"topic":"LLM Model","decision":"Initially GPT-4o-mini for all phases due to cost-effectiveness and a large context window. The system later evolved into a multi-model strategy.","rationale":"A single model simplified initial prompt engineering. Later, different models were chosen for their strengths in specific tasks (ranking, synthesis, etc.)."},{"topic":"Data Storage","decision":"SQLite with foreign keys.","rationale":"Sufficient for MVP, maintains referential integrity, and allows for simple single-file deployment. PostgreSQL was considered unnecessary complexity initially."},{"topic":"Caching","decision":"No caching for MVP.","rationale":"Prioritized accuracy over performance, as similarity matching could introduce false positives. Low query cost with GPT-4o-mini made caching non-essential at the start."},{"topic":"Frontend Framework","decision":"React 18 with TypeScript.","rationale":"Chosen as a learning objective for the user, its component model fits the UI requirements well, and TypeScript provides type safety."},{"topic":"Deployment","decision":"Docker container deployed to Railway (later Fly.io).","rationale":"Provides a simple, one-click deployment from GitHub, with a free tier sufficient for the MVP."}]},"configuration":{"c4_level":"Code","description":"Defines the project's dependencies, environment configuration, build processes, and developer tooling.","dependencies":{"python_backend":{"source":"backend/uv.lock","description":"The backend uses Python 3.11+. All dependencies are pinned in the `uv.lock` file for reproducible builds. Key libraries include FastAPI, SQLAlchemy, OpenAI, and Telethon."},"javascript_frontend":{"source":"frontend/package.json, quickstart_validate.py","description":"The frontend is built with Node.js 18+. Key libraries include React, TypeScript, Vite, TanStack Query, and Tailwind CSS."}},"environment":{"runtime_prerequisites":{"source":"quickstart_validate.py","description":"The development environment requires Python 3.11+ and Node.js 18+."},"environment_variables":{"source":".env.example, quickstart_validate.py","description":"Configuration is managed via `.env` files, which are excluded from version control. The primary required variable is `OPENROUTER_API_KEY` for LLM access. Other optional variables control the database URL and pipeline behavior."},"model_configuration":{"source":"backend/src/config.py","description":{"description":"A centralized and flexible configuration system manages which LLM is used for each phase of the pipeline, enabling performance and cost optimization by swapping models via environment variables without code changes. It implements two distinct hybrid strategies:\n1. **Synthesis Hybrid Strategy (Reduce & Comment Synthesis):** Managed by the `HybridLLMClient`, it prioritizes Google AI Studio (with API key rotation) for cost-effective models like Gemini Flash. If a call fails, it automatically falls back to a more powerful model on the OpenRouter gateway.\n2. **Map Hybrid Strategy:** The Map phase is configured with a primary model (e.g., `gemini-2.0-flash-lite`) and a fallback model (e.g., `qwen-2.5-72b-instruct`). If the primary model fails, the system automatically switches to the fallback model to ensure the query can be processed.","c4_level":"Code"}}},"build_and_deployment":{"version_control_exclusions":{"source":".gitignore","description":"The `.gitignore` file excludes secrets (.env, .pem), local data (data/, *.db), dependency folders (venv/, node_modules/), and IDE-specific configuration (.vscode/, .idea/) from version control."},"containerization_exclusions":{"source":".dockerignore, backend/.dockerignore","description":"Docker builds use `.dockerignore` files to create lean and secure images. The root `.dockerignore` is for the monolithic deployment and includes the compiled frontend, while `backend/.dockerignore` is for the two-container backend service."},"frontend_build":{"source":"frontend/vite.config.ts","description":"The frontend build is managed by Vite. The configuration includes path aliases ('@', '@components', etc.) for cleaner imports and a development server that proxies '/api' requests to the backend on port 8000. The dev server is configured to run on port 3000 with `strictPort` enabled.","c4_level":"Code"}},"developer_tooling":{"environment_validator":{"source":"quickstart_validate.py","description":"A validation script (`quickstart_validate.py`) is provided to check if the local development environment is correctly configured (Python/Node versions, dependencies, DB, env vars)."},"ai_assistant_integration":{"source":"claude-vscode-monitor.sh","description":"The repository contains a developer-specific script (`claude-vscode-monitor.sh`) for monitoring an AI coding assistant (Claude), indicating its use in the development process."}}}}