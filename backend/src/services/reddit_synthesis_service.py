"""Reddit Synthesis Service - Gemini-powered analysis of Reddit content.

This module provides synthesis of Reddit search results using Google Gemini AI
to extract insights, sentiment, and actionable information from community discussions.
"""

import logging
import html
from typing import Optional, List, Dict, Any

from .. import config
from ..utils.language_utils import detect_query_language
from .reddit_service import RedditSearchResult, RedditSource
from .google_ai_studio_client import create_google_ai_studio_client, GoogleAIStudioError

logger = logging.getLogger(__name__)

DEFAULT_SYNTHESIS_MODEL = "gemini-2.0-flash"


class RedditSynthesisService:
    """Service for synthesizing Reddit content via Gemini AI.
    
    Analyzes Reddit posts and extracts:
    - Reality Check: Bugs, edge cases, hardware issues
    - Hacks: Workarounds and unofficial solutions
    - Vibe: Overall sentiment and community opinion
    """
    
    def __init__(self, model: Optional[str] = None):
        """Initialize synthesis service.
        
        Args:
            model: Gemini model to use (default: gemini-2.0-flash)
        """
        # FIX: Use MODEL_SYNTHESIS (gemini-3-flash-preview) for high-quality Reddit analysis
        # This matches the main synthesis model for expert responses
        self.model = model or config.MODEL_SYNTHESIS or DEFAULT_SYNTHESIS_MODEL
        self._client = create_google_ai_studio_client()
    
    async def synthesize(
        self,
        query: str,
        reddit_result: Any, # Typed as Any to support both result types
        max_sources_in_context: int = 10
    ) -> str:
        """Synthesize Reddit insights via Gemini.
        
        Args:
            query: Original user query
            reddit_result: Result from Reddit search
            max_sources_in_context: Max number of sources to include in prompt
        
        Returns:
            Markdown-formatted synthesis with insights (in query language)
        """
        # Unified check for empty results
        has_posts = False
        if hasattr(reddit_result, 'posts') and reddit_result.posts:
            has_posts = True
        elif hasattr(reddit_result, 'sources') and reddit_result.sources:
            has_posts = True
            
        if not has_posts:
            query_lang = detect_query_language(query)
            if query_lang == "Russian":
                return "–û–±—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."
            return "No community discussions found for this query."
        
        # Detect query language for response
        query_language = detect_query_language(query)
        
        # Build context from sources
        context = self._build_context(reddit_result, max_sources_in_context)
        
        # Create synthesis prompt in query language
        messages = self._create_synthesis_prompt(query, context, query_language)

        try:
            response = await self._client.chat_completions_create(
                model=self.model,
                messages=messages,
                temperature=0.3,  # Lower temp for factual analysis
                max_tokens=4096
            )
            
            synthesis = response.choices[0].message.content.strip()
            
            # Unified access for logging
            if hasattr(reddit_result, 'posts'):
                count = reddit_result.total_found
            else:
                count = reddit_result.found_count
                
            logger.info(
                f"Reddit synthesis completed for query: {query[:50]}... "
                f"(found {count} posts)"
            )
            
            return synthesis
            
        except GoogleAIStudioError as e:
            logger.error(f"Gemini synthesis failed: {e}")
            # Fallback: return raw markdown if synthesis fails
            return self._create_fallback_response(reddit_result, query_language)
        except Exception as e:
            logger.error(f"Unexpected error in synthesis: {e}")
            return self._create_fallback_response(reddit_result, query_language)
    
    # High-signal keywords indicating the OP found the solution helpful
    VERIFICATION_KEYWORDS = {
        "worked", "thanks", "thank you", "solved", "fixed", 
        "—Å—Ä–∞–±–æ—Ç–∞–ª–æ", "—Å–ø–∞—Å–∏–±–æ", "—Ä–µ—à–∏–ª"
    }

    def _format_comments_recursive(self, comments: List[Dict[str, Any]], depth: int = 0, max_depth: int = 3, post_author: str = None) -> str:
        """Recursively format comments tree.
        
        Args:
            comments: List of comment dictionaries
            depth: Current nesting depth
            max_depth: Maximum recursion depth
            post_author: Username of the post author (OP) to detect verified solutions
        """
        if depth > max_depth or not comments:
            return ""
        
        output = []
        indent = "  " * (depth + 1)
        
        for i, comment in enumerate(comments, 1):
            # Handle different comment structures
            if isinstance(comment, str):
                # Legacy format: simple string comment
                author = "unknown"
                body = comment
                score = 0
                replies = []
                flair = ""
                distinguished = ""
                stickied = False
                is_op = False
            elif isinstance(comment, dict):
                author = comment.get('author', 'unknown')
                raw_body = comment.get('body', '') or comment.get('text', '')
                body = html.unescape(raw_body)
                score = comment.get('score', 0)
                replies = comment.get('replies') or []
                # New Metadata Fields
                flair = comment.get('flair', '')
                distinguished = comment.get('distinguished', '')
                stickied = comment.get('stickied', False)
                # Check for explicit is_op flag or compare with post_author (ignoring deleted/unknown)
                is_valid_author = author and author.lower() not in ["[deleted]", "unknown"]
                is_op = comment.get('is_op', False) or (is_valid_author and post_author and author.lower() == post_author.lower())
            else:
                # Fallback for objects
                author = getattr(comment, 'author', 'unknown')
                raw_body = getattr(comment, 'body', '') or getattr(comment, 'text', '')
                body = html.unescape(raw_body)
                score = getattr(comment, 'score', 0)
                replies = getattr(comment, 'replies', []) or []
                # New Metadata Fields
                flair = getattr(comment, 'flair', '')
                distinguished = getattr(comment, 'distinguished', '')
                stickied = getattr(comment, 'stickied', False)
                # Check for explicit is_op flag or compare with post_author (ignoring deleted/unknown)
                is_valid_author = author and author.lower() not in ["[deleted]", "unknown"]
                is_op = getattr(comment, 'is_op', False) or (is_valid_author and post_author and author.lower() == post_author.lower())

            if body:
                # Detect OP Verification (Golden Answer)
                # If the OP replied to this comment saying "thanks", "solved", "worked", etc.
                is_verified = False
                if post_author and post_author != "unknown" and replies:
                    for reply in replies:
                        # Check reply author safely
                        if isinstance(reply, str):
                            r_author = "unknown"
                            r_body = reply
                        elif isinstance(reply, dict):
                            r_author = reply.get('author', 'unknown')
                            r_body = reply.get('body', '') or reply.get('text', '')
                        else:
                            r_author = getattr(reply, 'author', 'unknown')
                            r_body = getattr(reply, 'body', '') or getattr(reply, 'text', '')
                        
                        if r_author == post_author and any(kw in r_body.lower() for kw in self.VERIFICATION_KEYWORDS):
                            is_verified = True
                            break

                # Truncate extremely long comments but keep enough for context (2000 chars)
                if len(body) > 2000:
                    body = body[:2000] + "... (truncated)"
                
                # Build Metadata Tags
                tags = []
                if is_op:
                    tags.append("[OP]")
                if distinguished: # moderator/admin
                    tags.append(f"[{distinguished.upper()}]")
                if stickied:
                    tags.append("[PINNED]")
                if flair:
                    tags.append(f'[Flair: "{flair}"]')
                if is_verified:
                    tags.append("[‚úÖ OP VERIFIED SOLUTION]")
                
                tags_str = " ".join(tags) + " " if tags else ""
                
                prefix = "‚îî‚îÄ " if depth > 0 else f"{i}. "
                # Format: [OP] [Flair: "Dev"] [User | Score: 100]: Body
                header = f"{indent}{prefix}{tags_str}[{author} | Score: {score}]: "
                
                # Handle multi-line content (code blocks, paragraphs) by indenting subsequent lines
                # This preserves structure for the LLM
                content_indent = "\n" + indent + ("   " if depth > 0 else "   ")
                formatted_body = body.replace("\n", content_indent)
                
                output.append(f"{header}{formatted_body}")
                
                # Process replies if they exist and we haven't hit max depth
                if replies:
                    replies_text = self._format_comments_recursive(replies, depth + 1, max_depth, post_author=post_author)
                    if replies_text:
                        output.append(replies_text)
        
        return "\n".join(output)

    def _build_context(
        self,
        reddit_result: Any, # Typed as Any to support both result types during migration
        max_sources: int
    ) -> str:
        """Build context string from Reddit sources.
        
        Args:
            reddit_result: Reddit search result (EnhancedSearchResult or RedditSearchResult)
            max_sources: Maximum sources to include
        
        Returns:
            Formatted context string
        """
        # Handle both result types
        if hasattr(reddit_result, 'posts'):
            sources = reddit_result.posts[:max_sources]
        else:
            sources = reddit_result.sources[:max_sources]
        
        # DEBUG: Log sources content
        logger.info(f"SYNTHESIS DEBUG: Building context from {len(sources)} sources")
        
        context_parts = []
        for i, src in enumerate(sources, 1):
            # Handle different content attributes (selftext vs content)
            raw_content = getattr(src, 'selftext', '') or getattr(src, 'content', '') or "[No content available]"
            
            # Increase limit to 15,000 chars to leverage Gemini's context window
            content_preview = raw_content[:15000]
            if len(raw_content) > 15000:
                content_preview += "... (truncated)"
            
            # Format top comments with tree structure
            comments_section = ""
            # Handle comments attribute (top_comments vs comments)
            comments_data = getattr(src, 'top_comments', []) or getattr(src, 'comments', [])
            
            if comments_data:
                # Pass post author to recursive formatter for OP verification detection
                post_author = getattr(src, 'author', 'unknown')
                comments_text = self._format_comments_recursive(comments_data, post_author=post_author)
                if comments_text:
                    comments_section = f"\n   - **Discussion Tree:**\n{comments_text}"
            
            context_parts.append(
                f"{i}. **{src.title}** (r/{src.subreddit})\n"
                f"   - Content: {content_preview}\n"
                # Use getattr for stats to be safe
                f"   - Stats: Score: {getattr(src, 'score', 0)} | Comments: {getattr(src, 'num_comments', getattr(src, 'comments_count', 0))}\n"
                f"   - URL: {src.url}"
                f"{comments_section}"
            )
        
        return "\n\n".join(context_parts)
    
    def _create_synthesis_prompt(
        self,
        query: str,
        context: str,
        query_language: str = "English"
    ) -> List[Dict[str, str]]:
        """Create synthesis prompt for Gemini.
        
        Args:
            query: User query
            context: Reddit posts context
            query_language: Language of the query (English or Russian)
        
        Returns:
            Messages list for chat completion
        """
        # Determine response language
        is_russian = query_language == "Russian"
        
        # Get current date for context (Project is in 2026)
        from datetime import datetime
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        
        if is_russian:
            system_prompt = f"""<?xml version="1.0" encoding="UTF-8"?>
<system_prompt>
    <role>–í—ã ‚Äî –í–µ–¥—É—â–∏–π –ò–Ω–∂–µ–Ω–µ—Ä (Staff Engineer), –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π –±–∞–∑—É –∑–Ω–∞–Ω–∏–π Reddit –¥–ª—è –∫–æ–ª–ª–µ–≥–∏.</role>
    <context>
        <date>–°–ï–ì–û–î–ù–Ø: {current_date_str}. –£—á–∏—Ç—ã–≤–∞–π—Ç–µ, —á—Ç–æ –º—ã –≤ 2026 –≥–æ–¥—É.</date>
    </context>
    <task>–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –ò–°–ß–ï–†–ü–´–í–ê–Æ–©–ò–ô —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç (+30% –¥–µ—Ç–∞–ª–µ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º summary).</task>
    <evaluation_criteria>
        <signal type="authority">FLAIRS: –î–æ–≤–µ—Ä—è–π—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å –ø–ª–∞—à–∫–∞–º–∏ —Ç–∏–ø–∞ "Maintainer", "Dev", "Contributor".</signal>
        <signal type="verification" priority="highest">OP VERIFICATION: –†–µ—à–µ–Ω–∏—è, –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ `[‚úÖ OP VERIFIED SOLUTION]`, –∏–º–µ—é—Ç –Ω–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–∞–≤—Ç–æ—Ä –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª, —á—Ç–æ —ç—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ).</signal>
        <signal type="skepticism">SCORE SKEPTICISM: –í—ã—Å–æ–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –Ω–µ –≤—Å–µ–≥–¥–∞ –æ–∑–Ω–∞—á–∞–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –ø—Ä–∞–≤–æ—Ç—É (—ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —à—É—Ç–∫–∞). –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Ñ–∞–∫—Ç—ã.</signal>
    </evaluation_criteria>
    <analysis_rules>
        <rule type="discovery">HIDDEN GEMS: –ò—â–∏—Ç–µ –≤ –≥–ª—É–±–∏–Ω–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–ª–∞–≥–∏, –∫–æ–Ω—Ñ–∏–≥–∏, –±–µ–Ω—á–º–∞—Ä–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–ø—É—Å—Ç–∏–ª –∞–≤—Ç–æ—Ä –ø–æ—Å—Ç–∞.</rule>
        <rule type="alternative">CONTROVERSIAL TAKES: –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∏–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –ü–†–û–¢–ò–í –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –º–Ω–µ–Ω–∏—è ‚Äî –≤—ã –æ–±—è–∑–∞–Ω—ã –∏—Ö –ø—Ä–∏–≤–µ—Å—Ç–∏.</rule>
        <rule type="context">VERSION SPECIFIC: –£–∫–∞–∑—ã–≤–∞–π—Ç–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫/—Å–æ—Ñ—Ç–∞, –æ –∫–æ—Ç–æ—Ä—ã—Ö –∏–¥–µ—Ç —Ä–µ—á—å.</rule>
        <rule type="citation">LINK PRIORITY: –°—Å—ã–ª–∫–∏ –Ω–∞ GitHub/HuggingFace = [PRIMARY SOURCE].</rule>
        <rule type="trend">PIVOT ALERT: –ï—Å–ª–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ –º–µ–Ω—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, "LangChain —É–º–µ—Ä, –±–µ—Ä–∏ LangGraph") ‚Äî –Ω–∞—á–Ω–∏—Ç–µ —Å –±–ª–æ–∫–∞ `üö® **–°–ú–ï–ù–ê –¢–†–ï–ù–î–ê**`.</rule>
    </analysis_rules>
    <output_format>
        <section order="1">Executive Summary: –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç, –∫–æ–Ω—Å–µ–Ω—Å—É—Å 2026 –≥–æ–¥–∞.</section>
        <section order="2">Deep Dive: –ö–æ–¥, –∫–æ–Ω—Ñ–∏–≥–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞. –°–∞–º–∞—è –±–æ–ª—å—à–∞—è —Å–µ–∫—Ü–∏—è.</section>
        <section order="3">Minority Report: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–Ω–µ–Ω–∏—è –æ–ø—ã—Ç–Ω—ã—Ö –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤ (–æ—Å–æ–±–µ–Ω–Ω–æ —Å Flair), –Ω–µ—Å–æ–≥–ª–∞—Å–Ω—ã–µ —Å –º–µ–π–Ω—Å—Ç—Ä–∏–º–æ–º.</section>
        <section order="4">Battle-tested Edge Cases: –†–µ–∞–ª—å–Ω—ã–µ –±–∞–≥–∏ –∏ –ø—Ä–æ–±–ª–µ–º—ã –∏–∑ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞.</section>
        <style>–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ë–µ–∑ –≤–æ–¥—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Markdown —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –û—Ç–≤–µ—á–∞–π—Ç–µ –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.</style>
    </output_format>
</system_prompt>"""

            user_prompt = f"""**–í–æ–ø—Ä–æ—Å:** {query}

**–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π Reddit:**

{context}

–î–∞–π—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç, –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –Ω–∞ {current_date_str}."""
        else:
            system_prompt = f"""<?xml version="1.0" encoding="UTF-8"?>
<system_prompt>
    <role>You are a Staff Engineer analyzing the Reddit knowledge base for a colleague.</role>
    <context>
        <date>TODAY IS: {current_date_str}. Keep in mind we are in 2026.</date>
    </context>
    <task>Synthesize a COMPREHENSIVE technical answer (+30% detail density compared to standard summary).</task>
    <evaluation_criteria>
        <signal type="authority">FLAIRS: Trust users with flairs like "Maintainer", "Dev", "Contributor".</signal>
        <signal type="verification" priority="highest">OP VERIFICATION: Solutions marked `[‚úÖ OP VERIFIED SOLUTION]` have highest priority (author confirmed it worked).</signal>
        <signal type="skepticism">SCORE SKEPTICISM: High score does not always mean technical correctness (could be a joke). Verify facts.</signal>
    </evaluation_criteria>
    <analysis_rules>
        <rule type="discovery">HIDDEN GEMS: Dig deep into comments for specific flags, configs, benchmarks that the OP missed.</rule>
        <rule type="alternative">CONTROVERSIAL TAKES: If there are strong arguments AGAINST the popular opinion, you MUST include them.</rule>
        <rule type="context">VERSION SPECIFIC: Mention library/software versions discussed.</rule>
        <rule type="citation">LINK PRIORITY: Links to GitHub/HuggingFace = [PRIMARY SOURCE].</rule>
        <rule type="trend">PIVOT ALERT: If the community is shifting standards (e.g., "LangChain is dead, use LangGraph") ‚Äî start with a `üö® **COMMUNITY PIVOT**` block.</rule>
    </analysis_rules>
    <output_format>
        <section order="1">Executive Summary: Direct answer, 2026 consensus.</section>
        <section order="2">Deep Dive: Code, configs, architecture. Largest section.</section>
        <section order="3">Minority Report: Alternative views from experienced engineers (esp. with Flair) against the mainstream.</section>
        <section order="4">Battle-tested Edge Cases: Real-world bugs and production issues.</section>
        <style>Maximum information density. No fluff. Use Markdown tables for comparisons. Answer in English.</style>
    </output_format>
</system_prompt>"""

            user_prompt = f"""**Query:** {query}

**Reddit Knowledge Base:**

{context}

Provide an expert technical synthesis relevant for {current_date_str}."""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    
    def _create_fallback_response(
        self, 
        reddit_result: Any,
        query_language: str = "English"
    ) -> str:
        """Create fallback response when synthesis fails.
        
        Args:
            reddit_result: Reddit search result
            query_language: Language of the query
        
        Returns:
            Basic markdown response with sources
        """
        is_russian = query_language == "Russian"
        
        # Unified access
        if hasattr(reddit_result, 'posts'):
            sources = reddit_result.posts
            count = reddit_result.total_found
        else:
            sources = reddit_result.sources
            count = reddit_result.found_count
            
        if not sources:
            if is_russian:
                return "–û–±—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."
            return "No community discussions found for this query."
        
        if is_russian:
            lines = ["### –û–±—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ", ""]
            lines.append(f"–ù–∞–π–¥–µ–Ω–æ {count} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–∞ Reddit:")
        else:
            lines = ["### Community Discussions", ""]
            lines.append(f"Found {count} relevant posts on Reddit:")
        
        lines.append("")
        
        for src in sources[:5]:
            # FIX: Escape markdown special characters to prevent injection/broken formatting
            escaped_title = src.title.replace("[", "\\[").replace("]", "\\]")
            escaped_url = src.url.replace(")", "%29")  # URL-encode closing parenthesis
            
            # Use getattr for unified access
            subreddit = getattr(src, 'subreddit', 'unknown')
            score = getattr(src, 'score', 0)
            comments = getattr(src, 'num_comments', getattr(src, 'comments_count', 0))
            
            lines.append(f"- **[{escaped_title}]({escaped_url})** (r/{subreddit})")
            if is_russian:
                lines.append(f"  –†–µ–π—Ç–∏–Ω–≥: {score} | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {comments}")
            else:
                lines.append(f"  Score: {score} | Comments: {comments}")
            lines.append("")
        
        return "\n".join(lines)
    
    async def quick_synthesize(
        self,
        query: str,
        reddit_result: Any
    ) -> Dict[str, Any]:
        """Quick synthesis returning structured data.
        
        Args:
            query: User query
            reddit_result: Reddit search result
        
        Returns:
            Dictionary with synthesis text and metadata
        """
        synthesis_text = await self.synthesize(query, reddit_result)
        
        # Unified access for stats
        if hasattr(reddit_result, 'posts'):
            count = len(reddit_result.posts)
            total = reddit_result.total_found
        else:
            count = len(reddit_result.sources)
            total = reddit_result.found_count
        
        return {
            "synthesis": synthesis_text,
            "sources_count": count,
            "total_found": total,
            "processing_time_ms": reddit_result.processing_time_ms,
            "model_used": self.model
        }


# Convenience function
async def synthesize_reddit_content(
    query: str,
    reddit_result: RedditSearchResult,
    model: Optional[str] = None
) -> str:
    """Convenience function to synthesize Reddit content.
    
    Args:
        query: User query
        reddit_result: Reddit search result
        model: Optional model override
    
    Returns:
        Synthesis markdown text
    """
    service = RedditSynthesisService(model)
    return await service.synthesize(query, reddit_result)
